{"cells":[{"cell_type":"markdown","source":["---\n","**Как работает механизм RAG, его реализации вручную и с ипользованием библиотеки Langchain, обзор библиотеки Langchain, а так же пример написания web-приложения чат-бота с механизмом RAG на фреймворке Gradio + Docker**"],"metadata":{"id":"r0V39CdhHzhY"}},{"cell_type":"markdown","metadata":{"id":"IOLcB9b1wHam"},"source":["---\n","RAG (Retrieval Augmented Generation) — простое и понятное объяснение статья  \n","https://habr.com/ru/articles/779526/\n","\n","Векторные БД статьи  \n","https://habr.com/ru/articles/807957/  \n","https://habr.com/ru/articles/817173/\n","https://habr.com/ru/articles/784158/\n","\n","RAG статьи  \n","https://habr.com/ru/companies/wunderfund/articles/779748/  \n","https://habr.com/ru/companies/bothub/articles/825850/  \n","https://www.promptingguide.ai/research/rag?utm_source=substack&utm_medium=email  \n","https://vinija.ai/nlp/RAG/  \n","https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system  \n","\n","Туториал по RAG от HF  \n","https://huggingface.co/learn/cookbook/advanced_rag\n","\n","Туториалы по langchain и RAG  \n","https://blog.davideai.dev/series/langchain  \n","https://habr.com/ru/articles/729664/  \n","\n","Курс Building Agentic RAG with LlamaIndex  \n","https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/\n","\n","Компактная реализация RAG через Ollama  \n","https://github.com/technovangelist/videoprojects/tree/main/2024-04-04-build-rag-with-python  \n","Видео про RAG где она используется (RU)  \n","https://www.youtube.com/watch?v=DyOKAVzMWaQ\n","\n","\n","---"]},{"cell_type":"markdown","source":["# Gradio app old all peges"],"metadata":{"id":"-WaFdMPrbGTI"}},{"cell_type":"markdown","source":["Туториал чат бот с помощью нового интерфейса  \n","https://www.gradio.app/guides/creating-a-chatbot-fast\n","\n","Туториал чат бот с помощью блоков  \n","https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks\n","\n","Туториал по темам  \n","https://www.gradio.app/guides/theming-guide\n","\n","Конструктор тем  \n","https://huggingface.co/spaces/gradio/theme_builder\n","\n","Галерея тем  \n","https://huggingface.co/spaces/gradio/theme-gallery\n","\n","Доки по блоками и событиям, декораторы  \n","https://www.gradio.app/guides/blocks-and-event-listeners\n","\n","Прогресс бар  \n","https://www.gradio.app/guides/key-features#progress-bars\n","\n","Туториал по катомному чат боту через gr.ChatBot  \n","https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks"],"metadata":{"id":"i0lS2jbKf9HM"}},{"cell_type":"markdown","source":["Примеры"],"metadata":{"id":"CkwupJ1zNA96"}},{"cell_type":"markdown","source":["Пример приложения Опенчат через Докер и create_chat_completion  \n","https://huggingface.co/spaces/Tomoniai/Open-Chat/blob/main/app.py  \n","\n","Пример простого приложения с разными базовыми моделями prefix suffix ctransformers   \n","https://huggingface.co/spaces/daniellefranca96/Open_LLMs_Playground/blob/main/app.py\n","\n"],"metadata":{"id":"BkDfPoPv8MWd"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"xotutjxBYBU7"}},{"cell_type":"code","source":["%%time\n","%%capture\n","# faiss-gpu or faiss-gpu\n","!pip install accelerate langchain chromadb sentence_transformers pdfminer.six llama-cpp-python \\\n","                        youtube-transcript-api rank_bm25 faiss-cpu datasets gradio"],"metadata":{"id":"76d-q4-XYECM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707397652951,"user_tz":-180,"elapsed":100383,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"e3437162-ccb9-4902-cc64-68be7b51f28b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 533 ms, sys: 149 ms, total: 682 ms\n","Wall time: 1min 40s\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6U4ZOneYECO"},"outputs":[],"source":["import gc\n","import psutil\n","from pathlib import Path\n","from shutil import rmtree\n","from collections import deque\n","from typing import List, Tuple, Dict, Union, Iterable, Optional, Any\n","import time\n","import csv\n","import os\n","\n","import requests\n","import torch\n","from huggingface_hub import hf_hub_download, list_repo_tree, list_repo_files, repo_info, repo_exists\n","from llama_cpp import Llama, llama\n","from youtube_transcript_api import YouTubeTranscriptApi\n","import gradio as gr\n","\n","from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","from langchain_community.vectorstores import FAISS, Chroma\n","from langchain_community.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n","from langchain_community.retrievers import BM25Retriever, TFIDFRetriever\n","from langchain.retrievers import EnsembleRetriever\n","\n","# Loader classes\n","from langchain_community.document_loaders import (\n","    CSVLoader,\n","    PDFMinerLoader,\n","    PyPDFLoader,\n","    TextLoader,\n","    UnstructuredEmailLoader,\n","    UnstructuredHTMLLoader,\n","    UnstructuredMarkdownLoader,\n","    UnstructuredODTLoader,\n","    UnstructuredPowerPointLoader,\n","    UnstructuredWordDocumentLoader,\n","    WebBaseLoader,\n","    YoutubeLoader,\n","    DirectoryLoader,\n",")\n","\n","# Annotations\n","from langchain_core.retrievers import BaseRetriever\n","from langchain.docstore.document import Document\n","from langchain_core.vectorstores import VectorStore\n","from langchain_core.embeddings import Embeddings"]},{"cell_type":"markdown","source":["`requirements.txt`  \n","```\n","llama-cpp-python\n","youtube-transcript-api\n","accelerate\n","langchain\n","chromadb\n","sentence_transformers\n","pdfminer.six\n","rank_bm25\n","faiss-cpu\n","datasets\n","gradio\n","```"],"metadata":{"id":"chcdYRU_dqDP"}},{"cell_type":"code","source":["!pip list | grep -P \\\n","'llama_cpp_python|youtube-transcript-api|accelerate|langchain|chromadb|sentence-transformers|pdfminer.six|rank_bm25|faiss-cpu|datasets|gradio|torch'\n","\n","# accelerate                               0.27.2\n","# chromadb                                 0.4.24\n","# datasets                                 2.18.0\n","# faiss-cpu                                1.8.0\n","# gradio                                   4.21.0\n","# gradio_client                            0.12.0\n","# langchain                                0.1.11\n","# langchain-community                      0.0.27\n","# langchain-core                           0.1.30\n","# langchain-text-splitters                 0.0.1\n","# llama_cpp_python                         0.2.56\n","# pdfminer.six                             20231228\n","# sentence-transformers                    2.5.1\n","# tensorflow-datasets                      4.9.4\n","# torch                                    2.1.0+cu121\n","# torchaudio                               2.1.0+cu121\n","# torchdata                                0.7.0\n","# torchsummary                             1.5.1\n","# torchtext                                0.16.0\n","# torchvision                              0.16.0+cu121\n","# vega-datasets                            0.9.0\n","# youtube-transcript-api                   0.6.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ESvGj0nEeD5Y","executionInfo":{"status":"ok","timestamp":1710015081128,"user_tz":-180,"elapsed":2807,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"63008aa9-47d0-4cd3-e937-aa1f03c3083c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accelerate                               0.27.2\n","chromadb                                 0.4.24\n","datasets                                 2.18.0\n","faiss-cpu                                1.8.0\n","gradio                                   4.21.0\n","gradio_client                            0.12.0\n","langchain                                0.1.11\n","langchain-community                      0.0.27\n","langchain-core                           0.1.30\n","langchain-text-splitters                 0.0.1\n","llama_cpp_python                         0.2.56\n","pdfminer.six                             20231228\n","sentence-transformers                    2.5.1\n","tensorflow-datasets                      4.9.4\n","torch                                    2.1.0+cu121\n","torchaudio                               2.1.0+cu121\n","torchdata                                0.7.0\n","torchsummary                             1.5.1\n","torchtext                                0.16.0\n","torchvision                              0.16.0+cu121\n","vega-datasets                            0.9.0\n","youtube-transcript-api                   0.6.2\n"]}]},{"cell_type":"markdown","source":["Докер - папки для моделей должны быть созданы"],"metadata":{"id":"SVU1RHOtkSUd"}},{"cell_type":"code","source":["'''\n","FROM python:3.10\n","WORKDIR /app\n","COPY ./requirements.txt .\n","RUN pip install --no-cache-dir --upgrade pip\n","RUN pip install --no-cache-dir --upgrade -r requirements.txt\n","COPY . .\n","CMD [\"python\", \"app.py\"]\n","'''"],"metadata":{"id":"ZJB8ujWykT7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","docker build -t openchat .\n","docker run --rm -v ./models/:/app/models/ -v ./embed_models/:/app/embed_models/ -p 80:7860 openchat\n","'''"],"metadata":{"id":"aObRQUVXkVlt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Инициализация моделей"],"metadata":{"id":"YntTr3JhOIM0"}},{"cell_type":"markdown","source":["Модель LLM"],"metadata":{"id":"w2OS1EJaYR5F"}},{"cell_type":"code","source":["!mkdir ./models\n","!mkdir ./embed_models"],"metadata":{"id":"_SW8BSbikKv2","executionInfo":{"status":"ok","timestamp":1710073738188,"user_tz":-180,"elapsed":467,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8bd9b08-bf98-477b-871c-8272dc5e882b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘./models’: File exists\n","mkdir: cannot create directory ‘./embed_models’: File exists\n"]}]},{"cell_type":"code","source":["!wget -qq --show-progress -P ./models https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q2_K.gguf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HnJ8ISvymZr5","executionInfo":{"status":"ok","timestamp":1710073792224,"user_tz":-180,"elapsed":54038,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"3d53faea-2eec-4e3c-dfe4-18ce39d055ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model-q2_K.gguf     100%[===================>]   2.87G  61.6MB/s    in 53s     \n"]}]},{"cell_type":"code","source":["from llama_cpp import Llama\n","\n","model_path = './models/model-q2_K.gguf'\n","n_ctx = 2000\n","model = Llama(\n","    model_path=model_path,\n","    n_ctx=n_ctx,\n","    # n_gpu_layers=-1,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710073801464,"user_tz":-180,"elapsed":9248,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"3c597488-ffe1-438c-c4c2-eaccaae0645b","id":"b9k7D_XIOWxg"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/model-q2_K.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = models\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 10\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q2_K:   65 tensors\n","llama_model_loader: - type q3_K:  160 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32002\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 32768\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 32768\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q2_K - Medium\n","llm_load_print_meta: model params     = 7.24 B\n","llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n","llm_load_print_meta: general.name     = models\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.11 MiB\n","llm_load_tensors:        CPU buffer size =  2939.58 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 2000\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =   250.00 MiB\n","llama_new_context_with_model: KV self size  =  250.00 MiB, K (f16):  125.00 MiB, V (f16):  125.00 MiB\n","llama_new_context_with_model:        CPU input buffer size   =    12.93 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   157.00 MiB\n","llama_new_context_with_model: graph splits (measure): 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n","Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n","Using fallback chat format: None\n"]}]},{"cell_type":"markdown","source":["Модель эмбедингов"],"metadata":{"id":"KTp31GIsYTzb"}},{"cell_type":"code","source":["%%capture\n","from langchain_community.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n","\n","# статья рейтинг энкодеров https://habr.com/ru/articles/669674/\n","# репозиторий рейтинг энкодеров https://github.com/avidale/encodechka (RuElectra тогда еще не появилась)\n","\n","# # 3 модели RuElectra https://huggingface.co/ai-forever?search_models=ruElectra\n","# model_name = 'ai-forever/ruElectra-medium'  # small 174 MB medium 356 MB large 1.71 GB\n","\n","# https://huggingface.co/inkoziev/sbert_pq/tree/main\n","model_name = 'inkoziev/sbert_pq'  # 117 MB\n","\n","# # https://huggingface.co/cointegrated/rubert-tiny\n","# model_name = 'cointegrated/rubert-tiny'  # 50M усреднине эмбедингов по длине предложения\n","\n","# # https://huggingface.co/cointegrated/rubert-tiny2\n","# model_name = 'cointegrated/rubert-tiny2'  # 100M усреднине эмбедингов по длине предложения\n","\n","# # https://huggingface.co/cointegrated/LaBSE-en-ru\n","# model_name = 'cointegrated/LaBSE-en-ru'  # 516M # выход pooler (норм модель)\n","\n","# # https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n","# model_name = \"sentence-transformers/all-mpnet-base-v2\"  # только английский вроде\n","\n","# # https://huggingface.co/ai-forever/ruBert-large/tree/main\n","# # https://huggingface.co/ai-forever/ruBert-base\n","# model_name = 'ai-forever/ruBert-large'  # 1.71 Gb\n","# model_name = 'ai-forever/ruBert-base'  # 716 MB\n","\n","# # https://huggingface.co/ai-forever/sbert_large_nlu_ru\n","# https://huggingface.co/ai-forever/sbert_large_mt_nlu_ru\n","# model_name = \"ai-forever/sbert_large_nlu_ru\"  # 1.71 Gb  говорят что норм\n","# model_name = \"ai-forever/sbert_large_mt_nlu_ru\"  # 1.71 Gb  Multitask Learning версия обученная на 3 задачах (NLI, NER, Sentiment)\n","\n","# # https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n","# model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # 1 Gb еще вариант от сайги мистраль ру весит\n","\n","# # https://huggingface.co/intfloat/multilingual-e5-large\n","# model_name = 'intfloat/multilingual-e5-large'  # 2.24G еще вариант говорят норм мультиязычная\n","\n","# за технику усреднения отвечают флаги при выводе модели, например pooling_mode_mean_tokens стоит True\n","# SentenceTransformer обычно на выходе делают нормализацию эмбедингов torch.nn.functional.normalize(embeddings)\n","model_kwargs = {\"device\": \"cpu\"}\n","embed_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, cache_folder='./embed_models')"],"metadata":{"id":"EyZsF4MwOSLO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Приложение после загрузки моделей"],"metadata":{"id":"kCMdINogOMV4"}},{"cell_type":"markdown","source":["Вариант через полностью кастомные элементы"],"metadata":{"id":"tk40_fHSWy0J"}},{"cell_type":"code","source":["# ======================= КОНФИГ ===================================\n","LOADER_CLASSES = {\n","    '.csv': CSVLoader,  # \".csv\": (CSVLoader, {\"csv_args\": {\"delimiter\": \";\"}})\n","    '.doc': UnstructuredWordDocumentLoader,\n","    '.docx': UnstructuredWordDocumentLoader,\n","    '.html': UnstructuredHTMLLoader,\n","    '.md': UnstructuredMarkdownLoader,\n","    '.pdf': PDFMinerLoader,\n","    '.ppt': UnstructuredPowerPointLoader,\n","    '.pptx': UnstructuredPowerPointLoader,\n","    '.txt': TextLoader,\n","    'web': WebBaseLoader,\n","    'directory': DirectoryLoader,\n","    'youtube': YoutubeLoader,\n","}\n","\n","RETRIEVER_CLASSES = [BM25Retriever, FAISS, TFIDFRetriever]\n","RETRIEVER_NAMES = [cls.__name__ for cls in RETRIEVER_CLASSES]\n","SUBTITLES_LANGUAGES = [\"Russian\", \"English\"]\n","HISTORY_LEN = 0\n","DO_SAMPLE = False\n","\n","generate_kwargs = dict(\n","    top_k=40,  # 40\n","    top_p=0.9,  # 0.95\n","    temp=0.8,  # 0.8 (параметр называется temperature для __call__, temp для generate)\n","    repeat_penalty=1.0,  # 1.1\n","    )\n","\n","INTERFACE_DESCRIPTION = '''Добро пожаловать\n","Задайте вопрос для получения ответа от бота\n","Для настроек семплирования настройте параметры в Additional inputs\n","Для использования контекста перейдите во вкладку Load documents\n","'''\n","\n","TEMPLATES = {\n","    'Saiga mistral': {\n","        'SYSTEM_TEMPLATE': '''<s> system\n","Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\n","</s> ''',\n","        'USER_TEMPLATE': '<s> user\\n{message}\\n</s> ',\n","        'BOT_TEMPLATE': '<s> bot\\n{message}\\n</s> ',\n","        'ADD_BOS': False,\n","        },\n","\n","    'Openchat': {\n","        'SYSTEM_TEMPLATE': '',\n","        'USER_TEMPLATE': ' GPT4 Correct User: {message}<|end_of_turn|>',\n","        'BOT_TEMPLATE': ' GPT4 Correct Assistant: {message}<|end_of_turn|>',\n","        'ADD_BOS': False,\n","        },\n","\n","    'Mistral instruct': {\n","        'SYSTEM_TEMPLATE': '',\n","        'USER_TEMPLATE': ' [INST] {message} [/INST]',\n","        'BOT_TEMPLATE': '{message}</s>  ',\n","        'ADD_BOS': True,\n","        },\n","    }\n","\n","DEFAULT_TEMPLATE = TEMPLATES['Saiga mistral']\n","\n","CONTEXT_TEMPLATE = '''Дан контекст:\n","{context}\n","Дан вопрос:\n","{message}\n","Ответ:'''\n","\n","CSS = '''\n",".gradio-container {width: 80% !important}\n","'''\n","\n","# ================ ОЧИСТКА ТЕКСТА ====================================\n","def clear_text(text: str) -> str:\n","    lines = text.split('\\n')\n","    lines = [line for line in lines if len(line.strip()) > 2]\n","    text = '\\n'.join(lines).strip()\n","    return text\n","\n","def clear_documents(documents: List[Document]) -> List[Document]:\n","    output_documents = []\n","    for document in documents:\n","        text = clear_text(document.page_content)\n","        if len(text) > 10:\n","            document.page_content = text\n","            output_documents.append(document)\n","    return output_documents\n","\n","\n","# ====================== ДОП ФУНКЦИИ ЗАГРУЗКИ ФАЙЛОВ ===================\n","def get_csv_delimiter(file_path: str) -> str:\n","    n_bytes = 4096\n","    with open(file_path) as csvfile:\n","        delimiter = csv.Sniffer().sniff(csvfile.read(n_bytes)).delimiter\n","    return delimiter\n","\n","def load_documents_from_files(upload_files: List[str]) -> Tuple[List[Document], str]:\n","    load_log = ''\n","    documents = []\n","    for upload_file in upload_files:\n","        file_extension = f'.{upload_file.rsplit(\".\", 1)[-1]}'\n","        if file_extension in LOADER_CLASSES:\n","            loader_kwargs = {}\n","            loader_class = LOADER_CLASSES[file_extension]\n","            if file_extension == '.csv':\n","                delimiter = get_csv_delimiter(upload_file)\n","                loader_kwargs = {'csv_args': {'delimiter': delimiter}}\n","            try:\n","                load_documents = loader_class(upload_file, **loader_kwargs).load()\n","                documents.extend(load_documents)\n","            except Exception as ex:\n","                load_log += f'Ошибка загрузки файла: {upload_file}\\n'\n","                load_log += f'Код ошибки: {ex}\\n'\n","                continue\n","        else:\n","            load_log += f'Неподдерживаемый формат файла {upload_file}\\n'\n","            continue\n","    return documents, load_log\n","\n","\n","def load_documents_from_links(\n","        web_links: str,\n","        subtitles_lang: str,\n","        ) -> Tuple[List[Document], str]:\n","    load_log = ''\n","    documents = []\n","    loader_class_kwargs = {}\n","\n","    web_links = [web_link.strip() for web_link in web_links.split('\\n') if web_link.strip()]\n","    for web_link in web_links:\n","        # ----------------- ссылка на Ютуб ---------------------------------\n","        if 'youtube.com' in web_link:\n","            # проверка что субтитры на выбранном языке subtitles_lang доступны в видео web_link\n","            youtube_id = web_link.split('watch?v=')[-1].split('&')[0]\n","            available_langs = [t.language for t in list(YouTubeTranscriptApi.list_transcripts(youtube_id))]\n","            if subtitles_lang not in str(available_langs):\n","                load_log += f'Язык субтитров {subtitles_lang} недоступен для видео {web_link}\\n'\n","                continue\n","            if len(available_langs) == 1 and 'auto-generated' in str(available_langs):\n","                load_log += f'Загружены автоматические субтитры, ручные недоступны для видео {web_link}\\n'\n","            # если субтитры доступны то будем доставать их с помощью именованных аргументов\n","            loader_class = LOADER_CLASSES['youtube'].from_youtube_url\n","            language = subtitles_lang[:2].lower()\n","            loader_class_kwargs = {'language': language}\n","\n","        # ----------------- ссылка не на Ютуб -------------------------------\n","        else:\n","            loader_class = LOADER_CLASSES['web']\n","        try:\n","            if requests.get(web_link).status_code != 200:\n","                load_log += f'Ссылка недоступна для Python: {web_link}\\n'\n","                continue\n","            load_documents = loader_class(web_link, **loader_class_kwargs).load()\n","            documents.extend(load_documents)\n","        except MissingSchema:\n","            load_log += f'Неверная ссылка: {web_link}\\n'\n","            continue\n","        except Exception as ex:\n","            load_log += f'Ошибка загрузки лоадером данных по ссылке: {web_link}\\n'\n","            load_log += f'Код ошибки: {ex}\\n'\n","            continue\n","    return documents, load_log\n","\n","\n","# =================== ИНИЦИАЛИЗАЦИЯ РЕТРИВЕРА =========================\n","def create_retriver(\n","        documents: List[Document],\n","        retriever_classes: List[BaseRetriever],\n","        k: int,\n","        ) -> BaseRetriever:\n","    retrievers = []\n","    for retriever_class in retriever_classes:\n","        if retriever_class.__name__ in ('FAISS', 'Chroma'):\n","            db = retriever_class.from_documents(\n","                documents=documents,\n","                embedding=embed_model,\n","                )\n","            retriever = db.as_retriever(search_kwargs={'k': k})\n","        else:\n","            retriever = retriever_class.from_documents(documents=documents)\n","            retriever.k = k\n","        retrievers.append(retriever)\n","    if len(retrievers) == 1:\n","        final_retriver = retrievers[0]\n","    else:\n","        weights = [0.5] * len(retrievers)\n","        final_retriver = EnsembleRetriever(retrievers=retrievers, weights=weights)\n","    return final_retriver\n","\n","\n","# ============= ГЛАВНАЯ ФУНКЦИЯ ОКНА ЗАГРУЗКИ ======================\n","def load_documents_and_create_retriver(\n","        upload_files: Optional[List[str]],\n","        web_links: str,\n","        subtitles_lang: str,\n","        chunk_size: int,\n","        chunk_overlap: int,\n","        k: Union[int, str],\n","        retriever_indexes: List[int],\n","        ) -> Tuple[List[Document], Optional[BaseRetriever], str]:\n","\n","    documents = []\n","    retriever = None\n","\n","    if not retriever_indexes:\n","        load_log = 'Не выбран ретривер'\n","        return documents, retriever, load_log\n","\n","    if upload_files is None and not web_links:\n","        load_log = 'Не выбраны файлы или ссылки'\n","        return documents, retriever, load_log\n","\n","    progress = gr.Progress()\n","    all_documents = []\n","    load_log = ''\n","\n","    if upload_files is not None:\n","        progress(0.3, desc='Шаг 1/2: Загрузка документов из файлов')\n","        docs, log = load_documents_from_files(upload_files)\n","        all_documents.extend(docs)\n","        load_log += log\n","\n","    if web_links:\n","        progress(0.3 if upload_files is None else 0.5, desc='Шаг 1/2: Загрузка документов по ссылкам')\n","        docs, log = load_documents_from_links(web_links, subtitles_lang)\n","        all_documents.extend(docs)\n","        load_log += log\n","\n","    if len(all_documents) == 0:\n","        load_log += 'Загрузка прервана так как не было извлечено ни одного документа\\n'\n","        load_log += 'Режим RAG не может быть активирован'\n","        return documents, retriever, load_log\n","\n","    load_log += f'Загружено документов: {len(all_documents)}\\n'\n","\n","    if k == 'max':\n","        documents = clear_documents(all_documents)\n","        load_log += f'Используются документы без разделения в кол-ве: {len(documents)}\\n'\n","        k = 1\n","    else:\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=chunk_size,\n","            chunk_overlap=chunk_overlap,\n","        )\n","        documents = text_splitter.split_documents(all_documents)\n","        documents = clear_documents(documents)\n","        load_log += f'Документы разделены, кол-во фрагментов текста: {len(documents)}\\n'\n","\n","    progress(0.7, desc='Шаг 2/2: Инициализация ретривера')\n","\n","    retriever_classes = [RETRIEVER_CLASSES[i] for i in retriever_indexes]\n","    retriever = create_retriver(documents, retriever_classes, k)\n","\n","    load_log += 'Режим RAG активирован и может быть дективирован на вкладке Generate'\n","    return documents, retriever, load_log\n","\n","\n","# ================== ГЛАВНАЯ ФУНКЦИЯ ОКНА БОТА ==========================\n","\n","def get_promt_with_context(\n","        user_message: str,\n","        chatbot: List[List[Optional[str]]],\n","        history_len: int,\n","        rag_mode: bool,\n","        retriever: BaseRetriever,\n","        bos_token: str,\n","        context_template,\n","        *template_components: List[str],\n","        ):\n","\n","    # user_message = user_message.strip()  # ?\n","    chatbot.append([user_message, None])\n","    if not user_message:\n","        return '', chatbot, full_promt\n","\n","    CONTEXT_TEMPLATE = context_template\n","    SYSTEM_TEMPLATE, USER_TEMPLATE, BOT_TEMPLATE, ADD_BOS = template_components\n","\n","    full_promt = ''\n","    if SYSTEM_TEMPLATE:\n","        full_promt += SYSTEM_TEMPLATE\n","\n","    if ADD_BOS:\n","        full_promt += bos_token\n","\n","    if history_len != 0:\n","        for user_msg, bot_msg in chatbot[-history_len:]:\n","            full_promt += USER_TEMPLATE.format(message=user_msg)\n","            full_promt += BOT_TEMPLATE.format(message=bot_msg)\n","\n","    if retriever is not None and rag_mode:\n","        retriever_docs = retriever.invoke(user_message)\n","        retriever_context = '\\n'.join([doc.page_content for doc in retriever_docs])\n","        user_message = CONTEXT_TEMPLATE.format(\n","            message=user_message,\n","            context=retriever_context,\n","            )\n","\n","    full_promt += USER_TEMPLATE.format(message=user_message)\n","    full_promt += BOT_TEMPLATE.split('{message}')[0]\n","    return '', chatbot, full_promt\n","\n","def generate_text(\n","        chatbot: List[List[Optional[str]]],\n","        full_promt: str,\n","        history_len: int,\n","        do_sample: bool,\n","        *generate_args: List[Union[int, float]],\n","        ):\n","    if chatbot[-1][0] == '':\n","        yield chatbot[:-1]\n","    else:\n","        promt = full_promt\n","        promt_tokens = model.tokenize(promt.encode('utf-8'), special=True, add_bos=False)\n","        gen_kwargs = dict(zip(generate_kwargs.keys(), generate_args))\n","        if not do_sample:\n","            gen_kwargs['top_k'] = 1\n","            gen_kwargs['repeat_penalty'] = 1\n","\n","        # generator = 'И тебе привет'\n","        chatbot[-1][1] = ''\n","        generator = model.generate(promt_tokens, **gen_kwargs)\n","        for token in generator:\n","            if token == model.token_eos():\n","                break\n","            character = model.detokenize([token]).decode('utf-8', errors='ignore')\n","            # character = token\n","            chatbot[-1][1] += character\n","            yield chatbot\n","\n","\n","# =================== ИНТЕРФЕЙС ПРИЛОЖЕНИЯ ===========================\n","# gr.themes.Glass() Glass Soft Monochrome Default Base\n","with gr.Blocks(theme=gr.themes.Monochrome(), css=CSS) as demo:\n","    documents = gr.State([])\n","    retriever = gr.State(None)\n","    full_promt = gr.State('')\n","\n","    def get_chatbot(chatbot_history: list = [], rag_mode_label: bool = False):\n","        label = 'RAG' if rag_mode_label else 'Chatbot'\n","        chatbot = gr.Chatbot(\n","                value=chatbot_history,\n","                scale=0,\n","                min_width=100,  # 160\n","                height=300,\n","                show_copy_button=True,\n","                bubble_full_width=False,\n","                label=label,\n","                )\n","        return chatbot\n","\n","    # ==================== СТРАНИЦА БОТА =================================\n","\n","    with gr.Tab(label='Generate'):\n","        # chatbot_title = gr.Markdown('Чат бот в режиме RAG')\n","        with gr.Row():\n","            with gr.Column(scale=3):\n","                chatbot = get_chatbot()\n","                user_message = gr.Textbox(label='User')\n","                with gr.Row():\n","                    user_message_btn = gr.Button('Отправить')\n","                    stop_btn = gr.Button('Стоп')\n","                    clear_btn = gr.Button('Очистить чат')\n","\n","                rag_mode = gr.Checkbox(value=False, label='Режим RAG', scale=1, visible=False)\n","                rag_mode.change(fn=get_chatbot, inputs=[chatbot, rag_mode], outputs=chatbot)\n","\n","            # ------------------ ПАРАМЕТРЫ ГЕНЕРАЦИИ -------------------------\n","            def get_generate_args(do_sample: bool):\n","                visible = do_sample\n","                generate_args = [\n","                    gr.Slider(5, 50, generate_kwargs['top_k'], 5, 'top_k', visible=visible),\n","                    gr.Slider(0, 1, generate_kwargs['top_p'], 0.1, 'top_p', visible=visible),\n","                    gr.Slider(0.1, 4, generate_kwargs['temp'], 0.1, 'temp', visible=visible),\n","                    gr.Slider(1, 5, generate_kwargs['repeat_penalty'], 0.1, 'repeat_penalty', visible=visible),\n","                ]\n","                return generate_args\n","\n","            with gr.Column(scale=1, min_width=80):\n","                with gr.Group():\n","                    gr.Markdown('Длина истории')\n","                    history_len = gr.Slider(\n","                        minimum=0,\n","                        maximum=5,\n","                        value=HISTORY_LEN,\n","                        step=1,\n","                        info='Кол-во предыдущих сообщенией, учитываемых в истории',\n","                        label='history len',\n","                        show_label=False,\n","                        )\n","\n","                    with gr.Group():\n","                        gr.Markdown('Параметры генерации')\n","                        do_sample = gr.Checkbox(value=DO_SAMPLE, label='do_sample')\n","                        generate_args = get_generate_args(do_sample.value)\n","\n","                    do_sample.change(\n","                        fn=get_generate_args,\n","                        inputs=do_sample,\n","                        outputs=generate_args,\n","                        show_progress=False,\n","                        )\n","\n","        # ------------------ ТЕКСТ ПРОМТА С КОНТЕКСТОМ --------------------\n","        full_promt = gr.Textbox(\n","            label='Полный текст текущего промта с контекстом',\n","            interactive=False,\n","            )\n","\n","        # -------- КОМПОНЕНТЫ РЕДАКТИРОВАНИЯ ПРОМТОВ И СПЕЦ ТОКЕНОВ --------\n","        def get_bos_token(visible=False, render=True):\n","            bos_token_component = gr.Textbox(\n","                value='<s>',\n","                label='Bos token',\n","                visible=visible,\n","                render=render,\n","                )\n","            return bos_token_component\n","\n","        # добавлять токен {BOS} только один раз вначале шаблона юзера\n","        add_bos = gr.Checkbox(\n","            value=False,\n","            label='add bos',\n","            scale=1,\n","            render=False,\n","            info='Добавлять ли токен BOS один раз вначале диалога перед промтом пользователя',\n","            )\n","        bos_token = get_bos_token(visible=False, render=False)\n","        add_bos.change(\n","            fn=get_bos_token,\n","            inputs=add_bos,\n","            outputs=bos_token,\n","            show_progress=False,\n","            )\n","\n","        system_template = gr.Textbox(\n","            DEFAULT_TEMPLATE['SYSTEM_TEMPLATE'],\n","            label='System template',\n","            lines=4,\n","            render=False,\n","            )\n","        user_template = gr.Textbox(\n","            DEFAULT_TEMPLATE['USER_TEMPLATE'],\n","            label='User template',\n","            lines=4,\n","            render=False,\n","            )\n","        bot_template = gr.Textbox(\n","            DEFAULT_TEMPLATE['BOT_TEMPLATE'],\n","            label='Bot template',\n","            lines=4,\n","            render=False,\n","            )\n","        template_components = [system_template, user_template, bot_template, add_bos]\n","\n","        context_template = gr.Textbox(\n","            CONTEXT_TEMPLATE,\n","            label='Context template',\n","            lines=6,\n","            render=False,\n","            )\n","\n","        # ------------------ КНОПКИ ОТПРАВИТЬ ОЧИСТИТЬ И СТОП ------------\n","        # нажатие Enter и кнопка отправить\n","        generate_event = gr.on(\n","            triggers=[user_message.submit, user_message_btn.click],\n","            fn=get_promt_with_context,\n","            inputs=[user_message, chatbot, history_len, rag_mode, retriever,\n","            bos_token, context_template, *template_components],\n","            outputs=[user_message, chatbot, full_promt],\n","            queue=True,\n","        ).then(\n","            fn=lambda promt: promt,  # попробовать поставить lambda: full_promt\n","            inputs=full_promt,\n","            outputs=full_promt,\n","            queue=False,\n","        ).then(\n","            fn=generate_text,\n","            inputs=[chatbot, full_promt, history_len, do_sample, *generate_args],\n","            outputs=chatbot,\n","            queue=True,\n","            )\n","        # кнопка Стоп\n","        stop_btn.click(\n","            fn=None,\n","            inputs=None,\n","            outputs=None,\n","            cancels=generate_event,\n","            queue=False,\n","        )\n","        # кнопка Очистить чат\n","        clear_btn.click(\n","            fn=lambda: (None, ''),\n","            inputs=None,\n","            outputs=[chatbot, full_promt],\n","            queue=False,\n","            )\n","\n","    # ===================== СТРАНИЦА ЗАГРУЗКИ ФАЙЛОВ =========================\n","\n","    def get_chunk_size_overlap(k: int = 2):\n","        visible = k != 'max'\n","        chunk_size = gr.Slider(50, 2000, value=500, step=50, label='Длина фрагментов', visible=visible)\n","        chunk_overlap = gr.Slider(0, 200, value=20, step=10, label=\"Длина пересечения фрагментов\", visible=visible)\n","        return chunk_size, chunk_overlap\n","\n","    with gr.Tab(label='Load documents'):\n","        with gr.Row():\n","            upload_files = gr.File(file_count='multiple', label='Загрузка текстовых файлов')\n","            web_links = gr.Textbox(lines=8, label='Ссылки на Web сайты или Ютуб')\n","        with gr.Row():\n","            with gr.Column(scale=2):\n","                retriever_indexes = gr.CheckboxGroup(\n","                    choices=RETRIEVER_NAMES,\n","                    value=RETRIEVER_NAMES[0],\n","                    type='index',\n","                    label='Ретривер',\n","                    )\n","                with gr.Row():\n","                    k = gr.Radio(\n","                        [1, 2, 3, 'max'],\n","                        value=2,\n","                        label='Количество релевантных документов для поиска',\n","                        )\n","                    subtitles_lang = gr.Radio(\n","                        SUBTITLES_LANGUAGES,\n","                        value=SUBTITLES_LANGUAGES[0],\n","                        label=\"Язык субтитров YouTube\",\n","                        )\n","\n","            with gr.Column(scale=1, min_width=160):\n","                chunk_size, chunk_overlap = get_chunk_size_overlap()\n","                k.change(\n","                    get_chunk_size_overlap,\n","                    inputs=k,\n","                    outputs=[chunk_size, chunk_overlap],\n","                    show_progress=False,\n","                    )\n","\n","        load_documents_btn = gr.Button(value='Загрузить документы и создать ретривер')\n","        load_docs_log = gr.Textbox(label='Прогресс загрузки и разделения документов')\n","\n","        load_event = load_documents_btn.click(\n","            fn=load_documents_and_create_retriver,\n","            inputs=[upload_files, web_links, subtitles_lang, chunk_size,\n","                    chunk_overlap, k, retriever_indexes],\n","            outputs=[documents, retriever, load_docs_log],\n","            )\n","\n","        def load_success(chatbot_history, retriever):\n","            rag_mode = retriever is not None\n","            chatbot = get_chatbot(chatbot_history, rag_mode)\n","            rag_mode_checkbox = gr.Checkbox(value=rag_mode, label='Режим RAG', scale=1, visible=rag_mode)\n","            return chatbot, rag_mode_checkbox\n","\n","        load_event.success(load_success, [chatbot, retriever], [chatbot, rag_mode])\n","\n","    # ================= СТРАНИЦА ПРОСМОТРА ВСЕХ ДОКУМЕНТОВ =================\n","\n","    with gr.Tab(label='View documents'):\n","        view_documents_btn = gr.Button(value='Отобразить загруженные фрагменты')\n","        view_documents_textbox = gr.Textbox(\n","            lines=1,\n","            placeholder='Для просмотра фрагментов загрузите документы ан вкладке Load documents',\n","            label='Загруженные фрагменты',\n","            )\n","        sep = '=' * 20\n","        view_documents_btn.click(\n","            lambda documents: f'\\n{sep}\\n\\n'.join([doc.page_content for doc in documents]),\n","            inputs=documents,\n","            outputs=view_documents_textbox,\n","        )\n","\n","    # ================= СТРАНИЦА РЕДАКТИРОВАНИЯ ШАБЛОНА ПРОМТА ===============\n","\n","    def select_template(template_name):\n","        template = list(TEMPLATES[template_name].values())\n","        return template\n","\n","    def get_example_prompt(bos_token, *template_components):\n","        user_message = 'Как дела?'\n","        chatbot_history = [['Привет', 'Здорова']]\n","\n","        _, _, example_prompt = get_promt_with_context(\n","            user_message, chatbot_history, 1, False, None,\n","            bos_token, '', *template_components)\n","\n","        example_prompt = gr.Textbox(\n","            value=example_prompt,\n","            label='Example prompt',\n","            lines=example_prompt.count('\\n'),\n","            )\n","        return example_prompt\n","\n","    with gr.Tab(label='Edit pomt templates'):\n","        with gr.Group():\n","            gr.Markdown('Готовые шаблоны по умолчанию')\n","            default_template = gr.Dropdown(\n","                choices=list(TEMPLATES.keys()),\n","                value=None,\n","                label='Default Templates',\n","                )\n","            default_template.change(\n","                fn=select_template,\n","                inputs=default_template,\n","                outputs=[*template_components],\n","            )\n","\n","        with gr.Group():\n","            gr.Markdown('Настройки форматирования промтов перед подачей в модель')\n","            with gr.Row():\n","                add_bos.render()\n","                bos_token.render()\n","\n","        with gr.Row():\n","            with gr.Column():\n","                system_template.render()\n","                user_template.render()\n","                bot_template.render()\n","\n","            with gr.Column():\n","                example_prompt = get_example_prompt(\n","                    bos_token.value,\n","                    *[c.value for c in template_components],\n","                    )\n","\n","        gr.on(\n","            triggers=[c.change for c in [add_bos, bos_token, *template_components]],\n","            fn=get_example_prompt,\n","            inputs=[bos_token, *template_components],\n","            outputs=[example_prompt],\n","            queue=False,\n","            )\n","\n","        with gr.Group():\n","            gr.Markdown('Настройки форматирования промта при условии контекста (RAG)')\n","            context_template.render()\n","\n","demo.queue().launch(debug=True, width='70%', height=700)"],"metadata":{"id":"peYmXY4vx-ve","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710073947660,"user_tz":-180,"elapsed":17,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"b5c6d0ea-17b0-49dc-bdeb-04ae1d7fc028"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://24d4cad74e6bb66528.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["## Страница загрузки моделей"],"metadata":{"id":"XOD0DtRFac5U"}},{"cell_type":"markdown","source":["Страница загрузки моделей отдельно  \n","API HF Python  \n","https://huggingface.co/docs/huggingface_hub/guides/repository"],"metadata":{"id":"hLNrgfAN4pRr"}},{"cell_type":"code","source":["EMBED_MODELS_PATH = Path('embed_models')\n","MODELS_PATH = Path('models')\n","\n","EMBEDERS_IDS = [\n","    # https://huggingface.co/cointegrated/rubert-tiny2\n","    'cointegrated/rubert-tiny2',\n","    # ttps://huggingface.co/cointegrated/rubert-tiny\n","    'cointegrated/rubert-tiny',\n","    # https://huggingface.co/cointegrated/LaBSE-en-ru\n","    'cointegrated/LaBSE-en-ru',\n","    # https://huggingface.co/intfloat/multilingual-e5-large\n","    'intfloat/multilingual-e5-large',\n","    # https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n","    'sentence-transformers/all-mpnet-base-v2',\n","    # https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n","    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n","    # https://huggingface.co/inkoziev/sbert_pq/tree/main\n","    'inkoziev/sbert_pq',\n","    # https://huggingface.co/ai-forever?search_models=ruElectra\n","    'ai-forever/ruElectra-medium',\n","    # https://huggingface.co/ai-forever/sbert_large_nlu_ru\n","    'ai-forever/sbert_large_nlu_ru',\n","]\n","\n","MODEL_IDS = [\n","    'IlyaGusev/saiga_mistral_7b_gguf',\n","    'TheBloke/openchat-3.5-0106-GGUF',\n","    'TheBloke/Mistral-7B-Instruct-v0.1-GGUF',\n","]\n","\n","\n","def get_memory_usage():\n","    '''Печатает кол-во свободной ОЗУ и свободной видеопамяти (если видеокарта она доступна)'''\n","    memory_type = 'CPU'\n","    psutil_stats = psutil.virtual_memory()\n","    memory_usage = psutil_stats.used / 1024**3\n","    memory_total = psutil_stats.total / 1024**3\n","    print_memory = f'{memory_type} Menory Usage: {memory_usage:.2f} / {memory_total:.2f} GB'\n","\n","    if torch.cuda.is_available():\n","        memory_type = 'GPU'\n","        memory_free, memory_total = torch.cuda.mem_get_info()\n","        memory_usage = total_memory - free_memory\n","        print_memory += f'{memory_type} Menory Usage: {memory_usage / 1024**3:.2f} / {memory_total:.2f} GB'\n","\n","    print_memory = f'---------------\\n{print_memory}\\n---------------\\n'\n","    return print_memory\n","\n","\n","def clear_memory():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\n","def load_model(model_id, model_file, n_ctx):\n","    model = None\n","    if isinstance(model_file, list):\n","        load_log = 'Не выбрана модель'\n","        return model, load_log\n","\n","    load_log = ''\n","    if '(' in model_file:\n","        model_file = model_file.split('(')[0].rstrip()\n","\n","    progress = gr.Progress()\n","    progress(0.3, desc='Шаг 1/2: Загрузка модели GGUF')\n","    model_path = MODELS_PATH / model_file\n","    if model_path.is_file():\n","        load_log += 'Модель уже загружена, повторная инициализация\\n'\n","    else:\n","        try:\n","            hf_hub_download(\n","                repo_id=model_id,\n","                filename=model_file,\n","                local_dir=MODELS_PATH,\n","                )\n","            load_log += 'Модель успешно загружена\\n'\n","        except Exception as ex:\n","            model_path = ''\n","            load_log += f'Ошибка загрузки модели, код ошибки:\\n{ex}\\n'\n","\n","    if model_path:\n","        progress(0.7, desc='Шаг 2/2: Инициализация модели')\n","        try:\n","            model = Llama(model_path=str(model_path), n_ctx=n_ctx)\n","            load_log += f'Модель {model_id}/{model_file} инициализирована\\n'\n","        except Exception as ex:\n","            load_log += f'Ошибка инициализации модели, код ошибки:\\n{ex}\\n'\n","\n","    clear_memory()\n","    model = {'model': model}\n","    return model, load_log\n","\n","\n","def load_embed_model(model_id):\n","    embed_model = None\n","    if isinstance(model_id, list):\n","        load_log = 'Не выбрана модель'\n","        return embed_model, load_log\n","\n","    progress = gr.Progress()\n","    load_log = ''\n","    folder_name = model_id.replace('/', '_')\n","    if Path(folder_name).is_dir():\n","        load_log += f'Модель {model_id} уже загружена, повторная инициализация\\n'\n","        progress(0.5, desc='Шаг 1/1: Инициализация загруженной модели модели')\n","    else:\n","        progress(0.5, desc='Шаг 1/1: Загрузка и инициализация модели')\n","\n","    model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","    embed_model = HuggingFaceEmbeddings(\n","        model_name=model_id,\n","        model_kwargs=model_kwargs,\n","        cache_folder=str(EMBED_MODELS_PATH),\n","        )\n","    load_log += f'Модель эмбедингов {model_id} успешно загружена и инициализирована\\n'\n","\n","    clear_memory()\n","    embed_model = {'embed_model': embed_model}\n","    return embed_model, load_log\n","\n","\n","# загрузка моделей\n","start_model = load_model(MODEL_IDS[0], 'model-q2_K.gguf', 2000)[0]['model']\n","start_embed_model = load_embed_model(EMBEDERS_IDS[0])[0]['embed_model']\n","\n","\n","def add_new_model_id(new_model_id: str, model_ids):\n","    load_log = ''\n","    model_id = new_model_id.strip()\n","    if model_id:\n","        model_id = model_id.split('/')[-2:]\n","        if len(model_id) == 2:\n","            model_id = '/'.join(model_id).split('?')[0]\n","            if repo_exists(model_id) and model_id not in model_ids:\n","                if any([file_name.endswith('.gguf') for file_name in list_repo_files(model_id)]):\n","                    model_ids.insert(0, model_id)\n","                    load_log += f'Репозиторий модели {model_id} успешно добавлен\\n'\n","                else:\n","                    load_log += f'Не найдены модели GGUF в репозитории {model_id}\\n'\n","            else:\n","                load_log += 'Неверное название репозитория HF\\n'\n","        else:\n","            load_log += 'Неверная ссылка на репозиторий HF\\n'\n","    else:\n","        load_log += 'Пустая строка в поле репозитория HF\\n'\n","    model_id_dropdown = gr.Dropdown(choices=model_ids, value=model_ids[0])\n","    return model_id_dropdown, load_log\n","\n","\n","def get_model_paths(model_id):\n","    load_log = ''\n","    repo_files = list(list_repo_tree(model_id))\n","    repo_files = [file for file in repo_files if file.path.endswith('.gguf')]\n","    model_paths = [f'{file.path} ({file.size / 1000 ** 3:.2f}G)' for file in repo_files]\n","\n","    model_paths_dropdown = gr.Dropdown(\n","        choices=model_paths,\n","        value=model_paths[0],\n","        label='Файл модели GGUF',\n","        )\n","    return model_paths_dropdown\n","\n","\n","def clear_folder(ignore_link):\n","    folder = EMBED_MODELS_PATH\n","    if len(ignore_link.split('/')) != 2:\n","        folder = MODELS_PATH\n","        if '(' in ignore_link:\n","            ignore_link = ignore_link.split('(')[0].rstrip()\n","\n","    for path in folder.iterdir():\n","        if path.name == ignore_link:\n","            continue\n","        if path.is_file():\n","            path.unlink()\n","        elif path.is_dir():\n","            rmtree(path)\n","\n","\n","with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n","    # ================= ЗАГРУЗКА GGUF МОДЕЛЕЙ ============================\n","    with gr.Tab('Load model'):\n","        model_ids_state = gr.State(MODEL_IDS)\n","        model = gr.State({'model': start_model})\n","\n","        new_model_id = gr.Textbox(\n","            value='',\n","            label='Добавить репозиторий',\n","            placeholder='Ссылка на репозиторий HF моделей в формате GGUF',\n","            )\n","        new_model_btn = gr.Button('Добавить репозиторий')\n","\n","        model_id = gr.Dropdown(\n","            choices=MODEL_IDS,\n","            value=None,\n","            label='Репозиторий модели HF',\n","            )\n","        model_path = gr.Dropdown(\n","            choices=[],\n","            value=None,\n","            label='Файл модели GGUF',\n","            )\n","\n","        n_ctx = gr.Slider(500, 500 * 8, step=500, label='n_ctx')\n","        load_model_btn = gr.Button('Загрука и инициализация модели')\n","        load_model_log = gr.Textbox(\n","            value='Модель IlyaGusev/saiga_mistral_7b_gguf/model-q2_K.gguf загружена по умолчанию',\n","            label='Статус загрузки модели',\n","            )\n","\n","        with gr.Group():\n","            gr.Markdown('Освободить место на диске путем удаления всех моделей кроме текущей')\n","            remove_models_btn = gr.Button('Очистить папку')\n","\n","        new_model_btn.click(\n","            fn=add_new_model_id,\n","            inputs=[new_model_id, model_ids_state],\n","            outputs=[model_id, load_model_log],\n","        ).success(\n","            fn=lambda: '',\n","            inputs=None,\n","            outputs=new_model_id,\n","        )\n","\n","        model_id.change(\n","            fn=get_model_paths,\n","            inputs=[model_id],\n","            outputs=[model_path],\n","        )\n","\n","        load_model_btn.click(\n","            fn=load_model,\n","            inputs=[model_id, model_path, n_ctx],\n","            outputs=[model, load_model_log],\n","            queue=True,\n","        ).success(\n","            fn=lambda log: log + get_memory_usage(),\n","            inputs=load_model_log,\n","            outputs=load_model_log,\n","        )\n","\n","        remove_models_btn.click(\n","            fn=clear_folder,\n","            inputs=[model_path],\n","            outputs=None,\n","        ).success(\n","            fn=lambda model: f'Модели кроме {model} удалены',\n","            inputs=model_path,\n","            outputs=None,\n","        )\n","\n","    # ================= ЗАГРУЗКА ЭМБЕДИНГ МОДЕЛЕЙ ========================\n","    with gr.Tab('Load embed model'):\n","        embed_ids_state = gr.State(EMBEDERS_IDS)\n","        embed_model = gr.State({'embed_model': start_embed_model})\n","\n","        new_embed_id = gr.Textbox(\n","            value='',\n","            label='Добавить репозиторий',\n","            placeholder='Ссылка на репозиторий модели HF',\n","            )\n","        new_embed_btn = gr.Button('Добавить репозиторий')\n","\n","        embed_id = gr.Dropdown(\n","            choices=EMBEDERS_IDS,\n","            value=None,\n","            label='Репозиторий модели HF',\n","            )\n","\n","        load_embed_btn = gr.Button('Загрука и инициализация модели')\n","        load_embed_log = gr.Textbox(\n","            value=f'Модель {EMBEDERS_IDS[0]} загружена по умолчанию',\n","            label='Статус загрузки модели',\n","            )\n","        with gr.Group():\n","            gr.Markdown('Освободить место на диске путем удаления всех моделей кроме текущей')\n","            remove_embed_models_btn = gr.Button('Очистить папку')\n","\n","        new_embed_btn.click(\n","            fn=add_new_model_id,\n","            inputs=[new_embed_id, embed_ids_state],\n","            outputs=[embed_id, load_embed_log],\n","        ).success(\n","            fn=lambda: '',\n","            inputs=None,\n","            outputs=new_embed_id,\n","        )\n","\n","        load_embed_btn.click(\n","            fn=load_embed_model,\n","            inputs=[embed_id],\n","            outputs=[embed_model, load_embed_log],\n","        ).success(\n","            fn=lambda log: log + get_memory_usage(),\n","            inputs=load_embed_log,\n","            outputs=load_embed_log,\n","        )\n","\n","        remove_embed_models_btn.click(\n","            fn=clear_folder,\n","            inputs=[embed_id],\n","            outputs=None,\n","        ).success(\n","            fn=lambda model: f'Модели кроме {model} удалены',\n","            inputs=embed_id,\n","            outputs=None,\n","        )\n","\n","demo.queue().launch(debug=True, width='70%', height=700)"],"metadata":{"id":"hNRfENoW3ap4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Страница About"],"metadata":{"id":"O9L1ihbfTr-U"}},{"cell_type":"code","source":["import gradio as gr\n","\n","markdown = '''\n","\n","'''\n","\n","CSS = '''\n",".gradio-container {width: 80% !important}\n","'''\n","\n","# gr.themes.Glass() Glass Soft Monochrome Default Base\n","with gr.Blocks(theme=gr.themes.Monochrome()) as demo:  # css=CSS\n","    gr.Markdown(markdown)\n","\n","demo.queue().launch(debug=True, width='70%', height=700)"],"metadata":{"id":"o3tGRcZ7JJYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QgCiWn7PTyGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5ldWtN3iTyLq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Итоговый вариант"],"metadata":{"id":"1myZYfwZLjSY"}},{"cell_type":"markdown","source":["### Импорты"],"metadata":{"id":"M1ajMs8FCn5H"}},{"cell_type":"code","source":["import gc\n","import psutil\n","from pathlib import Path\n","from shutil import rmtree\n","from collections import deque\n","from typing import List, Tuple, Dict, Union, Iterable, Optional, Any\n","import time\n","import csv\n","import os\n","\n","import requests\n","from requests.exceptions import MissingSchema\n","import torch\n","from huggingface_hub import hf_hub_download, list_repo_tree, list_repo_files, repo_info, repo_exists, snapshot_download\n","\n","from llama_cpp import Llama, llama\n","from youtube_transcript_api import YouTubeTranscriptApi\n","import gradio as gr\n","\n","from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","from langchain_community.vectorstores import FAISS, Chroma\n","from langchain_community.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n","from langchain_community.retrievers import BM25Retriever, TFIDFRetriever\n","from langchain.retrievers import EnsembleRetriever\n","\n","# Loader classes\n","from langchain_community.document_loaders import (\n","    CSVLoader,\n","    PDFMinerLoader,\n","    PyPDFLoader,\n","    TextLoader,\n","    UnstructuredEmailLoader,\n","    UnstructuredHTMLLoader,\n","    UnstructuredMarkdownLoader,\n","    UnstructuredODTLoader,\n","    UnstructuredPowerPointLoader,\n","    UnstructuredWordDocumentLoader,\n","    WebBaseLoader,\n","    YoutubeLoader,\n","    DirectoryLoader,\n",")\n","\n","# Annotations\n","from langchain_core.retrievers import BaseRetriever\n","from langchain.docstore.document import Document\n","from langchain_core.vectorstores import VectorStore\n","from langchain_core.embeddings import Embeddings"],"metadata":{"id":"La_7MhnKtnoT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Конфиг"],"metadata":{"id":"sakxtTA-vdNs"}},{"cell_type":"code","source":["# from pathlib import Path\n","\n","# from langchain_community.vectorstores import FAISS, Chroma\n","# from langchain_community.retrievers import BM25Retriever, TFIDFRetriever\n","\n","# from langchain_community.document_loaders import (\n","#     CSVLoader,\n","#     PDFMinerLoader,\n","#     PyPDFLoader,\n","#     TextLoader,\n","#     UnstructuredEmailLoader,\n","#     UnstructuredHTMLLoader,\n","#     UnstructuredMarkdownLoader,\n","#     UnstructuredODTLoader,\n","#     UnstructuredPowerPointLoader,\n","#     UnstructuredWordDocumentLoader,\n","#     WebBaseLoader,\n","#     YoutubeLoader,\n","#     DirectoryLoader,\n","# )\n","\n","\n","LOADER_CLASSES = {\n","    '.csv': CSVLoader,  # \".csv\": (CSVLoader, {\"csv_args\": {\"delimiter\": \";\"}})\n","    '.doc': UnstructuredWordDocumentLoader,\n","    '.docx': UnstructuredWordDocumentLoader,\n","    '.html': UnstructuredHTMLLoader,\n","    '.md': UnstructuredMarkdownLoader,\n","    '.pdf': PDFMinerLoader,\n","    '.ppt': UnstructuredPowerPointLoader,\n","    '.pptx': UnstructuredPowerPointLoader,\n","    '.txt': TextLoader,\n","    'web': WebBaseLoader,\n","    'directory': DirectoryLoader,\n","    'youtube': YoutubeLoader,\n","}\n","\n","# список классов ретривера либо классов БД с методом as_retriever()\n","RETRIEVER_CLASSES = [BM25Retriever, FAISS, TFIDFRetriever]\n","RETRIEVER_NAMES = [cls.__name__ for cls in RETRIEVER_CLASSES]\n","SUBTITLES_LANGUAGES = [\"Russian\", \"English\"]\n","\n","# начальные настройки бота при первом запуске\n","HISTORY_LEN = 0\n","\n","GENERATE_KWARGS = dict(\n","    top_k=40,  # 40 default\n","    top_p=0.9,  # 0.95 default\n","    temp=0.8,  # 0.8 default\n","    repeat_penalty=1.0,  # 1.1 default\n","    )\n","\n","# шаблоны промтов для юзера, бота и системный\n","# ADD_BOS - добавлять ли токен BOS один раз вначале промта юзера\n","TEMPLATES = {\n","    'Openchat': {\n","        'SYSTEM_TEMPLATE': '',\n","        'USER_TEMPLATE': ' GPT4 Correct User: {message}<|end_of_turn|>',\n","        'BOT_TEMPLATE': ' GPT4 Correct Assistant: {message}<|end_of_turn|>',\n","        'ADD_BOS': False,\n","        },\n","\n","    'Saiga mistral': {\n","        'SYSTEM_TEMPLATE': '''<s> system\n","Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\n","</s> ''',\n","        'USER_TEMPLATE': '<s> user\\n{message}\\n</s> ',\n","        'BOT_TEMPLATE': '<s> bot\\n{message}\\n</s> ',\n","        'ADD_BOS': False,\n","        },\n","\n","    'Mistral instruct': {\n","        'SYSTEM_TEMPLATE': '',\n","        'USER_TEMPLATE': ' [INST] {message} [/INST]',\n","        'BOT_TEMPLATE': '{message}</s>  ',\n","        'ADD_BOS': True,\n","        },\n","    }\n","\n","# шаблон промта при условии контекста\n","CONTEXT_TEMPLATE = '''Дан контекст:\n","{context}\n","Дан вопрос:\n","{message}\n","Ответ:'''\n","\n","CSS = '''\n",".gradio-container {width: 80% !important}\n","'''\n","\n","# пути куда будут скачиваться модели\n","MODELS_PATH = Path('models')\n","EMBED_MODELS_PATH = Path('embed_models')\n","\n","MODELS_PATH.mkdir(exist_ok=True)\n","EMBED_MODELS_PATH.mkdir(exist_ok=True)\n","\n","# список готовых моделей эмбедингов\n","EMBEDERS_IDS = [\n","    # https://huggingface.co/cointegrated/rubert-tiny2  # 118 MB\n","    'cointegrated/rubert-tiny2',\n","    # https://huggingface.co/cointegrated/LaBSE-en-ru  # 438 MB\n","    'cointegrated/LaBSE-en-ru',\n","    # https://huggingface.co/intfloat/multilingual-e5-large  # 2.24 GB\n","    'intfloat/multilingual-e5-large',\n","    # https://huggingface.co/intfloat/multilingual-e5-base  # 1.11 GB\n","    'intfloat/multilingual-e5-base',\n","    # https://huggingface.co/intfloat/multilingual-e5-small  # 471 MB\n","    'intfloat/multilingual-e5-small',\n","    # https://huggingface.co/sentence-transformers/all-mpnet-base-v2  # 438 MB\n","    'sentence-transformers/all-mpnet-base-v2',\n","    # https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2  # 1.11 GB\n","    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n","    # https://huggingface.co/inkoziev/sbert_pq/tree/main  # 438 MB\n","    'inkoziev/sbert_pq',\n","    # https://huggingface.co/ai-forever?search_models=ruElectra  # 117 MB\n","    'ai-forever/ruElectra-medium',\n","    # https://huggingface.co/ai-forever/sbert_large_nlu_ru  # 1.71 GB\n","    'ai-forever/sbert_large_nlu_ru',\n","]\n","\n","# список готовых моделей GGUF\n","MODEL_IDS = [\n","    # https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\n","    'TheBloke/openchat-3.5-0106-GGUF',\n","    # https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf\n","    'IlyaGusev/saiga_mistral_7b_gguf',\n","    # https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n","    'TheBloke/Mistral-7B-Instruct-v0.1-GGUF',\n","]"],"metadata":{"id":"3scSnZZet5h_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Функции"],"metadata":{"id":"eQlfLPOCtnzO"}},{"cell_type":"code","source":["# import gc\n","# import psutil\n","# from pathlib import Path\n","# from shutil import rmtree\n","# from typing import List, Tuple, Dict, Union, Iterable, Optional, Any\n","# import csv\n","\n","# import requests\n","# from requests.exceptions import MissingSchema\n","# import torch\n","# from huggingface_hub import hf_hub_download, list_repo_tree, list_repo_files, repo_info, repo_exists, snapshot_download\n","# from llama_cpp import Llama\n","# from youtube_transcript_api import YouTubeTranscriptApi\n","# import gradio as gr\n","\n","# from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","# from langchain_community.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n","# from langchain.retrievers import EnsembleRetriever\n","\n","# # Annotations\n","# from langchain_core.retrievers import BaseRetriever\n","# from langchain.docstore.document import Document\n","# from langchain_core.vectorstores import VectorStore\n","# from langchain_core.embeddings import Embeddings\n","\n","# from config import (\n","#     MODELS_PATH,\n","#     EMBED_MODELS_PATH,\n","#     LOADER_CLASSES,\n","#     RETRIEVER_CLASSES,\n","#     GENERATE_KWARGS,\n","#     )\n","\n","\n","# получение количества свободной памяти на диске, CPU и GPU\n","def get_memory_usage() -> str:\n","    print_memory = ''\n","\n","    memory_type = 'Disk'\n","    psutil_stats = psutil.disk_usage('.')\n","    memory_total = psutil_stats.total / 1024**3\n","    memory_usage = psutil_stats.used / 1024**3\n","    print_memory += f'{memory_type} Menory Usage: {memory_usage:.2f} / {memory_total:.2f} GB\\n'\n","\n","    memory_type = 'CPU'\n","    psutil_stats = psutil.virtual_memory()\n","    memory_total = psutil_stats.total / 1024**3\n","    memory_usage =  memory_total - (psutil_stats.available / 1024**3)\n","    print_memory += f'{memory_type} Menory Usage: {memory_usage:.2f} / {memory_total:.2f} GB\\n'\n","\n","    if torch.cuda.is_available():\n","        memory_type = 'GPU'\n","        memory_free, memory_total = torch.cuda.mem_get_info()\n","        memory_usage = memory_total - memory_free\n","        print_memory += f'{memory_type} Menory Usage: {memory_usage / 1024**3:.2f} / {memory_total:.2f} GB\\n'\n","\n","    print_memory = f'---------------\\n{print_memory}---------------\\n'\n","    return print_memory\n","\n","\n","def clear_memory() -> None:\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\n","# загрузка и инициализация модели GGUF\n","def load_model(model_id: str, model_file: str, n_ctx: int) -> Tuple[Dict[str, Llama], str]:\n","    model = None\n","    if isinstance(model_file, list):\n","        load_log = 'Не выбрана модель'\n","        return model, load_log\n","\n","    load_log = ''\n","    if '(' in model_file:\n","        model_file = model_file.split('(')[0].rstrip()\n","\n","    progress = gr.Progress()\n","    progress(0.3, desc='Шаг 1/2: Загрузка модели GGUF')\n","    model_path = MODELS_PATH / model_file\n","    if model_path.is_file():\n","        load_log += 'Модель уже загружена, повторная инициализация\\n'\n","    else:\n","        try:\n","            hf_hub_download(\n","                repo_id=model_id,\n","                filename=model_file,\n","                local_dir=MODELS_PATH,\n","                local_dir_use_symlinks=False,\n","                )\n","            load_log += 'Модель успешно загружена\\n'\n","        except Exception as ex:\n","            model_path = ''\n","            load_log += f'Ошибка загрузки модели, код ошибки:\\n{ex}\\n'\n","\n","    if model_path:\n","        progress(0.7, desc='Шаг 2/2: Инициализация модели')\n","        try:\n","            model = Llama(model_path=str(model_path), n_ctx=n_ctx, n_gpu_layers=-1)\n","            load_log += f'Модель {model_id}/{model_file} инициализирована\\n'\n","        except Exception as ex:\n","            load_log += f'Ошибка инициализации модели, код ошибки:\\n{ex}\\n'\n","\n","    model = {'model': model}\n","    clear_memory()\n","    return model, load_log\n","\n","\n","# загрузка и инициализация модели эмбедингов\n","def load_embed_model(model_id: str) -> Tuple[Dict[str, HuggingFaceEmbeddings], str]:\n","    embed_model = None\n","    if isinstance(model_id, list):\n","        load_log = 'Не выбрана модель'\n","        return embed_model, load_log\n","\n","    load_log = ''\n","    progress = gr.Progress()\n","\n","    folder_name = model_id.replace('/', '_')\n","    folder_path = EMBED_MODELS_PATH / folder_name\n","\n","    if Path(folder_path).is_dir():\n","        load_log += f'Повторная инициализация модели {model_id} \\n'\n","    else:\n","        progress(0.5, desc='Шаг 1/2: Загрузка модели')\n","        snapshot_download(\n","            repo_id=model_id,\n","            local_dir=folder_path,\n","            ignore_patterns='*.h5',\n","            local_dir_use_symlinks=False,\n","        )\n","\n","    progress(0.7, desc='Шаг 2/2: Инициализация модели')\n","    model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","    embed_model = HuggingFaceEmbeddings(\n","        model_name=str(folder_path),\n","        model_kwargs=model_kwargs,\n","        )\n","    load_log += f'Модель эмбедингов {model_id} инициализирована\\n'\n","    load_log += f'Выберите ретривер и загрузите документы повторно\\n'\n","\n","    embed_model = {'embed_model': embed_model}\n","    clear_memory()\n","    return embed_model, load_log\n","\n","\n","# добавление ноового репозитория HF new_model_id к текущему списку model_ids\n","def add_new_model_id(new_model_id: str, model_ids: List[str]) -> Tuple[gr.Dropdown, str]:\n","    load_log = ''\n","    model_id = new_model_id.strip()\n","\n","    if model_id:\n","        model_id = model_id.split('/')[-2:]\n","        if len(model_id) == 2:\n","            model_id = '/'.join(model_id).split('?')[0]\n","            if repo_exists(model_id) and model_id not in model_ids:\n","                if any([file_name.endswith('.gguf') for file_name in list_repo_files(model_id)]):\n","                    model_ids.insert(0, model_id)\n","                    load_log += f'Репозиторий модели {model_id} успешно добавлен\\n'\n","                else:\n","                    load_log += f'Не найдены модели GGUF в репозитории {model_id}\\n'\n","            else:\n","                load_log += 'Неверное название репозитория HF или модель уже есть в списке\\n'\n","        else:\n","            load_log += 'Неверная ссылка на репозиторий HF\\n'\n","    else:\n","        load_log += 'Пустая строка в поле репозитория HF\\n'\n","\n","    model_id_dropdown = gr.Dropdown(choices=model_ids, value=model_ids[0])\n","    return model_id_dropdown, load_log\n","\n","\n","# получить список моделей GGUF из репозитория HF\n","def get_gguf_model_names(model_id: str) -> gr.Dropdown:\n","    repo_files = list(list_repo_tree(model_id))\n","    repo_files = [file for file in repo_files if file.path.endswith('.gguf')]\n","    model_paths = [f'{file.path} ({file.size / 1000 ** 3:.2f}G)' for file in repo_files]\n","\n","    model_paths_dropdown = gr.Dropdown(\n","        choices=model_paths,\n","        value=model_paths[0],\n","        label='Файл модели GGUF',\n","        )\n","    return model_paths_dropdown\n","\n","\n","# удаление файлов и папок моделей для очистки места кроме текущей модели ignore_link\n","def clear_folder(ignore_link: str) -> None:\n","    folder = EMBED_MODELS_PATH\n","    if len(ignore_link.split('/')) != 2:\n","        folder = MODELS_PATH\n","        if '(' in ignore_link:\n","            ignore_link = ignore_link.split('(')[0].rstrip()\n","\n","    for path in folder.iterdir():\n","        if path.name == ignore_link:\n","            continue\n","        if path.is_file():\n","            path.unlink()\n","        elif path.is_dir():\n","            rmtree(path)\n","    clear_memory()\n","\n","\n","# очистка текста\n","def clear_text(text: str) -> str:\n","    lines = text.split('\\n')\n","    lines = [line for line in lines if len(line.strip()) > 2]\n","    text = '\\n'.join(lines).strip()\n","    return text\n","\n","\n","def clear_documents(documents: List[Document]) -> List[Document]:\n","    output_documents = []\n","    for document in documents:\n","        text = clear_text(document.page_content)\n","        if len(text) > 10:\n","            document.page_content = text\n","            output_documents.append(document)\n","    return output_documents\n","\n","\n","# получение разделителя для csv файла на слуяай если он отличется от ','\n","def get_csv_delimiter(file_path: str) -> str:\n","    n_bytes = 4096\n","    with open(file_path) as csvfile:\n","        delimiter = csv.Sniffer().sniff(csvfile.read(n_bytes)).delimiter\n","    return delimiter\n","\n","\n","# извлечение документов (в формате langchain Documents) из загруженных файлов\n","def load_documents_from_files(upload_files: List[str]) -> Tuple[List[Document], str]:\n","    load_log = ''\n","    documents = []\n","\n","    for upload_file in upload_files:\n","        file_extension = f'.{upload_file.split(\".\")[-1]}'\n","        if file_extension in LOADER_CLASSES:\n","            loader_class = LOADER_CLASSES[file_extension]\n","            loader_kwargs = {}\n","            if file_extension == '.csv':\n","                delimiter = get_csv_delimiter(upload_file)\n","                loader_kwargs = {'csv_args': {'delimiter': delimiter}}\n","            try:\n","                load_documents = loader_class(upload_file, **loader_kwargs).load()\n","                documents.extend(load_documents)\n","            except Exception as ex:\n","                load_log += f'Ошибка загрузки файла {upload_file}\\n'\n","                load_log += f'Код ошибки: {ex}\\n'\n","                continue\n","        else:\n","            load_log += f'Неподдерживаемый формат файла {upload_file}\\n'\n","            continue\n","    return documents, load_log\n","\n","\n","# извлечение документов (в формате langchain Documents) из WEB ссылок\n","def load_documents_from_links(\n","        web_links: str,\n","        subtitles_lang: str,\n","        ) -> Tuple[List[Document], str]:\n","    load_log = ''\n","    documents = []\n","    loader_class_kwargs = {}\n","    # фильтрация пустых строк\n","    web_links = [web_link.strip() for web_link in web_links.split('\\n') if web_link.strip()]\n","\n","    for web_link in web_links:\n","        # ----------------- ссылка на YouTube ---------------------------------\n","        if 'youtube.com' in web_link:\n","            # проверка что субтитры на выбранном языке subtitles_lang доступны в видео web_link\n","            youtube_id = web_link.split('watch?v=')[-1].split('&')[0]\n","            available_langs = [t.language for t in list(YouTubeTranscriptApi.list_transcripts(youtube_id))]\n","            if subtitles_lang not in str(available_langs):\n","                load_log += f'Язык субтитров {subtitles_lang} недоступен для видео {web_link}\\n'\n","                continue\n","            # если доступны только автоматические субтитры - запись в логи\n","            if len(available_langs) == 1 and 'auto-generated' in str(available_langs):\n","                load_log += f'Загружены автоматические субтитры, ручные недоступны для видео {web_link}\\n'\n","            # инициализация YouTubeLoader с параметром языка субтитров\n","            loader_class = LOADER_CLASSES['youtube'].from_youtube_url\n","            language = subtitles_lang[:2].lower()\n","            loader_class_kwargs = {'language': language}\n","\n","        # ----------------- ссылка не на YouTube ------------------------------\n","        else:\n","            loader_class = LOADER_CLASSES['web']\n","        try:\n","            if requests.get(web_link).status_code != 200:\n","                load_log += f'Ссылка недоступна для Python: {web_link}\\n'\n","                continue\n","            load_documents = loader_class(web_link, **loader_class_kwargs).load()\n","            documents.extend(load_documents)\n","        except MissingSchema:\n","            load_log += f'Неверная ссылка: {web_link}\\n'\n","            continue\n","        except Exception as ex:\n","            load_log += f'Ошибка загрузки лоадером данных по ссылке: {web_link}\\n'\n","            load_log += f'Код ошибки: {ex}\\n'\n","            continue\n","    return documents, load_log\n","\n","\n","# инициализация ретривера или ансамбля\n","def create_retriver(\n","        embed_model: Dict[str, HuggingFaceEmbeddings],\n","        documents: List[Document],\n","        retriever_classes: List[BaseRetriever],\n","        k: int,\n","        score_threshold: float,\n","        ) -> BaseRetriever:\n","\n","    retrievers = []\n","    for retriever_class in retriever_classes:\n","        # инициализация ретривера в зависимости от класса\n","        if retriever_class.__name__ in ('FAISS', 'Chroma'):\n","            db = retriever_class.from_documents(\n","                documents=documents,\n","                embedding=embed_model,\n","                )\n","            retriever = db.as_retriever(\n","                search_type='similarity_score_threshold',\n","                search_kwargs={'k': k, 'score_threshold': score_threshold},\n","                )\n","        else:\n","            retriever = retriever_class.from_documents(documents=documents)\n","            retriever.k = k\n","        retrievers.append(retriever)\n","\n","    if len(retrievers) == 1:\n","        final_retriver = retrievers[0]\n","\n","    # инициализация ансамбля если выбрано несколько ретриверов\n","    else:\n","        weights = [0.5] * len(retrievers)\n","        final_retriver = EnsembleRetriever(retrievers=retrievers, weights=weights)\n","\n","    return final_retriver\n","\n","\n","# загрузка файлов и формирование документов с ретривером\n","# если параметр k выбран 'max' то в качестве контекста к промту\n","# будут использованы все найденные фрагменты текста\n","def load_documents_and_create_retriver(\n","        upload_files: Optional[List[str]],\n","        web_links: str,\n","        subtitles_lang: str,\n","        chunk_size: int,\n","        chunk_overlap: int,\n","        k: Union[int, str],\n","        score_threshold: float,\n","        retriever_indexes: List[int],\n","        embed_model: Dict[str, HuggingFaceEmbeddings],\n","        ) -> Tuple[List[Document], Optional[BaseRetriever], str]:\n","\n","    documents = []\n","    retriever = None\n","\n","    embed_model = embed_model.get('embed_model')\n","    if embed_model is None:\n","        load_log = 'Не инициализирована модель эмбедингов'\n","        return documents, retriever, load_log\n","\n","    if not retriever_indexes:\n","        load_log = 'Не выбран ретривер'\n","        return documents, retriever, load_log\n","\n","    if upload_files is None and not web_links:\n","        load_log = 'Не выбраны файлы или ссылки'\n","        return documents, retriever, load_log\n","\n","\n","    progress = gr.Progress()\n","    all_documents = []\n","    load_log = ''\n","\n","    # загрузка документов из файлов\n","    if upload_files is not None:\n","        progress(0.3, desc='Шаг 1/2: Загрузка документов из файлов')\n","        docs, log = load_documents_from_files(upload_files)\n","        all_documents.extend(docs)\n","        load_log += log\n","\n","    # загрузка документов по ссылкам\n","    if web_links:\n","        progress(0.3 if upload_files is None else 0.5, desc='Шаг 1/2: Загрузка документов по ссылкам')\n","        docs, log = load_documents_from_links(web_links, subtitles_lang)\n","        all_documents.extend(docs)\n","        load_log += log\n","\n","    if len(all_documents) == 0:\n","        load_log += 'Загрузка прервана так как не было извлечено ни одного документа\\n'\n","        load_log += 'Режим RAG не может быть активирован'\n","        return documents, retriever, load_log\n","\n","    load_log += f'Загружено документов: {len(all_documents)}\\n'\n","\n","    # использовать документы все документы без разделения на фрагменты\n","    if k == 'max':\n","        documents = clear_documents(all_documents)\n","        load_log += f'Используются документы без разделения в кол-ве: {len(documents)}\\n'\n","        k = 1\n","    # разделить документы на фрагменты\n","    else:\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=chunk_size,\n","            chunk_overlap=chunk_overlap,\n","        )\n","        documents = text_splitter.split_documents(all_documents)\n","        documents = clear_documents(documents)\n","        load_log += f'Документы разделены, кол-во фрагментов текста: {len(documents)}\\n'\n","\n","    # создание ретривера или ансамбля\n","    progress(0.7, desc='Шаг 2/2: Инициализация ретривера')\n","\n","    retriever_classes = [RETRIEVER_CLASSES[i] for i in retriever_indexes]\n","    retriever = create_retriver(embed_model, documents, retriever_classes, k, score_threshold)\n","\n","    load_log += 'Режим RAG активирован и может быть дективирован на вкладке Generate'\n","    return documents, retriever, load_log\n","\n","\n","# загрузка документов, разбитие на фрагменты и инициализация ретривера\n","def get_promt_with_context(\n","        user_message: str,\n","        chatbot: List[List[Optional[str]]],\n","        history_len: int,\n","        rag_mode: bool,\n","        retriever: BaseRetriever,\n","        bos_token: str,\n","        context_template,\n","        *template_components: List[str],\n","        ) -> Tuple[str, List[List[Optional[str]]], str]:\n","\n","    chatbot.append([user_message, None])\n","    # если сообщение пустое - обработаем это в следующей функции generate_text\n","    if not user_message.strip():\n","        return '', chatbot, full_promt\n","\n","    # извлечение шаблонов\n","    CONTEXT_TEMPLATE = context_template\n","    SYSTEM_TEMPLATE, USER_TEMPLATE, BOT_TEMPLATE, ADD_BOS = template_components\n","\n","    # формирование промта из шаблонов\n","    full_promt = ''\n","    if SYSTEM_TEMPLATE:\n","        full_promt += SYSTEM_TEMPLATE\n","\n","    # добавлять ли токен BOS один раз вначале промта эзера\n","    if ADD_BOS:\n","        full_promt += bos_token\n","\n","    # формирование промта с историей если она есть и параметр history_len != 0\n","    if history_len != 0:\n","        for user_msg, bot_msg in chatbot[:-1][-history_len:]:\n","            full_promt += USER_TEMPLATE.format(message=user_msg)\n","            full_promt += BOT_TEMPLATE.format(message=bot_msg)\n","\n","    # если ретривер готов и включен режим RAG то ищем релевантные доки и добавляем в промт\n","    if retriever is not None and rag_mode:\n","        retriever_docs = retriever.invoke(user_message)  #--------------------------------------------------------------------------------\n","        retriever_context = '\\n'.join([doc.page_content for doc in retriever_docs])\n","        user_message = CONTEXT_TEMPLATE.format(\n","            message=user_message,\n","            context=retriever_context,\n","            )\n","\n","    # формирование последнего сообщения промта от юзера\n","    full_promt += USER_TEMPLATE.format(message=user_message)\n","    full_promt += BOT_TEMPLATE.split('{message}')[0].rstrip()\n","    return '', chatbot, full_promt\n","\n","\n","# генерация текста моделью\n","def generate_text(\n","        chatbot: List[List[Optional[str]]],\n","        full_promt: str,\n","        history_len: int,\n","        do_sample: bool,\n","        model: Dict[str, Llama],\n","        *generate_args: List[Union[int, float]],\n","        ) -> List[List[Optional[str]]]:\n","\n","    model = model.get('model')\n","    if model is None:\n","        gr.Info('Не выбрана модель GGUF')\n","        yield chatbot[:-1]\n","\n","    # если сообщение эзера пустое - ничего не делать\n","    if chatbot[-1][0].strip() == '':\n","        yield chatbot[:-1]\n","\n","    # генерация ответа моделью\n","    else:\n","        promt_tokens = model.tokenize(full_promt.encode('utf-8'), special=True, add_bos=False)\n","        gen_kwargs = dict(zip(GENERATE_KWARGS.keys(), generate_args))\n","        if not do_sample:\n","            gen_kwargs['top_k'] = 1\n","            gen_kwargs['repeat_penalty'] = 1\n","\n","        # generator = 'И тебе привет'\n","        chatbot[-1][1] = ''\n","        generator = model.generate(promt_tokens, **gen_kwargs)\n","        for token in generator:\n","            if token == model.token_eos():\n","                break\n","            character = model.detokenize([token]).decode('utf-8', errors='ignore')\n","            # character = token\n","            chatbot[-1][1] += character\n","            yield chatbot"],"metadata":{"id":"yUSn5CAXtpWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Приложение"],"metadata":{"id":"MidnR9iAtphv"}},{"cell_type":"code","source":["# import gradio as gr\n","\n","# from utils import (\n","#     get_memory_usage,\n","#     load_model,\n","#     load_embed_model,\n","#     add_new_model_id,\n","#     get_gguf_model_names,\n","#     clear_folder,\n","#     load_documents_and_create_retriver,\n","#     get_promt_with_context,\n","#     generate_text,\n","# )\n","\n","# from config import (\n","#     MODEL_IDS,\n","#     EMBEDERS_IDS,\n","#     TEMPLATES,\n","#     CONTEXT_TEMPLATE,\n","#     RETRIEVER_NAMES,\n","#     SUBTITLES_LANGUAGES,\n","#     HISTORY_LEN,\n","#     GENERATE_KWARGS,\n","# )\n","\n","\n","# загрузка и инициализация моделей\n","start_model = load_model(MODEL_IDS[0], 'openchat-3.5-0106.Q2_K.gguf', 2000)[0]['model']\n","start_embed_model = load_embed_model(EMBEDERS_IDS[0])[0]['embed_model']\n","\n","# шаблон промта при старте модели\n","DEFAULT_TEMPLATE = TEMPLATES['Openchat']\n","\n","\n","# =================== ИНТЕРФЕЙС ПРИЛОЖЕНИЯ ===========================\n","\n","with gr.Blocks(theme=gr.themes.Monochrome()) as demo:  # css=CSS\n","\n","    # ==================== СОСТОЯНИЯ ===============================\n","\n","    # загруженные фрагменты текста (список объектов langchain Document)\n","    documents = gr.State([])\n","    # ретривер\n","    retriever = gr.State(None)\n","    # полный текст текущего промта в сыром виде вместе с контекстом и служебными токенами\n","    full_promt = gr.State('')\n","\n","    # списки готовых репозиториев моделей HF\n","    model_ids_state = gr.State(MODEL_IDS)\n","    embed_ids_state = gr.State(EMBEDERS_IDS)\n","\n","    # модели GGUF и эмбедингов\n","    model = gr.State({'model': start_model})\n","    embed_model = gr.State({'embed_model': start_embed_model})\n","\n","    # ==================== СТРАНИЦА БОТА =================================\n","\n","    # рендер окна чат бота с названием текущего режима - RAG или Chatbot\n","    def get_chatbot(chatbot_history: list = [], rag_mode_label: bool = False):\n","        label = 'RAG' if rag_mode_label else 'Chatbot'\n","        chatbot = gr.Chatbot(\n","                value=chatbot_history,\n","                # height=300,\n","                show_copy_button=True,\n","                bubble_full_width=False,\n","                label=label,\n","                )\n","        return chatbot\n","\n","    with gr.Tab(label='Generate'):\n","        with gr.Row():\n","            with gr.Column(scale=3):\n","                # чат бот и кнопки отправить сообщение, стоп генерации и удалить историю чата\n","                chatbot = get_chatbot()\n","                user_message = gr.Textbox(label='User')\n","                with gr.Row():\n","                    user_message_btn = gr.Button('Отправить')\n","                    stop_btn = gr.Button('Стоп')\n","                    clear_btn = gr.Button('Очистить чат')\n","\n","                # включить или выключить режим RAG даже если ретривер готов к работе\n","                rag_mode = gr.Checkbox(value=False, label='Режим RAG', scale=1, visible=False)\n","                rag_mode.change(fn=get_chatbot, inputs=[chatbot, rag_mode], outputs=chatbot)\n","\n","            # ------------------ ПАРАМЕТРЫ ГЕНЕРАЦИИ -------------------------\n","            def get_generate_args(do_sample: bool):\n","                visible = do_sample\n","                generate_args = [\n","                    gr.Slider(5, 50, GENERATE_KWARGS['top_k'], step=5, label='top_k', visible=visible),\n","                    gr.Slider(0, 1, GENERATE_KWARGS['top_p'], step=0.1, label='top_p', visible=visible),\n","                    gr.Slider(0.1, 4, GENERATE_KWARGS['temp'], step=0.1, label='temperature', visible=visible),\n","                    gr.Slider(1, 5, GENERATE_KWARGS['repeat_penalty'], step=0.1, label='repeat penalty', visible=visible),\n","                ]\n","                return generate_args\n","\n","            with gr.Column(scale=1, min_width=80):\n","                with gr.Group():\n","                    # длина истории которую будет учитывать модель\n","                    gr.Markdown('Размер истории')\n","                    history_len = gr.Slider(\n","                        minimum=0,\n","                        maximum=5,\n","                        value=HISTORY_LEN,\n","                        step=1,\n","                        info='Кол-во предыдущих сообщенией, учитываемых в истории',\n","                        label='history len',\n","                        show_label=False,\n","                        )\n","\n","                    # with gr.Group():\n","                    #     gr.Markdown('Настройка отбора фрагментов для ретриривера Faiss или Chroma')\n","                    #     score_threshold = gr.Slider(\n","                    #         label='searh_score_threshold',\n","                    #         value=0.5,\n","                    #         minimum=0.1,\n","                    #         maximum=1,\n","                    #         step=0.1,\n","                    #         )\n","\n","                    with gr.Group():\n","                        gr.Markdown('Параметры генерации')\n","                        # переключатель активации случайного семплирования при генерации текста моделью\n","                        do_sample = gr.Checkbox(\n","                            value=False,\n","                            label='do_sample',\n","                            info='Активация случайного семплирования',\n","                            )\n","                        # настройки семплирования\n","                        generate_args = get_generate_args(do_sample.value)\n","\n","                        do_sample.change(\n","                            fn=get_generate_args,\n","                            inputs=do_sample,\n","                            outputs=generate_args,\n","                            show_progress=False,\n","                            )\n","\n","        # ------------------ ТЕКСТ ПРОМТА С КОНТЕКСТОМ --------------------\n","        full_promt = gr.Textbox(\n","            label='Полный текст текущего промта с контекстом',\n","            interactive=False,\n","            )\n","\n","        # -------- КОМПОНЕНТЫ РЕДАКТИРОВАНИЯ ПРОМТОВ И СПЕЦ ТОКЕНОВ --------\n","        def get_bos_token(visible=False, render=True):\n","            bos_token_component = gr.Textbox(\n","                value='<s>',\n","                label='Bos token',\n","                visible=visible,\n","                render=render,\n","                )\n","            return bos_token_component\n","\n","        # добавлять токен {BOS} только один раз вначале шаблона юзера\n","        add_bos = gr.Checkbox(\n","            value=False,\n","            label='add bos',\n","            scale=1,\n","            render=False,\n","            info='Добавлять ли токен BOS один раз вначале диалога перед промтом пользователя',\n","            )\n","        bos_token = get_bos_token(visible=False, render=False)\n","        add_bos.change(\n","            fn=get_bos_token,\n","            inputs=add_bos,\n","            outputs=bos_token,\n","            show_progress=False,\n","            )\n","\n","        # -------------------- ШАБЛОНЫ ПРОМТОВ ---------------------------\n","        # окно редактирования шаблона системного промта\n","        system_template = gr.Textbox(\n","            DEFAULT_TEMPLATE['SYSTEM_TEMPLATE'],\n","            label='System template',\n","            lines=4,\n","            render=False,\n","            )\n","        # окно редактирования шаблона промта пользователя\n","        user_template = gr.Textbox(\n","            DEFAULT_TEMPLATE['USER_TEMPLATE'],\n","            label='User template',\n","            lines=4,\n","            render=False,\n","            )\n","        # окно редактирования шаблона промта бота\n","        bot_template = gr.Textbox(\n","            DEFAULT_TEMPLATE['BOT_TEMPLATE'],\n","            label='Bot template',\n","            lines=4,\n","            render=False,\n","            )\n","\n","        template_components = [system_template, user_template, bot_template, add_bos]\n","\n","        # окно редактирования шаблона промта при условии контекста\n","        context_template = gr.Textbox(\n","            CONTEXT_TEMPLATE,\n","            label='Context template',\n","            lines=6,\n","            render=False,\n","            )\n","\n","        # ------------------ КНОПКИ ОТПРАВИТЬ ОЧИСТИТЬ И СТОП ------------\n","        # нажатие Enter и кнопка отправить\n","        generate_event = gr.on(\n","            triggers=[user_message.submit, user_message_btn.click],\n","            fn=get_promt_with_context,\n","            inputs=[user_message, chatbot, history_len, rag_mode, retriever,\n","            bos_token, context_template, *template_components],\n","            outputs=[user_message, chatbot, full_promt],\n","            queue=True,\n","        ).then(\n","            fn=lambda promt: promt,\n","            inputs=full_promt,\n","            outputs=full_promt,\n","            queue=False,\n","        ).then(\n","            fn=generate_text,\n","            inputs=[chatbot, full_promt, history_len, do_sample, model, *generate_args],\n","            outputs=chatbot,\n","            queue=True,\n","            )\n","        # кнопка Стоп\n","        stop_btn.click(\n","            fn=None,\n","            inputs=None,\n","            outputs=None,\n","            cancels=generate_event,\n","            queue=False,\n","        )\n","        # кнопка Очистить чат\n","        clear_btn.click(\n","            fn=lambda: (None, ''),\n","            inputs=None,\n","            outputs=[chatbot, full_promt],\n","            queue=False,\n","            )\n","\n","    # ===================== СТРАНИЦА ЗАГРУЗКИ ФАЙЛОВ =========================\n","\n","    # настройки параметров для нарезки текстов\n","    def get_chunk_size_overlap(k: int = 2):\n","        visible = k != 'max'\n","        chunk_size = gr.Slider(50, 2000, value=500, step=50, label='Длина фрагментов', visible=visible)\n","        chunk_overlap = gr.Slider(0, 200, value=20, step=10, label=\"Длина пересечения фрагментов\", visible=visible)\n","        return chunk_size, chunk_overlap\n","\n","    with gr.Tab(label='Load documents'):\n","        with gr.Row(variant='compact'):\n","            # загрузка файлов и ссылок\n","            upload_files = gr.File(file_count='multiple', label='Загрузка текстовых файлов')\n","            web_links = gr.Textbox(lines=8, label='Ссылки на Web сайты или Ютуб')\n","\n","        with gr.Row(variant='compact'):\n","            with gr.Column(scale=2):\n","                # выбор ретриверов\n","                retriever_indexes = gr.CheckboxGroup(\n","                    choices=RETRIEVER_NAMES,\n","                    value=RETRIEVER_NAMES[0],\n","                    type='index',\n","                    label='Ретривер',\n","                    )\n","\n","                with gr.Row(variant='compact'):\n","                    score_threshold = gr.Slider(\n","                        label='searh_score_threshold',\n","                        value=0.5,\n","                        minimum=0.1,\n","                        maximum=1,\n","                        step=0.1,\n","                        )\n","\n","                    # кол-во релевантных кусков текста для поиска ретривером\n","                    k = gr.Radio(\n","                        choices=[1, 2, 3, 'max'],\n","                        value=2,\n","                        label='Количество релевантных документов для поиска',\n","                        )\n","\n","                    # язык субтитров для YouTube\n","                    subtitles_lang = gr.Radio(\n","                        SUBTITLES_LANGUAGES,\n","                        value=SUBTITLES_LANGUAGES[0],\n","                        label=\"Язык субтитров YouTube\",\n","                        )\n","\n","            with gr.Column(scale=1, min_width=160):\n","                # параметры нарезки текста\n","                chunk_size, chunk_overlap = get_chunk_size_overlap()\n","                k.change(\n","                    get_chunk_size_overlap,\n","                    inputs=k,\n","                    outputs=[chunk_size, chunk_overlap],\n","                    show_progress=False,\n","                    )\n","\n","        # кнопка загрузки документов и инициализации ретривера\n","        load_documents_btn = gr.Button(value='Загрузить документы и создать ретривер')\n","        # статус прогресса\n","        load_docs_log = gr.Textbox(label='Прогресс загрузки и разделения документов')\n","\n","        # главный цикл загрузки доков и инициализации ретривера\n","        load_event = load_documents_btn.click(\n","            fn=load_documents_and_create_retriver,\n","            inputs=[upload_files, web_links, subtitles_lang, chunk_size,\n","                    chunk_overlap, k, score_threshold, retriever_indexes, embed_model],\n","            outputs=[documents, retriever, load_docs_log],\n","            )\n","\n","        # сменить название бота на RAG или Chatbot в зависимости от готовности ретривера\n","        def load_success(chatbot_history, retriever):\n","            rag_mode = retriever is not None\n","\n","            chatbot = get_chatbot(chatbot_history, rag_mode)\n","            rag_mode_checkbox = gr.Checkbox(value=rag_mode, label='Режим RAG', scale=1, visible=rag_mode)\n","            return chatbot, rag_mode_checkbox\n","\n","        load_event.success(\n","            fn=load_success,\n","            inputs=[chatbot, retriever],\n","            outputs=[chatbot, rag_mode],\n","            )\n","\n","    # ================= СТРАНИЦА ПРОСМОТРА ВСЕХ ДОКУМЕНТОВ =================\n","\n","    with gr.Tab(label='View documents'):\n","        view_documents_btn = gr.Button(value='Отобразить загруженные фрагменты')\n","        view_documents_textbox = gr.Textbox(\n","            lines=1,\n","            placeholder='Для просмотра фрагментов загрузите документы на вкладке Load documents',\n","            label='Загруженные фрагменты',\n","            )\n","        sep = '=' * 20\n","        # отображение загруженных фрагментов текста если они готовы\n","        view_documents_btn.click(\n","            lambda documents: f'\\n{sep}\\n\\n'.join([doc.page_content for doc in documents]),\n","            inputs=documents,\n","            outputs=view_documents_textbox,\n","        )\n","\n","    # ================= СТРАНИЦА РЕДАКТИРОВАНИЯ ШАБЛОНА ПРОМТА ===============\n","\n","    # выбор заготовок шаблонов для протмтов\n","    def select_template(template_name):\n","        template = list(TEMPLATES[template_name].values())\n","        return template\n","\n","    # отображение результата ручного редактирования промтов на примере\n","    def get_example_prompt(bos_token, *template_components):\n","        user_message = 'Как дела?'\n","        chatbot_history = [['Привет', 'Здорова']]\n","\n","        _, _, example_prompt = get_promt_with_context(\n","            user_message, chatbot_history, 1, False, None,\n","            bos_token, '', *template_components)\n","\n","        example_prompt = gr.Textbox(\n","            value=example_prompt,\n","            label='Example prompt',\n","            lines=example_prompt.count('\\n'),\n","            )\n","        return example_prompt\n","\n","    with gr.Tab(label='Edit pomt templates'):\n","        with gr.Group():\n","            # выбор шаблонов промтов из предварительных\n","            gr.Markdown('Готовые шаблоны по умолчанию')\n","            default_template = gr.Dropdown(\n","                choices=list(TEMPLATES.keys()),\n","                value=None,\n","                label='Default Templates',\n","                )\n","            default_template.change(\n","                fn=select_template,\n","                inputs=default_template,\n","                outputs=[*template_components],\n","            )\n","\n","        with gr.Group():\n","            gr.Markdown('Настройки форматирования промтов перед подачей в модель')\n","            with gr.Row(variant='compact'):\n","                # добавлять ли токен BOS один раз перед шаблоном юзера\n","                add_bos.render()\n","                bos_token.render()\n","\n","        with gr.Row(variant='compact'):\n","            with gr.Column():\n","                # окна редактирования промтов (системный, юзер, бот)\n","                system_template.render()\n","                user_template.render()\n","                bot_template.render()\n","\n","            with gr.Column():\n","                # окно отображения результата ручного редактирования промтов на примере\n","                example_prompt = get_example_prompt(\n","                    bos_token.value,\n","                    *[c.value for c in template_components],\n","                    )\n","\n","        gr.on(\n","            triggers=[c.change for c in [add_bos, bos_token, *template_components]],\n","            fn=get_example_prompt,\n","            inputs=[bos_token, *template_components],\n","            outputs=[example_prompt],\n","            queue=False,\n","            )\n","\n","        with gr.Group():\n","            # окно редактирования промта при условии контекста\n","            gr.Markdown('Настройки форматирования промта при условии контекста (RAG)')\n","            context_template.render()\n","\n","    # ================= ЗАГРУЗКА GGUF МОДЕЛЕЙ ============================\n","\n","    with gr.Tab('Load model'):\n","        # окно добавления нового репозитория с HF\n","        new_model_id = gr.Textbox(\n","            value='',\n","            label='Добавить репозиторий',\n","            placeholder='Ссылка на репозиторий HF моделей в формате GGUF',\n","            )\n","        new_model_btn = gr.Button('Добавить репозиторий')\n","\n","        # выбор репозитория HF из доступных\n","        model_id = gr.Dropdown(\n","            choices=MODEL_IDS,\n","            value=None,\n","            label='Репозиторий модели HF',\n","            )\n","        # выбор модели GGUF из выбранного репозитория\n","        model_path = gr.Dropdown(\n","            choices=[],\n","            value=None,\n","            label='Файл модели GGUF',\n","            )\n","        # размер контекста для модели llama-cpp\n","        n_ctx = gr.Slider(500, 500 * 8, step=500, label='n_ctx')\n","        load_model_btn = gr.Button('Загрука и инициализация модели')\n","\n","        # статус загрузки модели\n","        load_model_log = gr.Textbox(\n","            value=f'Модель {MODEL_IDS[0]} загружена по умолчанию',\n","            label='Статус загрузки модели',\n","            )\n","\n","        with gr.Group():\n","            gr.Markdown('Освободить место на диске путем удаления всех моделей кроме текущей')\n","            remove_models_btn = gr.Button('Очистить папку')\n","\n","        # добавление нового репозитория и отображение статуса\n","        new_model_btn.click(\n","            fn=add_new_model_id,\n","            inputs=[new_model_id, model_ids_state],\n","            outputs=[model_id, load_model_log],\n","        ).success(\n","            fn=lambda: '',\n","            inputs=None,\n","            outputs=new_model_id,\n","        )\n","\n","        # получение списка моделей GGUF из выбранного репозитория\n","        model_id.change(\n","            fn=get_gguf_model_names,\n","            inputs=[model_id],\n","            outputs=[model_path],\n","        )\n","\n","        # загрузка модели GGUF\n","        load_model_btn.click(\n","            fn=load_model,\n","            inputs=[model_id, model_path, n_ctx],\n","            outputs=[model, load_model_log],\n","            queue=True,\n","        ).success(\n","            fn=lambda log: log + get_memory_usage(),\n","            inputs=load_model_log,\n","            outputs=load_model_log,\n","        )\n","\n","        # очистка папки с моделями кроме текущей загруженной\n","        remove_models_btn.click(\n","            fn=clear_folder,\n","            inputs=[model_path],\n","            outputs=None,\n","        ).success(\n","            fn=lambda model: f'Модели кроме {model} удалены',\n","            inputs=model_path,\n","            outputs=None,\n","        )\n","\n","    # ================= ЗАГРУЗКА ЭМБЕДИНГ МОДЕЛЕЙ ========================\n","    with gr.Tab('Load embed model'):\n","        # окно добавления нового репозитория с HF\n","        new_embed_id = gr.Textbox(\n","            value='',\n","            label='Добавить репозиторий',\n","            placeholder='Ссылка на репозиторий модели HF',\n","            )\n","        new_embed_btn = gr.Button('Добавить репозиторий')\n","\n","        # выбор репозитория HF из доступных\n","        embed_id = gr.Dropdown(\n","            choices=EMBEDERS_IDS,\n","            value=None,\n","            label='Репозиторий модели HF',\n","            )\n","\n","        # статус загрузки модели\n","        load_embed_btn = gr.Button('Загрука и инициализация модели')\n","        load_embed_log = gr.Textbox(\n","            value=f'Модель {EMBEDERS_IDS[0]} загружена по умолчанию',\n","            label='Статус загрузки модели',\n","            )\n","        with gr.Group():\n","            gr.Markdown('Освободить место на диске путем удаления всех моделей кроме текущей')\n","            remove_embed_models_btn = gr.Button('Очистить папку')\n","\n","        # добавление нового репозитория и отображение статуса\n","        new_embed_btn.click(\n","            fn=add_new_model_id,\n","            inputs=[new_embed_id, embed_ids_state],\n","            outputs=[embed_id, load_embed_log],\n","        ).success(\n","            fn=lambda: '',\n","            inputs=None,\n","            outputs=new_embed_id,\n","        )\n","\n","        # загрузка модели эмбедингов\n","        load_embed_btn.click(\n","            fn=load_embed_model,\n","            inputs=[embed_id],\n","            outputs=[embed_model, load_embed_log],\n","        ).success(\n","            fn=lambda log: log + get_memory_usage(),\n","            inputs=load_embed_log,\n","            outputs=load_embed_log,\n","        )\n","\n","        # очистка папки с моделями кроме текущей загруженной\n","        remove_embed_models_btn.click(\n","            fn=clear_folder,\n","            inputs=[embed_id],\n","            outputs=None,\n","        ).success(\n","            fn=lambda model: f'Модели кроме {model} удалены',\n","            inputs=embed_id,\n","            outputs=None,\n","        )\n","\n","demo.queue().launch(debug=True)"],"metadata":{"id":"fYz0b6in95aY","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1710079855550,"user_tz":-180,"elapsed":150336,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"c65cfd5b-b36e-434a-ea73-a38a998792a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from models/openchat-3.5-0106.Q2_K.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = openchat_openchat-3.5-0106\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 10\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n","llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q2_K:   65 tensors\n","llama_model_loader: - type q3_K:  160 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32002\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q2_K - Medium\n","llm_load_print_meta: model params     = 7.24 B\n","llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n","llm_load_print_meta: general.name     = openchat_openchat-3.5-0106\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 32000 '<|end_of_turn|>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.11 MiB\n","llm_load_tensors:        CPU buffer size =  2939.58 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 2000\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =   250.00 MiB\n","llama_new_context_with_model: KV self size  =  250.00 MiB, K (f16):  125.00 MiB, V (f16):  125.00 MiB\n","llama_new_context_with_model:        CPU input buffer size   =    12.93 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   157.00 MiB\n","llama_new_context_with_model: graph splits (measure): 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n","Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '8192', 'general.name': 'openchat_openchat-3.5-0106', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n","Using gguf chat template: {{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\n","Using chat eos_token: <|end_of_turn|>\n","Using chat bos_token: <s>\n","llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from models/openchat-3.5-0106.Q2_K.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = openchat_openchat-3.5-0106\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 10\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n","llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q2_K:   65 tensors\n","llama_model_loader: - type q3_K:  160 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32002\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q2_K - Medium\n","llm_load_print_meta: model params     = 7.24 B\n","llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n","llm_load_print_meta: general.name     = openchat_openchat-3.5-0106\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 32000 '<|end_of_turn|>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.11 MiB\n","llm_load_tensors:        CPU buffer size =  2939.58 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 2000\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =   250.00 MiB\n","llama_new_context_with_model: KV self size  =  250.00 MiB, K (f16):  125.00 MiB, V (f16):  125.00 MiB\n","llama_new_context_with_model:        CPU input buffer size   =    12.93 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   157.00 MiB\n","llama_new_context_with_model: graph splits (measure): 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n","Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '8192', 'general.name': 'openchat_openchat-3.5-0106', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n"]},{"output_type":"stream","name":"stdout","text":["Здорова\n","Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://36a268dccb5d25b581.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://36a268dccb5d25b581.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://36a268dccb5d25b581.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## Вариант с классами"],"metadata":{"id":"-1n-YEiGuoC2"}},{"cell_type":"markdown","source":["Конфиг"],"metadata":{"id":"Xx7l3pOkv2ey"}},{"cell_type":"code","source":["from dataclasses import dataclass\n","from pathlib import Path\n","\n","# from langchain_community.vectorstores import FAISS, Chroma\n","# from langchain_community.retrievers import BM25Retriever, TFIDFRetriever\n","\n","# from langchain_community.document_loaders import (\n","#     CSVLoader,\n","#     PDFMinerLoader,\n","#     PyPDFLoader,\n","#     TextLoader,\n","#     UnstructuredEmailLoader,\n","#     UnstructuredHTMLLoader,\n","#     UnstructuredMarkdownLoader,\n","#     UnstructuredODTLoader,\n","#     UnstructuredPowerPointLoader,\n","#     UnstructuredWordDocumentLoader,\n","#     WebBaseLoader,\n","#     YoutubeLoader,\n","#     DirectoryLoader,\n","# )\n","\n","@dataclass\n","class Config:\n","\n","    LOADER_CLASSES = {\n","        '.csv': CSVLoader,\n","        '.doc': UnstructuredWordDocumentLoader,\n","        '.docx': UnstructuredWordDocumentLoader,\n","        '.html': UnstructuredHTMLLoader,\n","        '.md': UnstructuredMarkdownLoader,\n","        '.pdf': PDFMinerLoader,\n","        '.ppt': UnstructuredPowerPointLoader,\n","        '.pptx': UnstructuredPowerPointLoader,\n","        '.txt': TextLoader,\n","        'web': WebBaseLoader,\n","        'directory': DirectoryLoader,\n","        'youtube': YoutubeLoader,\n","    }\n","\n","    TEMPLATES = {\n","        'Openchat': {\n","            'SYSTEM_TEMPLATE': '',\n","            'USER_TEMPLATE': ' GPT4 Correct User: {message}<|end_of_turn|>',\n","            'BOT_TEMPLATE': ' GPT4 Correct Assistant: {message}<|end_of_turn|>',\n","            'ADD_BOS': False,\n","            },\n","\n","        'Saiga mistral': {\n","            'SYSTEM_TEMPLATE': '''<s> system\n","Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\n","</s> ''',\n","            'USER_TEMPLATE': '<s> user\\n{message}\\n</s> ',\n","            'BOT_TEMPLATE': '<s> bot\\n{message}\\n</s> ',\n","            'ADD_BOS': False,\n","            },\n","\n","        'Mistral instruct': {\n","            'SYSTEM_TEMPLATE': '',\n","            'USER_TEMPLATE': ' [INST] {message} [/INST]',\n","            'BOT_TEMPLATE': '{message}</s>  ',\n","            'ADD_BOS': True,\n","            },\n","        }\n","\n","    CONTEXT_TEMPLATE = '''Дан контекст:\n","    {context}\n","    Дан вопрос:\n","    {message}\n","    Ответ:'''\n","\n","    CSS = '''\n","    .gradio-container {width: 80% !important}\n","    '''\n","\n","    MODELS_PATH = Path('models')\n","    EMBED_MODELS_PATH = Path('embed_models')\n","\n","    EMBEDERS_IDS = [\n","        'cointegrated/rubert-tiny2',\n","        'cointegrated/LaBSE-en-ru',\n","        'intfloat/multilingual-e5-large',\n","        'intfloat/multilingual-e5-base',\n","        'intfloat/multilingual-e5-small',\n","        'sentence-transformers/all-mpnet-base-v2',\n","        'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n","        'inkoziev/sbert_pq',\n","        'ai-forever/ruElectra-medium',\n","        'ai-forever/sbert_large_nlu_ru',\n","    ]\n","\n","    MODEL_IDS = [\n","        'TheBloke/openchat-3.5-0106-GGUF',\n","        'IlyaGusev/saiga_mistral_7b_gguf',\n","        'TheBloke/Mistral-7B-Instruct-v0.1-GGUF',\n","    ]\n","\n","    RETRIEVER_CLASSES = [BM25Retriever, FAISS, TFIDFRetriever]\n","\n","    SUBTITLES_LANGUAGES = ['Russian', 'English']\n","\n","    HISTORY_LEN = 0\n","    GENERATE_KWARGS = dict(\n","        top_k=40,\n","        top_p=0.9,\n","        temp=0.8,\n","        repeat_penalty=1.0,\n","        )\n","\n","    def __post_init__(self):\n","        self.RETRIEVER_NAMES = [cls.__name__ for cls in self.RETRIEVER_CLASSES]\n","\n","\n","CONFIG = Config()\n","\n","CONFIG.MODELS_PATH.mkdir(exist_ok=True)\n","CONFIG.EMBED_MODELS_PATH.mkdir(exist_ok=True)"],"metadata":{"id":"R48k-pxpv6Bp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CONFIG.TEMPLATES"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IW_RUruJMhhe","executionInfo":{"status":"ok","timestamp":1710071503682,"user_tz":-180,"elapsed":5,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"4d73a119-d204-42f7-c55e-c6ab6a84c637"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Openchat': {'SYSTEM_TEMPLATE': '',\n","  'USER_TEMPLATE': ' GPT4 Correct User: {message}<|end_of_turn|>',\n","  'BOT_TEMPLATE': ' GPT4 Correct Assistant: {message}<|end_of_turn|>',\n","  'ADD_BOS': False},\n"," 'Saiga mistral': {'SYSTEM_TEMPLATE': '<s> system\\nТы — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\\n</s> ',\n","  'USER_TEMPLATE': '<s> user\\n{message}\\n</s> ',\n","  'BOT_TEMPLATE': '<s> bot\\n{message}\\n</s> ',\n","  'ADD_BOS': False},\n"," 'Mistral instruct': {'SYSTEM_TEMPLATE': '',\n","  'USER_TEMPLATE': ' [INST] {message} [/INST]',\n","  'BOT_TEMPLATE': '{message}</s>  ',\n","  'ADD_BOS': True}}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["CONFIG.RETRIEVER_NAMES"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mNsXKRwcNALA","executionInfo":{"status":"ok","timestamp":1710071504051,"user_tz":-180,"elapsed":5,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"a5d591db-1c1f-4eb5-898c-ea4f70ee9bed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['BM25Retriever', 'FAISS', 'TFIDFRetriever']"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["Классы"],"metadata":{"id":"qu_ax1-qvAf4"}},{"cell_type":"markdown","source":["Не делал только начал"],"metadata":{"id":"w_tgbEZ_USvv"}},{"cell_type":"code","source":["# # from utils import (\n","# #     get_memory_usage,\n","# #     load_model,\n","# #     load_embed_model,\n","# #     add_new_model_id,\n","# #     get_gguf_model_names,\n","# #     clear_folder,\n","# #     load_documents_and_create_retriver,\n","# #     get_promt_with_context,\n","# #     generate_text,\n","# # )\n","\n","# # from config import (\n","# #     MODEL_IDS,\n","# #     EMBEDERS_IDS,\n","# #     TEMPLATES,\n","# #     CONTEXT_TEMPLATE,\n","# #     RETRIEVER_NAMES,\n","# #     SUBTITLES_LANGUAGES,\n","# #     HISTORY_LEN,\n","# #     GENERATE_KWARGS,\n","# # )\n","\n","\n","# class RagUtils:\n","\n","#     # очистка текста\n","#     def clear_text(text: str) -> str:\n","#         lines = text.split('\\n')\n","#         lines = [line for line in lines if len(line.strip()) > 2]\n","#         text = '\\n'.join(lines).strip()\n","#         return text\n","\n","#     def clear_documents(documents: List[Document]) -> List[Document]:\n","#         output_documents = []\n","#         for document in documents:\n","#             text = clear_text(document.page_content)\n","#             if len(text) > 10:\n","#                 document.page_content = text\n","#                 output_documents.append(document)\n","#         return output_documents\n","\n","\n","#     # получение разделителя для csv файла на слуяай если он отличется от ','\n","#     def get_csv_delimiter(file_path: str) -> str:\n","#         n_bytes = 4096\n","#         with open(file_path) as csvfile:\n","#             delimiter = csv.Sniffer().sniff(csvfile.read(n_bytes)).delimiter\n","#         return delimiter\n","\n","\n","#     # извлечение документов (в формате langchain Documents) из загруженных файлов\n","#     def load_documents_from_files(upload_files: List[str]) -> Tuple[List[Document], str]:\n","#         load_log = ''\n","#         documents = []\n","\n","#         for upload_file in upload_files:\n","#             file_extension = f'.{upload_file.split(\".\")[-1]}'\n","#             if file_extension in LOADER_CLASSES:\n","#                 loader_class = LOADER_CLASSES[file_extension]\n","#                 loader_kwargs = {}\n","#                 if file_extension == '.csv':\n","#                     delimiter = get_csv_delimiter(upload_file)\n","#                     loader_kwargs = {'csv_args': {'delimiter': delimiter}}\n","#                 try:\n","#                     load_documents = loader_class(upload_file, **loader_kwargs).load()\n","#                     documents.extend(load_documents)\n","#                 except Exception as ex:\n","#                     load_log += f'Ошибка загрузки файла {upload_file}\\n'\n","#                     load_log += f'Код ошибки: {ex}\\n'\n","#                     continue\n","#             else:\n","#                 load_log += f'Неподдерживаемый формат файла {upload_file}\\n'\n","#                 continue\n","#         return documents, load_log\n","\n","\n","#     # извлечение документов (в формате langchain Documents) из WEB ссылок\n","#     def load_documents_from_links(\n","#             web_links: str,\n","#             subtitles_lang: str,\n","#             ) -> Tuple[List[Document], str]:\n","#         load_log = ''\n","#         documents = []\n","#         loader_class_kwargs = {}\n","#         # фильтрация пустых строк\n","#         web_links = [web_link.strip() for web_link in web_links.split('\\n') if web_link.strip()]\n","\n","#         for web_link in web_links:\n","#             # ----------------- ссылка на YouTube ---------------------------------\n","#             if 'youtube.com' in web_link:\n","#                 # проверка что субтитры на выбранном языке subtitles_lang доступны в видео web_link\n","#                 youtube_id = web_link.split('watch?v=')[-1].split('&')[0]\n","#                 available_langs = [t.language for t in list(YouTubeTranscriptApi.list_transcripts(youtube_id))]\n","#                 if subtitles_lang not in str(available_langs):\n","#                     load_log += f'Язык субтитров {subtitles_lang} недоступен для видео {web_link}\\n'\n","#                     continue\n","#                 # если доступны только автоматические субтитры - запись в логи\n","#                 if len(available_langs) == 1 and 'auto-generated' in str(available_langs):\n","#                     load_log += f'Загружены автоматические субтитры, ручные недоступны для видео {web_link}\\n'\n","#                 # инициализация YouTubeLoader с параметром языка субтитров\n","#                 loader_class = LOADER_CLASSES['youtube'].from_youtube_url\n","#                 language = subtitles_lang[:2].lower()\n","#                 loader_class_kwargs = {'language': language}\n","\n","#             # ----------------- ссылка не на YouTube ------------------------------\n","#             else:\n","#                 loader_class = LOADER_CLASSES['web']\n","#             try:\n","#                 if requests.get(web_link).status_code != 200:\n","#                     load_log += f'Ссылка недоступна для Python: {web_link}\\n'\n","#                     continue\n","#                 load_documents = loader_class(web_link, **loader_class_kwargs).load()\n","#                 documents.extend(load_documents)\n","#             except MissingSchema:\n","#                 load_log += f'Неверная ссылка: {web_link}\\n'\n","#                 continue\n","#             except Exception as ex:\n","#                 load_log += f'Ошибка загрузки лоадером данных по ссылке: {web_link}\\n'\n","#                 load_log += f'Код ошибки: {ex}\\n'\n","#                 continue\n","#         return documents, load_log\n","\n","\n","#     # инициализация ретривера или ансамбля\n","#     def create_retriver(\n","#             embed_model: Dict[str, HuggingFaceEmbeddings],\n","#             documents: List[Document],\n","#             retriever_classes: List[BaseRetriever],\n","#             k: int,\n","#             ) -> BaseRetriever:\n","\n","#         retrievers = []\n","#         for retriever_class in retriever_classes:\n","#             # инициализация ретривера в зависимости от класса\n","#             if retriever_class.__name__ in ('FAISS', 'Chroma'):\n","#                 db = retriever_class.from_documents(\n","#                     documents=documents,\n","#                     embedding=embed_model,\n","#                     )\n","#                 retriever = db.as_retriever(search_kwargs={'k': k})\n","#             else:\n","#                 retriever = retriever_class.from_documents(documents=documents)\n","#                 retriever.k = k\n","#             retrievers.append(retriever)\n","\n","#         if len(retrievers) == 1:\n","#             final_retriver = retrievers[0]\n","\n","#         # инициализация ансамбля если выбрано несколько ретриверов\n","#         else:\n","#             weights = [0.5] * len(retrievers)\n","#             final_retriver = EnsembleRetriever(retrievers=retrievers, weights=weights)\n","\n","#         return final_retriver\n","\n","\n","#     # загрузка файлов и формирование документов с ретривером\n","#     # если параметр k выбран 'max' то в качестве контекста к промту\n","#     # будут использованы все найденные фрагменты текста\n","#     def load_documents_and_create_retriver(\n","#             upload_files: Optional[List[str]],\n","#             web_links: str,\n","#             subtitles_lang: str,\n","#             chunk_size: int,\n","#             chunk_overlap: int,\n","#             k: Union[int, str],\n","#             retriever_indexes: List[int],\n","#             embed_model: Dict[str, HuggingFaceEmbeddings],\n","#             ) -> Tuple[List[Document], Optional[BaseRetriever], str]:\n","\n","#         documents = []\n","#         retriever = None\n","\n","#         embed_model = embed_model.get('embed_model')\n","#         if embed_model is None:\n","#             load_log = 'Не инициализирована модель эмбедингов'\n","#             return documents, retriever, load_log\n","\n","#         if not retriever_indexes:\n","#             load_log = 'Не выбран ретривер'\n","#             return documents, retriever, load_log\n","\n","#         if upload_files is None and not web_links:\n","#             load_log = 'Не выбраны файлы или ссылки'\n","#             return documents, retriever, load_log\n","\n","\n","#         progress = gr.Progress()\n","#         all_documents = []\n","#         load_log = ''\n","\n","#         # загрузка документов из файлов\n","#         if upload_files is not None:\n","#             progress(0.3, desc='Шаг 1/2: Загрузка документов из файлов')\n","#             docs, log = load_documents_from_files(upload_files)\n","#             all_documents.extend(docs)\n","#             load_log += log\n","\n","#         # загрузка документов по ссылкам\n","#         if web_links:\n","#             progress(0.3 if upload_files is None else 0.5, desc='Шаг 1/2: Загрузка документов по ссылкам')\n","#             docs, log = load_documents_from_links(web_links, subtitles_lang)\n","#             all_documents.extend(docs)\n","#             load_log += log\n","\n","#         if len(all_documents) == 0:\n","#             load_log += 'Загрузка прервана так как не было извлечено ни одного документа\\n'\n","#             load_log += 'Режим RAG не может быть активирован'\n","#             return documents, retriever, load_log\n","\n","#         load_log += f'Загружено документов: {len(all_documents)}\\n'\n","\n","#         # использовать документы все документы без разделения на фрагменты\n","#         if k == 'max':\n","#             documents = clear_documents(all_documents)\n","#             load_log += f'Используются документы без разделения в кол-ве: {len(documents)}\\n'\n","#             k = 1\n","#         # разделить документы на фрагменты\n","#         else:\n","#             text_splitter = RecursiveCharacterTextSplitter(\n","#                 chunk_size=chunk_size,\n","#                 chunk_overlap=chunk_overlap,\n","#             )\n","#             documents = text_splitter.split_documents(all_documents)\n","#             documents = clear_documents(documents)\n","#             load_log += f'Документы разделены, кол-во фрагментов текста: {len(documents)}\\n'\n","\n","#         # создание ретривера или ансамбля\n","#         progress(0.7, desc='Шаг 2/2: Инициализация ретривера')\n","\n","#         retriever_classes = [RETRIEVER_CLASSES[i] for i in retriever_indexes]\n","#         retriever = create_retriver(embed_model, documents, retriever_classes, k)\n","\n","#         load_log += 'Режим RAG активирован и может быть дективирован на вкладке Generate'\n","#         return documents, retriever, load_log\n","\n","\n","#     # загрузка документов, разбитие на фрагменты и инициализация ретривера\n","#     def get_promt_with_context(\n","#             user_message: str,\n","#             chatbot: List[List[Optional[str]]],\n","#             history_len: int,\n","#             rag_mode: bool,\n","#             retriever: BaseRetriever,\n","#             bos_token: str,\n","#             context_template,\n","#             *template_components: List[str],\n","#             ) -> Tuple[str, List[List[Optional[str]]], str]:\n","\n","#         chatbot.append([user_message, None])\n","#         # если сообщение пустое - обработаем это в следующей функции generate_text\n","#         if not user_message.strip():\n","#             return '', chatbot, full_promt\n","\n","#         # извлечение шаблонов\n","#         CONTEXT_TEMPLATE = context_template\n","#         SYSTEM_TEMPLATE, USER_TEMPLATE, BOT_TEMPLATE, ADD_BOS = template_components\n","\n","#         # формирование промта из шаблонов\n","#         full_promt = ''\n","#         if SYSTEM_TEMPLATE:\n","#             full_promt += SYSTEM_TEMPLATE\n","\n","#         # добавлять ли токен BOS один раз вначале промта эзера\n","#         if ADD_BOS:\n","#             full_promt += bos_token\n","\n","#         # формирование промта с историей если она есть и параметр history_len != 0\n","#         if history_len != 0:\n","#             for user_msg, bot_msg in chatbot[:-1][-history_len:]:\n","#                 full_promt += USER_TEMPLATE.format(message=user_msg)\n","#                 full_promt += BOT_TEMPLATE.format(message=bot_msg)\n","#                 print(bot_msg)\n","\n","#         # если ретривер готов и включен режим RAG то ищем релевантные доки и добавляем в промт\n","#         if retriever is not None and rag_mode:\n","#             retriever_docs = retriever.invoke(user_message)\n","#             retriever_context = '\\n'.join([doc.page_content for doc in retriever_docs])\n","#             user_message = CONTEXT_TEMPLATE.format(\n","#                 message=user_message,\n","#                 context=retriever_context,\n","#                 )\n","\n","#         # формирование последнего сообщения промта от юзера\n","#         full_promt += USER_TEMPLATE.format(message=user_message)\n","#         full_promt += BOT_TEMPLATE.split('{message}')[0].rstrip()\n","#         return '', chatbot, full_promt\n","\n","\n","#     # генерация текста моделью\n","#     def generate_text(\n","#             chatbot: List[List[Optional[str]]],\n","#             full_promt: str,\n","#             history_len: int,\n","#             do_sample: bool,\n","#             model: Dict[str, Llama],\n","#             *generate_args: List[Union[int, float]],\n","#             ) -> List[List[Optional[str]]]:\n","\n","#         model = model.get('model')\n","#         if model is None:\n","#             gr.Info('Не выбрана модель GGUF')\n","#             yield chatbot[:-1]\n","\n","#         # если сообщение эзера пустое - ничего не делать\n","#         if chatbot[-1][0].strip() == '':\n","#             yield chatbot[:-1]\n","\n","#         # генерация ответа моделью\n","#         else:\n","#             promt_tokens = model.tokenize(full_promt.encode('utf-8'), special=True, add_bos=False)\n","#             gen_kwargs = dict(zip(GENERATE_KWARGS.keys(), generate_args))\n","#             if not do_sample:\n","#                 gen_kwargs['top_k'] = 1\n","#                 gen_kwargs['repeat_penalty'] = 1\n","\n","#             # generator = 'И тебе привет'\n","#             chatbot[-1][1] = ''\n","#             generator = model.generate(promt_tokens, **gen_kwargs)\n","#             for token in generator:\n","#                 if token == model.token_eos():\n","#                     break\n","#                 character = model.detokenize([token]).decode('utf-8', errors='ignore')\n","#                 # character = token\n","#                 chatbot[-1][1] += character\n","#                 yield chatbot\n","\n","\n","# class ModelUtils:\n","\n","#     # получение количества свободной памяти на диске, CPU и GPU\n","#     def get_memory_usage() -> str:\n","#         print_memory = ''\n","\n","#         memory_type = 'Disk'\n","#         psutil_stats = psutil.disk_usage('.')\n","#         memory_total = psutil_stats.total / 1024**3\n","#         memory_usage = psutil_stats.used / 1024**3\n","#         print_memory += f'{memory_type} Menory Usage: {memory_usage:.2f} / {memory_total:.2f} GB\\n'\n","\n","#         memory_type = 'CPU'\n","#         psutil_stats = psutil.virtual_memory()\n","#         memory_total = psutil_stats.total / 1024**3\n","#         memory_usage =  memory_total - (psutil_stats.available / 1024**3)\n","#         print_memory += f'{memory_type} Menory Usage: {memory_usage:.2f} / {memory_total:.2f} GB\\n'\n","\n","#         if torch.cuda.is_available():\n","#             memory_type = 'GPU'\n","#             memory_free, memory_total = torch.cuda.mem_get_info()\n","#             memory_usage = memory_total - memory_free\n","#             print_memory += f'{memory_type} Menory Usage: {memory_usage / 1024**3:.2f} / {memory_total:.2f} GB\\n'\n","\n","#         print_memory = f'---------------\\n{print_memory}---------------\\n'\n","#         return print_memory\n","\n","\n","#     def clear_memory() -> None:\n","#         gc.collect()\n","#         torch.cuda.empty_cache()\n","\n","\n","#     # загрузка и инициализация модели GGUF\n","#     def load_model(model_id: str, model_file: str, n_ctx: int) -> Tuple[Dict[str, Llama], str]:\n","#         model = None\n","#         if isinstance(model_file, list):\n","#             load_log = 'Не выбрана модель'\n","#             return model, load_log\n","\n","#         load_log = ''\n","#         if '(' in model_file:\n","#             model_file = model_file.split('(')[0].rstrip()\n","\n","#         progress = gr.Progress()\n","#         progress(0.3, desc='Шаг 1/2: Загрузка модели GGUF')\n","#         model_path = MODELS_PATH / model_file\n","#         if model_path.is_file():\n","#             load_log += 'Модель уже загружена, повторная инициализация\\n'\n","#         else:\n","#             try:\n","#                 hf_hub_download(\n","#                     repo_id=model_id,\n","#                     filename=model_file,\n","#                     local_dir=MODELS_PATH,\n","#                     local_dir_use_symlinks=False,\n","#                     )\n","#                 load_log += 'Модель успешно загружена\\n'\n","#             except Exception as ex:\n","#                 model_path = ''\n","#                 load_log += f'Ошибка загрузки модели, код ошибки:\\n{ex}\\n'\n","\n","#         if model_path:\n","#             progress(0.7, desc='Шаг 2/2: Инициализация модели')\n","#             try:\n","#                 model = Llama(model_path=str(model_path), n_ctx=n_ctx)\n","#                 load_log += f'Модель {model_id}/{model_file} инициализирована\\n'\n","#             except Exception as ex:\n","#                 load_log += f'Ошибка инициализации модели, код ошибки:\\n{ex}\\n'\n","\n","#         model = {'model': model}\n","#         clear_memory()\n","#         return model, load_log\n","\n","\n","#     # загрузка и инициализация модели эмбедингов\n","#     def load_embed_model(model_id: str) -> Tuple[Dict[str, HuggingFaceEmbeddings], str]:\n","#         embed_model = None\n","#         if isinstance(model_id, list):\n","#             load_log = 'Не выбрана модель'\n","#             return embed_model, load_log\n","\n","#         load_log = ''\n","#         progress = gr.Progress()\n","\n","#         folder_name = model_id.replace('/', '_')\n","#         folder_path = EMBED_MODELS_PATH / folder_name\n","\n","#         if Path(folder_path).is_dir():\n","#             load_log += f'Повторная инициализация модели {model_id} \\n'\n","#         else:\n","#             progress(0.5, desc='Шаг 1/2: Загрузка модели')\n","#             snapshot_download(\n","#                 repo_id=model_id,\n","#                 local_dir=folder_path,\n","#                 ignore_patterns='*.h5',\n","#                 local_dir_use_symlinks=False,\n","#             )\n","\n","#         progress(0.7, desc='Шаг 2/2: Инициализация модели')\n","#         model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","#         embed_model = HuggingFaceEmbeddings(\n","#             model_name=str(folder_path),\n","#             model_kwargs=model_kwargs,\n","#             )\n","#         load_log += f'Модель эмбедингов {model_id} инициализирована\\n'\n","#         load_log += f'Выберите ретривер и загрузите документы повторно\\n'\n","\n","#         embed_model = {'embed_model': embed_model}\n","#         clear_memory()\n","#         return embed_model, load_log\n","\n","\n","#     # добавление ноового репозитория HF new_model_id к текущему списку model_ids\n","#     def add_new_model_id(new_model_id: str, model_ids: List[str]) -> Tuple[gr.Dropdown, str]:\n","#         load_log = ''\n","#         model_id = new_model_id.strip()\n","\n","#         if model_id:\n","#             model_id = model_id.split('/')[-2:]\n","#             if len(model_id) == 2:\n","#                 model_id = '/'.join(model_id).split('?')[0]\n","#                 if repo_exists(model_id) and model_id not in model_ids:\n","#                     if any([file_name.endswith('.gguf') for file_name in list_repo_files(model_id)]):\n","#                         model_ids.insert(0, model_id)\n","#                         load_log += f'Репозиторий модели {model_id} успешно добавлен\\n'\n","#                     else:\n","#                         load_log += f'Не найдены модели GGUF в репозитории {model_id}\\n'\n","#                 else:\n","#                     load_log += 'Неверное название репозитория HF или модель уже есть в списке\\n'\n","#             else:\n","#                 load_log += 'Неверная ссылка на репозиторий HF\\n'\n","#         else:\n","#             load_log += 'Пустая строка в поле репозитория HF\\n'\n","\n","#         model_id_dropdown = gr.Dropdown(choices=model_ids, value=model_ids[0])\n","#         return model_id_dropdown, load_log\n","\n","\n","#     # получить список моделей GGUF из репозитория HF\n","#     def get_gguf_model_names(model_id: str) -> gr.Dropdown:\n","#         repo_files = list(list_repo_tree(model_id))\n","#         repo_files = [file for file in repo_files if file.path.endswith('.gguf')]\n","#         model_paths = [f'{file.path} ({file.size / 1000 ** 3:.2f}G)' for file in repo_files]\n","\n","#         model_paths_dropdown = gr.Dropdown(\n","#             choices=model_paths,\n","#             value=model_paths[0],\n","#             label='Файл модели GGUF',\n","#             )\n","#         return model_paths_dropdown\n","\n","\n","#     # удаление файлов и папок моделей для очистки места кроме текущей модели ignore_link\n","#     def clear_folder(ignore_link: str) -> None:\n","#         folder = EMBED_MODELS_PATH\n","#         if len(ignore_link.split('/')) != 2:\n","#             folder = MODELS_PATH\n","#             if '(' in ignore_link:\n","#                 ignore_link = ignore_link.split('(')[0].rstrip()\n","\n","#         for path in folder.iterdir():\n","#             if path.name == ignore_link:\n","#                 continue\n","#             if path.is_file():\n","#                 path.unlink()\n","#             elif path.is_dir():\n","#                 rmtree(path)\n","#         clear_memory()\n","\n","\n","# class Interface:\n","#     pass\n","\n"],"metadata":{"id":"StA5wBDBuuHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def page1():\n","#     with gr.Tab(label='Page1'):\n","#         textbox = gr.Text(value='hello')\n","#         gr.Markdown('page1')\n","\n","# def page2():\n","#     with gr.Tab(label='Page2'):\n","#         gr.Markdown('page2')\n","\n","# with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n","#     page1()\n","#     page2()\n","#     gr.Markdown(textbox.text)\n","\n","# demo.queue().launch(debug=True)"],"metadata":{"id":"aHO__L_-uuUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def change():\n","#     return 2\n","\n","# with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n","#     dr = gr.Dropdown(choices=[1, 2, 3], value=1)\n","#     gr.Button().click(change, None, dr)\n","\n","# demo.queue().launch(debug=True)"],"metadata":{"id":"Gx_j01b-Nq7E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Приложение"],"metadata":{"id":"S2IbePt3vDvP"}},{"cell_type":"code","source":[],"metadata":{"id":"rsoho-8ZPX75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"82FnaEU2PYBl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Other"],"metadata":{"id":"E0NmxaFUTQQL"}},{"cell_type":"markdown","source":["Посмотреть список файлов репозитория  \n","https://huggingface.co/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.list_repo_files"],"metadata":{"id":"KuDsnsy6TFvk"}},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","repo_id = 'IlyaGusev/saiga_mistral_7b_gguf'\n","api = HfApi()\n","repo_files = api.list_repo_files(repo_id, repo_type='model')\n","repo_files = [repo_file for repo_file in repo_files if repo_file.endswith('.gguf')]\n","repo_files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HnhA07x469K-","executionInfo":{"status":"ok","timestamp":1706386314845,"user_tz":-180,"elapsed":498,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"97f12ce1-f268-4207-bcef-e86df2a4cc48"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['model-q2_K.gguf', 'model-q4_K.gguf', 'model-q8_0.gguf']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["MODELS_PATH = './models'\n","model_path = hf_hub_download(\n","    repo_id=repo_id,\n","    filename=repo_files[0],\n","    local_dir=MODELS_PATH,\n","    )"],"metadata":{"id":"iduuWljOcyci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"w45abTwso3tg","executionInfo":{"status":"ok","timestamp":1706354272850,"user_tz":-180,"elapsed":8,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"27332302-1c3a-45e0-9446-9c01fc78bce0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'./models/model-q2_K.gguf'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","source":["Доп методы"],"metadata":{"id":"XGnf-uiFTDqQ"}},{"cell_type":"code","source":["# методы можно вызывать либо через api. либо через функции\n","from huggingface_hub import list_files_info, get_paths_info, list_repo_tree, list_repo_files\n","\n","# либо посмотреть дерево (возвращает генератор)\n","repo_files = api.list_repo_tree(repo_id)\n","repo_files = list(repo_files)\n","repo_files\n","\n","# # get_paths_info вместо list_files_info который устарел - неудобно нужно передавать файлы\n","# files_info = api.get_paths_info(model_name, ['README.md'])\n","# files_info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tE7GEZc88Zo3","executionInfo":{"status":"ok","timestamp":1706367185390,"user_tz":-180,"elapsed":842,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"bdeb69f7-43a7-43ed-db06-e671714d2e38"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[RepoFile(path='.gitattributes', size=1675, blob_id='09adc185a26b5a2b5a67d1e336302448b2643437', lfs=None, last_commit=None, security=None),\n"," RepoFile(path='README.md', size=978, blob_id='f2718d0a4626ddbd8253ea2567c3fd6f346e9570', lfs=None, last_commit=None, security=None),\n"," RepoFile(path='model-q2_K.gguf', size=3083107232, blob_id='001ce60f387e43cfb13386796a4c49e35e0a8654', lfs={'size': 3083107232, 'sha256': '178e59c10b8bd6fc31a6596f4a4250aff5b66c370018910fb2ead50a0a57594a', 'pointer_size': 135}, last_commit=None, security=None),\n"," RepoFile(path='model-q4_K.gguf', size=4368450336, blob_id='321e4c61d4cbb20a250c0dd0bdeed7afb4c1be76', lfs={'size': 4368450336, 'sha256': '2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644', 'pointer_size': 135}, last_commit=None, security=None),\n"," RepoFile(path='model-q8_0.gguf', size=7695874784, blob_id='4bb8e4b2e335d8fef53ba9355d6f392319fbd2c1', lfs={'size': 7695874784, 'sha256': 'a39fdd999a61231b274ea7ed14aaca0e77e1bd8754699542328a84ceaeba4ab6', 'pointer_size': 135}, last_commit=None, security=None)]"]},"metadata":{},"execution_count":116}]},{"cell_type":"code","source":["repo_files_sizes = [f'{file.path} ({file.size / 1024 ** 3:.2f}G)' for file in repo_files]\n","repo_files_sizes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hu495_rgcLiz","executionInfo":{"status":"ok","timestamp":1706367905574,"user_tz":-180,"elapsed":311,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"747d5dbf-448a-436e-8804-760428e43fed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.gitattributes (0.00G)',\n"," 'README.md (0.00G)',\n"," 'model-q2_K.gguf (2.87G)',\n"," 'model-q4_K.gguf (4.07G)',\n"," 'model-q8_0.gguf (7.17G)']"]},"metadata":{},"execution_count":126}]},{"cell_type":"code","source":["import os\n","os.path.getsize('/content/models/mistral-7b-instruct-v0.1.Q2_K.gguf')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qNzF_fgchmT9","executionInfo":{"status":"ok","timestamp":1706369328024,"user_tz":-180,"elapsed":4,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"afd28963-1da5-41f9-9237-f45c18464e62"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3083097760"]},"metadata":{},"execution_count":141}]},{"cell_type":"code","source":["os.path.getsize('/content/models/mistral-7b-instruct-v0.1.Q2_K.gguf') / 1000 ** 3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLmsPk2hiZl8","executionInfo":{"status":"ok","timestamp":1706369329725,"user_tz":-180,"elapsed":403,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"a263e35e-265e-4c45-ead6-987143d45b31"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.08309776"]},"metadata":{},"execution_count":142}]},{"cell_type":"code","source":["os.path.getsize('/content/models/mistral-7b-instruct-v0.1.Q2_K.gguf') / 1024 ** 3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOcpFT_-i22s","executionInfo":{"status":"ok","timestamp":1706369453270,"user_tz":-180,"elapsed":360,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"471a1851-7d35-4fd9-ec4d-27b0ea3ee0d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.8713585436344147"]},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["os.path.getsize('/content/models/mistral-7b-instruct-v0.1.Q2_K.gguf') / 1000 / 1024 / 1024"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvajVpJQi3aV","executionInfo":{"status":"ok","timestamp":1706369509826,"user_tz":-180,"elapsed":4,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"ce739053-662e-4458-f985-3da3b0fdb5a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.9402711486816404"]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["from huggingface_hub import repo_exists\n","repo_exists(repo_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5kwKowiaNSI","executionInfo":{"status":"ok","timestamp":1706367201161,"user_tz":-180,"elapsed":307,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"6ff7abfb-7ab3-42fb-96ec-0b6d58888a4d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":117}]},{"cell_type":"code","source":["from huggingface_hub import repo_info\n","info = repo_info(repo_id)\n","info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53VdVKKobFcp","executionInfo":{"status":"ok","timestamp":1706367438478,"user_tz":-180,"elapsed":6,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"97b20b68-f143-47bb-ba89-b0de1141a0c2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ModelInfo(id='IlyaGusev/saiga_mistral_7b_gguf', author='IlyaGusev', sha='a184baabd9d99676f8051d00da1a09ebe59c5aa2', created_at=datetime.datetime(2023, 10, 9, 17, 44, 21, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2023, 10, 9, 17, 58, 4, tzinfo=datetime.timezone.utc), private=False, gated=False, disabled=False, downloads=0, likes=48, library_name=None, tags=['gguf', 'conversational', 'ru', 'dataset:IlyaGusev/ru_turbo_saiga', 'dataset:IlyaGusev/ru_sharegpt_cleaned', 'dataset:IlyaGusev/oasst1_ru_main_branch', 'dataset:IlyaGusev/ru_turbo_alpaca_evol_instruct', 'dataset:lksy/ru_instruct_gpt4', 'license:apache-2.0', 'has_space', 'region:us'], pipeline_tag='conversational', mask_token=None, card_data={'language': ['ru'], 'license': 'apache-2.0', 'library_name': None, 'tags': None, 'datasets': ['IlyaGusev/ru_turbo_saiga', 'IlyaGusev/ru_sharegpt_cleaned', 'IlyaGusev/oasst1_ru_main_branch', 'IlyaGusev/ru_turbo_alpaca_evol_instruct', 'lksy/ru_instruct_gpt4'], 'metrics': None, 'eval_results': None, 'model_name': None, 'inference': False, 'pipeline_tag': 'conversational'}, widget_data=None, model_index=None, config={}, transformers_info=None, siblings=[RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model-q2_K.gguf', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model-q4_K.gguf', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model-q8_0.gguf', size=None, blob_id=None, lfs=None)], spaces=['IlyaGusev/saiga_mistral_7b_gguf', 'valeriylo/saiga_rag', 'valeriylo/rag_demo', 'WaveCut/saiga_mistral_7b_gguf', 'Cran-May/saiga_mistral_7b_gguf', 'rus78/saiga_mistral_7b_gguf', 'stash888/rag_demo'], safetensors=None)"]},"metadata":{},"execution_count":119}]},{"cell_type":"markdown","metadata":{"id":"91K6ogZnzJD4"},"source":["# Gradio App RAG"]},{"cell_type":"markdown","source":["https://colab.research.google.com/drive/1tClehCQILXLSXYqIZII569S1eTWnf8hZ#forceEdit=true&sandboxMode=true"],"metadata":{"id":"ecKkb05ONTXs"}},{"cell_type":"markdown","source":["## Screenshots"],"metadata":{"id":"_8iMzzp6p9Gh"}},{"cell_type":"markdown","source":["**Скриншоты интерфейса приложения**"],"metadata":{"id":"-0M2nvT8qDO4"}},{"cell_type":"markdown","source":["Скриншоты сделаны с использованием LLM модели `openchat/openchat-3.6-8b-20240522` и ембединг модели `sentence-transformers/all-mpnet-base-v2`, в качестве текстов для БД использовались ссылки на YouTube"],"metadata":{"id":"3SYzWdk7rm1B"}},{"cell_type":"markdown","source":["Страница интерфейса 1  \n","\n","<img src='https://drive.google.com/uc?export=view&id=1O08_gmClVTZnhcrvCG9g6_n0jrna2s5n' width=100%>"],"metadata":{"id":"83S9hH6sqH_R"}},{"cell_type":"markdown","source":["Страница интерфейса 2  \n","\n","<img src='https://drive.google.com/uc?export=view&id=166q7QiCA2SEw1SUq16bK5NLiy3UA3w_L' width=100%>"],"metadata":{"id":"QcGloQZQqQYk"}},{"cell_type":"markdown","source":["Страница интерфейса 3  \n","\n","<img src='https://drive.google.com/uc?export=view&id=1F5CMhWQ8iSGY5KR4mV9zouHB9Ot6iuYx' width=100%>"],"metadata":{"id":"_TADLjvQqQvR"}},{"cell_type":"markdown","source":["Страница интерфейса 1 демонстрация RAG  \n","\n","<img src='https://drive.google.com/uc?export=view&id=1OxPNQOp7pNAJM-fWpZUKv_Hjl5fD-60b' width=100%>"],"metadata":{"id":"ayzZ8WbMqRLS"}},{"cell_type":"markdown","source":["Страница интерфейса 1 демонстрация RAG  \n","\n","<img src='https://drive.google.com/uc?export=view&id=1fpfT8iccu2Cacb90aHWNHKfQNAwuxL4n' width=100%>"],"metadata":{"id":"jsRW-InkqRkl"}},{"cell_type":"markdown","source":["Скриншоты сделаны с использованием LLM модели `openchat/openchat-3.6-8b-20240522` (GGUF версия `openchat-3.5-0106.Q2_K.gguf` 3Gb) и ембединг модели `cointegrated/rubert-tiny2`, в качестве текстов использовалась документация мотокультиватора"],"metadata":{"id":"JmEzR-zlQ3bx"}},{"cell_type":"markdown","source":["Страница интерфейса 1 демонстрация RAG  \n","\n","<img src='https://drive.google.com/uc?export=view&id=1VkAm8E4CFoWKucnImBMgy93DLMuWj7kw' width=100%>"],"metadata":{"id":"sK742Nh4Q4Xv"}},{"cell_type":"markdown","source":["Страница интерфейса 1 демонстрация RAG  \n","\n","<img src='https://drive.google.com/uc?export=view&id=17EX6EQ6GqWdHxK8qfTh18F_YfGNXGwx6' width=100%>"],"metadata":{"id":"VQ2a8eM0Q4su"}},{"cell_type":"markdown","source":["## Запуск приложения в Colab"],"metadata":{"id":"o18AOcSERr5l"}},{"cell_type":"markdown","metadata":{"id":"PVEAYBZ1Cis-"},"source":["### Установка библиотек и импорты"]},{"cell_type":"markdown","source":["Установка библиотек"],"metadata":{"id":"Xd0yt9DWas15"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1d1c3c3-7132-4f35-9c11-c550dd2aae5c","id":"K8IwiJj67sAj","executionInfo":{"status":"ok","timestamp":1727686305239,"user_tz":-180,"elapsed":42807,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 325 ms, sys: 89.4 ms, total: 414 ms\n","Wall time: 41.9 s\n"]}],"source":["%%time\n","%%capture\n","\n","# faiss-gpu or faiss-gpu\n","!pip install accelerate langchain langchain_community langchain_huggingface \\\n","            pdfminer.six faiss-cpu gradio youtube-transcript-api  # sentence_transformers"]},{"cell_type":"markdown","metadata":{"id":"6tkbi6lUaoAp"},"source":["Доки llama-cpp-python  \n","https://llama-cpp-python.readthedocs.io/en/latest/?badge=latest  \n","Страница llama-cpp-python   \n","https://github.com/abetlen/llama-cpp-python\n"]},{"cell_type":"markdown","metadata":{"id":"C8qmQjH3aoA3"},"source":["Установка библиотеки llama-cpp-python"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"269969d3-f6a8-4d85-837e-0523266a671b","id":"dMWPWpq7aoA3","executionInfo":{"status":"ok","timestamp":1727686814409,"user_tz":-180,"elapsed":233880,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# установка с поддержкой CPU\n","!pip install -q llama-cpp-python==0.2.88\n","\n","# установка с поддержкой GPU CUDA\n","# !CMAKE_ARGS='-DGGML_CUDA=on' FORCE_CMAKE=1 pip install llama-cpp-python"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"r4L4INFe7sAk","executionInfo":{"status":"ok","timestamp":1727686833168,"user_tz":-180,"elapsed":18766,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0384468-d296-4c8a-a3b7-5fad63915004"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]}],"source":["import gc\n","import psutil\n","import csv\n","from pathlib import Path\n","from shutil import rmtree\n","from typing import List, Tuple, Dict, Union, Optional, Any\n","from tqdm import tqdm\n","\n","import requests\n","from requests.exceptions import MissingSchema\n","\n","import torch\n","import numpy as np\n","import gradio as gr\n","\n","from llama_cpp import Llama\n","from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled\n","\n","from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","\n","from langchain_community.document_loaders import (\n","    CSVLoader,\n","    PDFMinerLoader,\n","    PyPDFLoader,\n","    TextLoader,\n","    UnstructuredHTMLLoader,\n","    UnstructuredMarkdownLoader,\n","    UnstructuredPowerPointLoader,\n","    UnstructuredWordDocumentLoader,\n","    WebBaseLoader,\n","    YoutubeLoader,\n","    DirectoryLoader,\n",")\n","\n","# annotations\n","from langchain_core.retrievers import BaseRetriever\n","from langchain.docstore.document import Document\n","from langchain_core.vectorstores import VectorStore\n","from langchain_core.embeddings import Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b7e25a6-ec72-44ea-e797-7d842e226dda","id":"dUy5vQWP7sAk","executionInfo":{"status":"ok","timestamp":1727637081405,"user_tz":-180,"elapsed":1444,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["accelerate                       0.34.2\n","faiss-cpu                        1.8.0.post1\n","gradio                           4.44.0\n","gradio_client                    1.3.0\n","huggingface-hub                  0.24.7\n","langchain                        0.3.1\n","langchain-community              0.3.1\n","langchain-core                   0.3.6\n","langchain-huggingface            0.1.0\n","langchain-text-splitters         0.3.0\n","llama_cpp_python                 0.2.88\n","pdfminer.six                     20240706\n","sentence-transformers            3.1.1\n","torch                            2.4.1+cu121\n","torchaudio                       2.4.1+cu121\n","torchsummary                     1.5.1\n","torchvision                      0.19.1+cu121\n","transformers                     4.44.2\n","youtube-transcript-api           0.6.2\n"]}],"source":["!pip list | grep -P 'accelerate|torch|langchain|transformers|llama_cpp|gradio|youtube|huggingface-hub|pdfminer.six|faiss'"]},{"cell_type":"code","source":["# сохранить зависимости в requirements.txt (при необходимости вырезать лишнее)\n","!pip freeze | grep -P 'accelerate|torch|langchain|transformers|llama_cpp|gradio|youtube|huggingface-hub|pdfminer.six|faiss' > requirements.txt"],"metadata":{"id":"DoMcd2Hc2v3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# просмотр requirements.txt\n","!cat requirements.txt"],"metadata":{"id":"MZj3OhLtstvo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9911d830-acdd-44b9-b4a9-26323f25821f","executionInfo":{"status":"ok","timestamp":1727637090582,"user_tz":-180,"elapsed":9,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accelerate==0.34.2\n","faiss-cpu==1.8.0.post1\n","gradio==4.44.0\n","gradio_client==1.3.0\n","huggingface-hub==0.24.7\n","langchain==0.3.1\n","langchain-community==0.3.1\n","langchain-core==0.3.6\n","langchain-huggingface==0.1.0\n","langchain-text-splitters==0.3.0\n","llama_cpp_python==0.2.88\n","pdfminer.six==20240706\n","sentence-transformers==3.1.1\n","torch @ https://download.pytorch.org/whl/cu121_full/torch-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=f3ed9a2b7f8671b2b32a2f036d1b81055eb3ad9b18ba43b705aa34bae4289e1a\n","torchaudio @ https://download.pytorch.org/whl/cu121_full/torchaudio-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=da8c87c80a1c1376a48dc33eef30b03bbdf1df25a05bd2b1c620b8811c7b19be\n","torchsummary==1.5.1\n","torchvision @ https://download.pytorch.org/whl/cu121_full/torchvision-0.19.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=b8cc4bf381b75522995b601e07a1b433b5fd925dc3e34a7fa6cd22f449d65379\n","transformers==4.44.2\n","youtube-transcript-api==0.6.2\n"]}]},{"cell_type":"code","source":["# версия Python\n","!python -V"],"metadata":{"id":"f83AaouSOwhl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"865343e4-afee-45b5-fa51-f8ad1f50745e","executionInfo":{"status":"ok","timestamp":1727637093889,"user_tz":-180,"elapsed":314,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}]},{"cell_type":"markdown","source":["### Загрузка и инициализация моделей"],"metadata":{"id":"CXaXu9agKbO7"}},{"cell_type":"markdown","source":["Функция для загрузки моделей через библиотеку `requests` с прогресс баром"],"metadata":{"id":"Bp2YuApLsKrs"}},{"cell_type":"code","source":["def download_file(file_url: str, file_path: Union[str, Path]) -> None:\n","    response = requests.get(file_url, stream=True)\n","    total_size = int(response.headers.get('content-length', 0))\n","    chunk_size = 4096  # 4 Kb\n","    progress_bar = tqdm(desc='Загрузка файла GGUF', total=total_size, unit='iB', unit_scale=True)\n","\n","    with open(file_path, 'wb') as file:\n","        for data in response.iter_content(chunk_size):\n","            file.write(data)\n","            progress_bar.update(len(data))\n","    # progress_bar.close()"],"metadata":{"id":"LX5M0sl2nks0","executionInfo":{"status":"ok","timestamp":1727686833169,"user_tz":-180,"elapsed":10,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Инийциализация модели ембедингов и LLM модели"],"metadata":{"id":"UPh1kts9J53P"}},{"cell_type":"code","source":["# инициализация модели эмбедингов\n","print('Инициализация модели эмбедингов ...')\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","embed_model_name = 'sentence-transformers/all-mpnet-base-v2'\n","embed_model_path = Path('embed_model')\n","embed_model = HuggingFaceEmbeddings(\n","    model_name=embed_model_name,\n","    model_kwargs={'device': device},  # cpu cuda\n","    cache_folder=str(embed_model_path),\n","    )\n","\n","# прямая ссылка на модель в формате GGUF\n","# llm_model_url = 'https://huggingface.co/bartowski/openchat-3.6-8b-20240522-GGUF/resolve/main/openchat-3.6-8b-20240522-IQ4_XS.gguf'\n","llm_model_url = 'https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf'\n","\n","#\n","# полный путь до модели в папке ./models\n","llm_model_path = Path('models') / gguf_url.rsplit('/')[-1]\n","llm_model_path.parent.mkdir(exist_ok=True)\n","\n","# если модель отсутствует в папке models то нужно ее загрузить - например через библиотеку wget\n","if not llm_model_path.is_file():\n","    if not str(llm_model_url).endswith('.gguf'):\n","        raise Exception('Ссылка на модель LLM должна быть прямой ссылкой на файл GGUF')\n","    else:\n","        print('Загрузка файла LLM модели ...')\n","        download_file(llm_model_url, llm_model_path)\n","        # model_path = wget.download(llm_model_url, out=str(llm_model_path))\n","\n","# инициализация модели для генерации текста (установить нужный или поддерживаемый размер контекста n_ctx)\n","print('Инициализация LLM модели ...')\n","llm_model = Llama(model_path=str(llm_model_path), n_gpu_layers=-1, verbose=True)\n","\n","# поддерживает ли модель системный промт\n","support_system_role = 'System role not supported' not in llm_model.metadata['tokenizer.chat_template']\n","\n","# параметры генерации\n","# чтобы модель отвечала одинаково достаточно поставить top_k=1, top_p=0 и repeat_penalty=1 не зависимо от остальных параметров\n","GENERATE_KWARGS = dict(\n","    temperature=1,  # температура для софтмакса\n","    top_p=0.0,  # сумма вероятностей токенов из которых нужно выбирать следующий токен\n","    top_k=1,  # из скольки максимально вероятных токенов выбирать следующий токен\n","    repeat_penalty=1.0,  # штраф модели за повторения\n","    )"],"metadata":{"id":"8zuh3PjbJW0B","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5d9e33a4-1dce-48ce-dc4b-3701e4324bae","executionInfo":{"status":"ok","timestamp":1727688551066,"user_tz":-180,"elapsed":5156,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Инициализация модели эмбедингов ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from model/gemma-2-2b-it-Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n","llama_model_loader: - kv   1:                               general.type str              = model\n","llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n","llama_model_loader: - kv   3:                           general.finetune str              = it\n","llama_model_loader: - kv   4:                           general.basename str              = gemma-2\n","llama_model_loader: - kv   5:                         general.size_label str              = 2B\n","llama_model_loader: - kv   6:                            general.license str              = gemma\n","llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n","llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n","llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n","llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n","llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n","llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n","llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n","llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n","llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n","llama_model_loader: - kv  17:                          general.file_type u32              = 7\n","llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n","llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n","llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n","llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n"]},{"output_type":"stream","name":"stdout","text":["Инициализация LLM модели ...\n"]},{"output_type":"stream","name":"stderr","text":["llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n","llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n","llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n","llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n","llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n","llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n","llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n","llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n","llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n","llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\n","llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n","llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n","llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\n","llama_model_loader: - type  f32:  105 tensors\n","llama_model_loader: - type q8_0:  183 tensors\n","llm_load_vocab: special tokens cache size = 249\n","llm_load_vocab: token to piece cache size = 1.6014 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = gemma2\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 256000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: vocab_only       = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 2304\n","llm_load_print_meta: n_layer          = 26\n","llm_load_print_meta: n_head           = 8\n","llm_load_print_meta: n_head_kv        = 4\n","llm_load_print_meta: n_rot            = 256\n","llm_load_print_meta: n_swa            = 4096\n","llm_load_print_meta: n_embd_head_k    = 256\n","llm_load_print_meta: n_embd_head_v    = 256\n","llm_load_print_meta: n_gqa            = 2\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 9216\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 2\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 2B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 2.61 B\n","llm_load_print_meta: model size       = 2.59 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = Gemma 2 2b It\n","llm_load_print_meta: BOS token        = 2 '<bos>'\n","llm_load_print_meta: EOS token        = 1 '<eos>'\n","llm_load_print_meta: UNK token        = 3 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<pad>'\n","llm_load_print_meta: LF token         = 227 '<0x0A>'\n","llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n","llm_load_print_meta: max token length = 48\n","llm_load_tensors: ggml ctx size =    0.13 MiB\n","llm_load_tensors:        CPU buffer size =  2649.74 MiB\n","..................................................................\n","llama_new_context_with_model: n_ctx      = 4096\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =   416.00 MiB\n","llama_new_context_with_model: KV self size  =  416.00 MiB, K (f16):  208.00 MiB, V (f16):  208.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   504.50 MiB\n","llama_new_context_with_model: graph nodes  = 1050\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '182', 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'gemma2.attention.head_count': '8', 'gemma2.feed_forward_length': '9216', 'gemma2.block_count': '26', 'tokenizer.ggml.pre': 'default', 'general.license': 'gemma', 'general.type': 'model', 'gemma2.embedding_length': '2304', 'general.basename': 'gemma-2', 'tokenizer.ggml.padding_token_id': '0', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'it', 'general.file_type': '7', 'gemma2.attention.sliding_window': '4096', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '4', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.model': 'llama', 'general.name': 'Gemma 2 2b It', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'general.size_label': '2B', 'tokenizer.ggml.add_bos_token': 'true'}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n","' + message['content'] | trim + '<end_of_turn>\n","' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n","'}}{% endif %}\n","Using chat eos_token: <eos>\n","Using chat bos_token: <bos>\n"]}]},{"cell_type":"code","source":["support_system_role = 'System role not supported' not in start_model.metadata['tokenizer.chat_template']\n","support_system_role"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJyIGZD9V_x3","executionInfo":{"status":"ok","timestamp":1727686947365,"user_tz":-180,"elapsed":14,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"f0877843-6561-499b-bb03-61fdbd431ca3"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["texts = ['зима', 'весна']\n","db = FAISS.from_texts(texts=texts, embedding=embed_model)"],"metadata":{"id":"9DTj05dhNA88","executionInfo":{"status":"ok","timestamp":1727687782024,"user_tz":-180,"elapsed":3512,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["len(db)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"3SZ6J_weN3Cn","executionInfo":{"status":"error","timestamp":1727687787776,"user_tz":-180,"elapsed":7,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"7c580d85-b39c-4dbc-e733-aaa3ab772f2a"},"execution_count":12,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"object of type 'FAISS' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-984830482563>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: object of type 'FAISS' has no len()"]}]},{"cell_type":"code","source":["len(db.docstore._dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzhidwG4N7cC","executionInfo":{"status":"ok","timestamp":1727687871423,"user_tz":-180,"elapsed":610,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"a64606c0-198c-4fdc-8db7-26edd8734daa"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Проверка генерации текста LLM моделью"],"metadata":{"id":"ZojJztBIEsss"}},{"cell_type":"markdown","source":["Генерация текста моделью\n","\n","Документация по методу `create_chat_completion`  \n","https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion"],"metadata":{"id":"xyvdcWVcJ8wq"}},{"cell_type":"code","source":["%%time\n","\n","# системный промт\n","system_prompt = ''\n","# входной запрос пользователя\n","user_message = 'Почему трава зеленая?'\n","# список с репликами юзера и бота (в данном примере одна)\n","messages = []\n","\n","# формирование стандартного промта с запросом от пользователя\n","messages.append({'role': 'user', 'content': user_message})\n","\n","# параметры генерации\n","# чтобы модель отвечала одинаково достаточно поставить top_k=1, top_p=0 и repeat_penalty=1 не зависимо от остальных параметров\n","GENERATE_KWARGS = dict(\n","    temperature=1,  # температура для софтмакса\n","    top_p=0.0,  # сумма вероятностей токенов из которых нужно выбирать следующий токен\n","    top_k=1,  # из скольки максимально вероятных токенов выбирать следующий токен\n","    repeat_penalty=1.0,  # штраф модели за повторения\n","    )\n","\n","# создание объекта итератора для генерации текста\n","# при итерации по этому объекту в цикле она будет возвращать сгенерированный текст частями текста (токенами)\n","stream_response = llm_model.create_chat_completion(\n","    messages=messages,  # входной промт на который надо сгенерировать ответ\n","    stream=True,  # вернуть генератор\n","    **GENERATE_KWARGS,  # передача параметров генерации\n","    )\n","\n","# пустую строку будем конкатенировать с токенами ответа модели\n","response_text = ''\n","# итерация и последовательная генерация текста моделью в цикле\n","for chunk in stream_response:\n","    # извлечение текущего сгенерированного токена\n","    token = chunk['choices'][0]['delta'].get('content')\n","    if token is not None:\n","        response_text += token\n","        print(token, end='')"],"metadata":{"id":"Ng99Cz76ryrA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727640087050,"user_tz":-180,"elapsed":92751,"user":{"displayName":"Сергей","userId":"08873757262896960569"}},"outputId":"ed263ad3-cd94-4c9b-dc5a-433e8ccb2bb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Трава зеленая из-за пигмента **хлорофилла**. \n","\n","Вот как это работает:\n","\n","* **Хлорофилл** - это пигмент, который содержится в хлоропластах, органеллах, расположенных в клетках растений. \n","* **Хлорофилл** поглощает солнечный свет, особенно в красном и синем диапазоне, и отражает зеленый свет. \n","* **Зеленый свет** - это то, что мы видим, когда трава отражает свет. \n","\n","Таким образом, трава кажется зеленой, потому что она отражает зеленый свет, который поглощается хлорофиллом. \n"]},{"output_type":"stream","name":"stderr","text":["\n","llama_print_timings:        load time =   14636.85 ms\n","llama_print_timings:      sample time =      45.09 ms /   149 runs   (    0.30 ms per token,  3304.50 tokens per second)\n","llama_print_timings: prompt eval time =   14631.99 ms /    15 tokens (  975.47 ms per token,     1.03 tokens per second)\n","llama_print_timings:        eval time =   76900.53 ms /   148 runs   (  519.60 ms per token,     1.92 tokens per second)\n","llama_print_timings:       total time =   92487.25 ms /   163 tokens\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 12s, sys: 465 ms, total: 1min 12s\n","Wall time: 1min 32s\n"]}]},{"cell_type":"markdown","source":["### Функции"],"metadata":{"id":"6m1t2-GdKj_B"}},{"cell_type":"code","source":["# аннотации\n","CHAT_HISTORY = List[Tuple[Optional[str], Optional[str]]]\n","MODEL_DICT = Dict[str, Llama]\n","\n","# получение количества свободной памяти на диске, CPU и GPU\n","def get_memory_usage() -> str:\n","    print_memory = ''\n","\n","    memory_type = 'Disk'\n","    psutil_stats = psutil.disk_usage('.')\n","    memory_total = psutil_stats.total / 1024**3\n","    memory_usage = psutil_stats.used / 1024**3\n","    print_memory += f'{memory_type} Menory Usage: {memory_usage:.2f} / {memory_total:.2f} GB\\n'\n","\n","    memory_type = 'CPU'\n","    psutil_stats = psutil.virtual_memory()\n","    memory_total = psutil_stats.total / 1024**3\n","    memory_usage =  memory_total - (psutil_stats.available / 1024**3)\n","    print_memory += f'{memory_type} Menory Usage: {memory_usage:.2f} / {memory_total:.2f} GB\\n'\n","\n","    if torch.cuda.is_available():\n","        memory_type = 'GPU'\n","        memory_free, memory_total = torch.cuda.mem_get_info()\n","        memory_usage = memory_total - memory_free\n","        print_memory += f'{memory_type} Menory Usage: {memory_usage / 1024**3:.2f} / {memory_total:.2f} GB\\n'\n","\n","    print_memory = f'---------------\\n{print_memory}---------------\\n'\n","    return print_memory\n","\n","\n","def clear_memory() -> None:\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\n","# загрузка и инициализация модели GGUF\n","def load_model(model_id: str, model_file: str, n_ctx: int) -> Tuple[Dict[str, Llama], str]:\n","    model = None\n","    if isinstance(model_file, list):\n","        load_log = 'Не выбрана модель'\n","        return model, load_log\n","\n","    load_log = ''\n","    if '(' in model_file:\n","        model_file = model_file.split('(')[0].rstrip()\n","\n","    progress = gr.Progress()\n","    progress(0.3, desc='Шаг 1/2: Загрузка модели GGUF')\n","    model_path = MODELS_PATH / model_file\n","    if model_path.is_file():\n","        load_log += 'Модель уже загружена, повторная инициализация\\n'\n","    else:\n","        try:\n","            hf_hub_download(\n","                repo_id=model_id,\n","                filename=model_file,\n","                local_dir=MODELS_PATH,\n","                local_dir_use_symlinks=False,\n","                )\n","            load_log += 'Модель успешно загружена\\n'\n","        except Exception as ex:\n","            model_path = ''\n","            load_log += f'Ошибка загрузки модели, код ошибки:\\n{ex}\\n'\n","\n","    if model_path:\n","        progress(0.7, desc='Шаг 2/2: Инициализация модели')\n","        try:\n","            model = Llama(model_path=str(model_path), n_ctx=n_ctx, n_gpu_layers=-1)\n","            load_log += f'Модель {model_id}/{model_file} инициализирована\\n'\n","        except Exception as ex:\n","            load_log += f'Ошибка инициализации модели, код ошибки:\\n{ex}\\n'\n","\n","    model = {'model': model}\n","    clear_memory()\n","    return model, load_log\n","\n","\n","# загрузка и инициализация модели эмбедингов\n","def load_embed_model(model_id: str) -> Tuple[Dict[str, HuggingFaceEmbeddings], str]:\n","    embed_model = None\n","    if isinstance(model_id, list):\n","        load_log = 'Не выбрана модель'\n","        return embed_model, load_log\n","\n","    load_log = ''\n","    progress = gr.Progress()\n","\n","    folder_name = model_id.replace('/', '_')\n","    folder_path = EMBED_MODELS_PATH / folder_name\n","\n","    if Path(folder_path).is_dir():\n","        load_log += f'Повторная инициализация модели {model_id} \\n'\n","    else:\n","        progress(0.5, desc='Шаг 1/2: Загрузка модели')\n","        snapshot_download(\n","            repo_id=model_id,\n","            local_dir=folder_path,\n","            ignore_patterns='*.h5',\n","            local_dir_use_symlinks=False,\n","        )\n","\n","    progress(0.7, desc='Шаг 2/2: Инициализация модели')\n","    model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","    embed_model = HuggingFaceEmbeddings(\n","        model_name=str(folder_path),\n","        model_kwargs=model_kwargs,\n","        )\n","    load_log += f'Модель эмбедингов {model_id} инициализирована\\n'\n","    load_log += f'Выберите ретривер и загрузите документы повторно\\n'\n","\n","    embed_model = {'embed_model': embed_model}\n","    clear_memory()\n","    return embed_model, load_log\n","\n","\n","# добавление ноового репозитория HF new_model_id к текущему списку model_ids\n","def add_new_model_id(new_model_id: str, model_ids: List[str]) -> Tuple[gr.Dropdown, str]:\n","    load_log = ''\n","    model_id = new_model_id.strip()\n","\n","    if model_id:\n","        model_id = model_id.split('/')[-2:]\n","        if len(model_id) == 2:\n","            model_id = '/'.join(model_id).split('?')[0]\n","            if repo_exists(model_id) and model_id not in model_ids:\n","                if any([file_name.endswith('.gguf') for file_name in list_repo_files(model_id)]):\n","                    model_ids.insert(0, model_id)\n","                    load_log += f'Репозиторий модели {model_id} успешно добавлен\\n'\n","                else:\n","                    load_log += f'Не найдены модели GGUF в репозитории {model_id}\\n'\n","            else:\n","                load_log += 'Неверное название репозитория HF или модель уже есть в списке\\n'\n","        else:\n","            load_log += 'Неверная ссылка на репозиторий HF\\n'\n","    else:\n","        load_log += 'Пустая строка в поле репозитория HF\\n'\n","\n","    model_id_dropdown = gr.Dropdown(choices=model_ids, value=model_ids[0])\n","    return model_id_dropdown, load_log\n","\n","\n","# получить список моделей GGUF из репозитория HF\n","def get_gguf_model_names(model_id: str) -> gr.Dropdown:\n","    repo_files = list(list_repo_tree(model_id))\n","    repo_files = [file for file in repo_files if file.path.endswith('.gguf')]\n","    model_paths = [f'{file.path} ({file.size / 1000 ** 3:.2f}G)' for file in repo_files]\n","\n","    model_paths_dropdown = gr.Dropdown(\n","        choices=model_paths,\n","        value=model_paths[0],\n","        label='Файл модели GGUF',\n","        )\n","    return model_paths_dropdown\n","\n","\n","# удаление файлов и папок моделей для очистки места кроме текущей модели ignore_link\n","def clear_folder(ignore_link: str) -> None:\n","    folder = EMBED_MODELS_PATH\n","    if len(ignore_link.split('/')) != 2:\n","        folder = MODELS_PATH\n","        if '(' in ignore_link:\n","            ignore_link = ignore_link.split('(')[0].rstrip()\n","\n","    for path in folder.iterdir():\n","        if path.name == ignore_link:\n","            continue\n","        if path.is_file():\n","            path.unlink()\n","        elif path.is_dir():\n","            rmtree(path)\n","    clear_memory()\n","\n","# очистка текста\n","def clear_text(text: str) -> str:\n","    lines = text.split('\\n')\n","    # брать строки, кол-во символов в которых больше 2\n","    lines = [line for line in lines if len(line.strip()) > 2]\n","    text = '\\n'.join(lines).strip()\n","    return text\n","\n","\n","# очистка списка документов\n","def clear_documents(documents: List[Document]) -> List[Document]:\n","    output_documents = []\n","    # итерация по документам\n","    for document in documents:\n","        # очистка текста текущего документа\n","        text = clear_text(document.page_content)\n","        # берем документы с длиной текста более 1\n","        if len(text) > 10:\n","            # записываем очищенный текст обратно в документ и добавляем в список\n","            document.page_content = text\n","            output_documents.append(document)\n","    return output_documents\n","\n","\n","# получение разделителя для csv файла на слуяай если он отличется от ','\n","def get_csv_delimiter(file_path: str) -> str:\n","    n_bytes = 4096\n","    with open(file_path) as csvfile:\n","        delimiter = csv.Sniffer().sniff(csvfile.read(n_bytes)).delimiter\n","    return delimiter\n","\n","\n","# функция проверки доступности субтитров, если доступны ручные или автоматические - возвращает True и логи\n","# если субтитры недоступны - возвращает False и логи\n","def check_subtitles_available(yt_video_link: str, target_lang: str) -> Tuple[bool, str]:\n","    # извлечение ID видео из полной ссылки видео на YouTube\n","    video_id = yt_video_link.split('watch?v=')[-1].split('&')[0]\n","    # строка с логами\n","    load_log = ''\n","    # статус доступности субтитров\n","    available = True\n","    try:\n","        # доступные языки субтитров для текущего видео\n","        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n","        try:\n","            # поиск языка target_lang в доступных языках\n","            transcript = transcript_list.find_transcript([target_lang])\n","            # проверка что субтитры автоматические или ручные\n","            if transcript.is_generated:\n","                load_log += f'Будут загружены автоматические субтитры, ручные недоступны для видео {yt_video_link}\\n'\n","            else:\n","                load_log += f'Будут загружены ручные субтитры для видео {yt_video_link}\\n'\n","\n","        # если субтитры для видео есть но нет субтитров на желаемом языке\n","        except NoTranscriptFound:\n","            load_log += f'Язык субтитров {target_lang} недоступен для видео {yt_video_link}\\n'\n","            available = False\n","\n","    # если никаких субтитров для видео нет\n","    except TranscriptsDisabled:\n","        load_log += f'Нет субтитров для видео {yt_video_link}\\n'\n","        available = False\n","\n","    return available, load_log\n","\n","\n","# извлечение документов (в формате langchain Documents) из загруженных файлов\n","def load_documents_from_files(upload_files: List[str]) -> Tuple[List[Document], str]:\n","    # логи загрузки и список для загруженных документов\n","    load_log = ''\n","    documents = []\n","    # итерация по путям до файлов\n","    for upload_file in upload_files:\n","        # получение расширения файла\n","        file_extension = f\".{upload_file.split('.')[-1]}\"\n","        # если файл расширения есть в словаре с лоадерами LOADER_CLASSES\n","        if file_extension in LOADER_CLASSES:\n","            # извлекаем соотвествующий раширению лоадер\n","            loader_class = LOADER_CLASSES[file_extension]\n","            loader_kwargs = {}\n","            # если это csv то дополнительно находим разделитель\n","            if file_extension == '.csv':\n","                delimiter = get_csv_delimiter(upload_file)\n","                loader_kwargs = {'csv_args': {'delimiter': delimiter}}\n","            try:\n","                # получаем документы из файлов\n","                load_documents = loader_class(upload_file, **loader_kwargs).load()\n","                documents.extend(load_documents)\n","            except Exception as ex:\n","                load_log += f'Ошибка загрузки файла {upload_file}\\n'\n","                load_log += f'Код ошибки: {ex}\\n'\n","                continue\n","        else:\n","            load_log += f'Неподдерживаемый формат файла {upload_file}\\n'\n","            continue\n","    return documents, load_log\n","\n","\n","# извлечение документов (в формате langchain Documents) из WEB ссылок\n","def load_documents_from_links(\n","        web_links: str,  # ссылки на web-сайты или на видео YouTube\n","        subtitles_lang: str,  # желаемый язык субтитров на YouTube\n","        ) -> Tuple[List[Document], str]:\n","\n","    # логи загрузки, список для документов и параметры для лоадера\n","    load_log = ''\n","    documents = []\n","    loader_class_kwargs = {}\n","\n","    # фильтрация пустых строк\n","    web_links = [web_link.strip() for web_link in web_links.split('\\n') if web_link.strip()]\n","\n","    # итераци по переданным ссылкам\n","    for web_link in web_links:\n","        # ----------------- ссылка на YouTube ---------------------------------\n","        if 'youtube.com' in web_link:\n","            # проверка что субтитры на выбранном языке subtitles_lang доступны в видео web_link\n","            available, log = check_subtitles_available(web_link, subtitles_lang)\n","            load_log += log\n","\n","            # если субтитры недоступны - пропускаем ссылку\n","            if not available:\n","                continue\n","\n","            # инициализация YouTubeLoader с параметром языка субтитров\n","            loader_class = LOADER_CLASSES['youtube'].from_youtube_url\n","            loader_class_kwargs = {'language': subtitles_lang}\n","\n","        # ----------------- ссылка не на YouTube ------------------------------\n","        else:\n","            # инициализация WebBaseLoader\n","            loader_class = LOADER_CLASSES['web']\n","        try:\n","            # проверка что сайт доступен для парсинга с помощью python requests\n","            if requests.get(web_link).status_code != 200:\n","                load_log += f'Ссылка недоступна для Python requests: {web_link}\\n'\n","                continue\n","            # если доступен то загружаем документ с текстом из web ссылки\n","            load_documents = loader_class(web_link, **loader_class_kwargs).load()\n","            if len(load_documents) == 0:\n","                load_log += f'Фрагменты текста не были найдены по ссылке: {web_link}\\n'\n","                continue\n","            documents.extend(load_documents)\n","        except MissingSchema:\n","            # если ссылка неверная\n","            load_log += f'Неверная ссылка: {web_link}\\n'\n","            continue\n","        except Exception as ex:\n","            # если какая то другая ошибка\n","            load_log += f'Ошибка загрузки web лоадером данных по ссылке: {web_link}\\n'\n","            load_log += f'Код ошибки: {ex}\\n'\n","            continue\n","    return documents, load_log\n","\n","\n","# загрузка файлов и формирование документов и БД\n","def load_documents_and_create_db(\n","        upload_files: Optional[List[str]],  # пути до файлов, загружаемые через gr.File()\n","        web_links: str,  # ссылки на web-сайты или на видео YouTube\n","        subtitles_lang: str,  # желаемый язык субтитров на YouTube\n","        chunk_size: int,  # кол-во символов\n","        chunk_overlap: int,\n","        embed_model,\n","        ) -> Tuple[List[Document], Optional[BaseRetriever], str]:\n","\n","    # логи загрузки, список для документов из всех источников, БД и прогресс бар Gradio\n","    load_log = ''\n","    all_documents = []\n","    db = None\n","    progress = gr.Progress()\n","\n","    # если не переданы ни пути до файлов ни ссылки\n","    if upload_files is None and not web_links:\n","        load_log = 'Не выбраны файлы или ссылки'\n","        return all_documents, db, load_log\n","\n","    # загрузка документов из файлов\n","    if upload_files is not None:\n","        progress(0.3, desc='Шаг 1/2: Загрузка документов из файлов')\n","        docs, log = load_documents_from_files(upload_files)\n","        all_documents.extend(docs)\n","        load_log += log\n","\n","    # загрузка документов по ссылкам\n","    if web_links:\n","        progress(0.3 if upload_files is None else 0.5, desc='Шаг 1/2: Загрузка документов по ссылкам')\n","        docs, log = load_documents_from_links(web_links, subtitles_lang)\n","        all_documents.extend(docs)\n","        load_log += log\n","\n","    # если не загружено ни одного документа\n","    if len(all_documents) == 0:\n","        load_log += 'Загрузка прервана так как не было извлечено ни одного документа\\n'\n","        load_log += 'Режим RAG не может быть активирован'\n","        return all_documents, db, load_log\n","\n","    load_log += f'Загружено документов: {len(all_documents)}\\n'\n","\n","    # разделить документы на фрагменты и очистка фрагментов\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size,\n","        chunk_overlap=chunk_overlap,\n","    )\n","    documents = text_splitter.split_documents(all_documents)\n","    documents = clear_documents(documents)\n","    load_log += f'Документы разделены, кол-во фрагментов текста: {len(documents)}\\n'\n","\n","    # инициализация БД\n","    progress(0.7, desc='Шаг 2/2: Инициализация БД')\n","    db = FAISS.from_documents(documents=documents, embedding=embed_model)\n","    load_log += 'Режим RAG активирован и может быть дективирован на вкладке Generate'\n","    return documents, db, load_log\n","\n","\n","# добавление сообщение пользователя в окошко чат бота\n","def user_message_to_chatbot(user_message: str, chatbot: CHAT_HISTORY) -> Tuple[str, CHAT_HISTORY]:\n","    if user_message:\n","        chatbot.append((user_message, None))\n","    return '', chatbot\n","\n","\n","# форматирование промта с добавлением контекста, если БД доступна и режим RAG активирован\n","# иначе формирование промта без контекста - для простого общения с ботом\n","def update_user_message_with_context(\n","        chatbot: List[List[Optional[str]]],  # объект чатбота - список списков реплик юзера и бота\n","        rag_mode: bool,  # режим RAG - включен или отключен\n","        db: VectorStore,  # БД\n","        k: Union[int, str],  # кол-во релевантных запросу пользователя фрагментов текста для контекста\n","        score_threshold: float,  # порого по которому ищутся релевантные фрагменты текста для контекста\n","        ) -> Tuple[str, List[List[Optional[str]]], str]:\n","\n","    # текущее сообщение пользователя\n","    user_message = chatbot[-1][0]\n","\n","    # если сообщение пользователя пустое - обработаем это в следующей функции generate_text\n","    if not user_message.strip():\n","        return '', chatbot\n","\n","    # если БД готова и включен режим RAG то ищем релевантные доки и добавляем в промт\n","    if db is not None and rag_mode:\n","        # использовать все документы, скор которых больше score_threshold\n","        if k == 'max':\n","            k = len(db.docstore._dict)\n","        # поиск релевантных фрагментов текста\n","        docs_and_distances = db.similarity_search_with_relevance_scores(\n","            user_message,\n","            k=k,\n","            score_threshold=score_threshold,\n","            )\n","        # если релевантные документы не найдены\n","        if len(docs_and_distances) == 0:\n","            gr.Info((\n","                f'Релевантные запросу документы не найдены, '\n","                f'используется промт без контекста (попробуйте уменьшить searh_score_threshold)'\n","                ))\n","        else:\n","            # формирование контекста из релевантных фрагментов текста\n","            retriever_context = '\\n\\n'.join([doc[0].page_content for doc in docs_and_distances])\n","            # обогащение сообщения юзера контекстом\n","            user_message = CONTEXT_TEMPLATE.format(\n","                user_message=user_message,\n","                context=retriever_context,\n","                )\n","\n","    # добавление в чат бот текущего сообщения юзера чтобы оно было видно на экране\n","    chatbot.append([user_message, None])\n","    return '', chatbot\n","\n","\n","# получить текст текущего промта с контекстом\n","def get_user_message_with_context(chatbot, rag_mode) -> gr.component:\n","    user_message = chatbot[-1][0] if len(chatbot) > 0 else ''\n","    return gr.Texbox(user_message, visible=rag_mode, interactive=False)\n","\n","\n","# генерация текста моделью\n","def get_llm_response(\n","        chatbot: CHAT_HISTORY,  # список переписок пользователя и бота, который отображается в окошке чат-бота\n","        model_dict: MODEL_DICT,  # словарь с модель llama-cpp-python\n","        system_prompt: str,  # системный промт\n","        support_system_role: bool,  # поддержиывает ли модель системный промт\n","        history_len: int,  # кол-во предыдущих сообщений которые учитываются в истории\n","        do_sample: bool,  # использовать ли случайное семплирование при генерации ответа моделью\n","        *generate_args,  # параметры генерации ответа модели\n","        ) -> List[List[Optional[str]]]:\n","\n","\n","    # если сообщение пользователя пустое - обработаем это в следующей функции generate_text\n","    user_message = chatbot[-1][0]\n","    if not user_message.strip():\n","        return '', chatbot\n","\n","    # получение модели\n","    model = model_dict.get('model')\n","    # список с переписками юзера и бота\n","    messages = []\n","\n","    # обновление словаря параметров генерации текста\n","    gen_kwargs = dict(zip(GENERATE_KWARGS.keys(), generate_args))\n","    gen_kwargs['top_k'] = int(gen_kwargs['top_k'])\n","\n","    # отключение случайного семплирования если do_sample=False\n","    if not do_sample:\n","        gen_kwargs['top_p'] = 0.0\n","        gen_kwargs['top_k'] = 1\n","        gen_kwargs['repeat_penalty'] = 1.0\n","\n","    # добавление системного промта если он есть и поддерживается моделью\n","    if support_system_role and system_prompt:\n","        messages.append({'role': 'system', 'content': system_prompt})\n","\n","    # добавление истории переписки в промт\n","    if history_len != 0:\n","        for user_msg, bot_msg in chatbot[:-1][-history_len:]:\n","            print(user_msg, bot_msg)\n","            messages.append({'role': 'user', 'content': user_msg})\n","            messages.append({'role': 'assistant', 'content': bot_msg})\n","\n","    # формирование генератора для генерации ответа моделью с форматированием промта\n","    stream_response = model.create_chat_completion(\n","        messages=messages,\n","        stream=True,\n","        **gen_kwargs,\n","        )\n","\n","    # пустую строку будем конкатенировать с токенами ответа модели\n","    chatbot[-1][1] = ''\n","    # итерация и генерация токенов ответа модели в цикле\n","    for chunk in stream_response:\n","        token = chunk['choices'][0]['delta'].get('content')\n","        if token is not None:\n","            chatbot[-1][1] += token\n","            yield chatbot\n","\n","\n","# получение окошка для системного промта. если interactive=True то можно вводить системный промт\n","# примеры системных промтов:\n","# Отвечай на вопросы максимально кратко, если не знаешь ответ - говори \"не знаю\".\n","# Если в вопросе есть хоть малейший намек на политику, насилие, ругательства, ответь так: \"На эту тему я не могу говорить\".\n","def get_system_prompt_component(interactive: bool) -> gr.Textbox:\n","    value = '' if interactive else 'System prompt is not supported by this model'\n","    return gr.Textbox(value=value, label='System prompt', interactive=interactive)\n","\n","\n","# компоненты настройки конфига генерации текста\n","def get_generate_args(do_sample: bool) -> List[gr.component]:\n","    # если do_sample включен (элемент gr.Checkbox() активен) то отображать слайдера с параметрами генерации\n","    generate_args = [\n","        gr.Slider(label='temperature', value=GENERATE_KWARGS['temperature'], minimum=0.1, maximum=3, step=0.1, visible=do_sample),\n","        gr.Slider(label='top_p', value=GENERATE_KWARGS['top_p'], minimum=0.1, maximum=1, step=0.1, visible=do_sample),\n","        gr.Slider(label='top_k', value=GENERATE_KWARGS['top_k'], minimum=1, maximum=50, step=5, visible=do_sample),\n","        gr.Slider(label='repeat_penalty', value=GENERATE_KWARGS['repeat_penalty'], minimum=1, maximum=5, step=0.1, visible=do_sample),\n","    ]\n","    return generate_args"],"metadata":{"id":"I8LD34gFKxEv","executionInfo":{"status":"ok","timestamp":1727693914687,"user_tz":-180,"elapsed":2979,"user":{"displayName":"Сергей","userId":"08873757262896960569"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["### Приложение"],"metadata":{"id":"vbODSV_KKlv4"}},{"cell_type":"markdown","source":["Запуск приложения"],"metadata":{"id":"orsFLHsFe7Lt"}},{"cell_type":"code","source":["# классы langchain для извлечения текста из различных источников\n","LOADER_CLASSES = {\n","    '.csv': CSVLoader,\n","    '.doc': UnstructuredWordDocumentLoader,\n","    '.docx': UnstructuredWordDocumentLoader,\n","    '.html': UnstructuredHTMLLoader,\n","    '.md': UnstructuredMarkdownLoader,\n","    '.pdf': PDFMinerLoader,\n","    '.ppt': UnstructuredPowerPointLoader,\n","    '.pptx': UnstructuredPowerPointLoader,\n","    '.txt': TextLoader,\n","    'web': WebBaseLoader,\n","    'directory': DirectoryLoader,\n","    'youtube': YoutubeLoader,\n","}\n","\n","# языки для субтитров YouTube\n","SUBTITLES_LANGUAGES = ['ru', 'en']\n","\n","# пример видео с автоматически сгенерированными субтитрами\n","# https://www.youtube.com/watch?v=CFVABT8wtl4\n","# пример видео с обычными субтитрами\n","# https://www.youtube.com/watch?v=EEGk7gHoKfY\n","\n","# начальные настройки бота при первом запуске\n","HISTORY_LEN = 0\n","\n","# шаблон промта при условии контекста\n","CONTEXT_TEMPLATE = '''Ответь на вопрос при условии контекста.\n","\n","Контекст:\n","{context}\n","\n","Вопрос:\n","{user_message}\n","\n","Ответ:'''\n","\n","# параметры генерации\n","# чтобы модель отвечала одинаково достаточно поставить top_k=1, top_p=0 и repeat_penalty=1 не зависимо от остальных параметров\n","GENERATE_KWARGS = dict(\n","    temperature=1,  # температура для софтмакса\n","    top_p=0.0,  # сумма вероятностей токенов из которых нужно выбирать следующий токен\n","    top_k=1,  # из скольки максимально вероятных токенов выбирать следующий токен\n","    repeat_penalty=1.0,  # штраф модели за повторения\n","    )\n","\n","\n","# ====================== ИНТЕРФЕЙС ПРИЛОЖЕНИЯ ============================\n","\n","# тема оформления и CSS\n","# theme = gr.themes.Monochrome()\n","theme = gr.themes.Base(primary_hue='green', secondary_hue='yellow', neutral_hue='zinc').set(\n","    loader_color='rgb(0, 255, 0)',\n","    slider_color='rgb(0, 200, 0)',\n","    body_text_color_dark='rgb(0, 200, 0)',\n","    button_secondary_background_fill_dark='green',\n",")\n","css = '''.gradio-container {width: 60% !important}'''\n","\n","# начало описания интерфейса приложения\n","with gr.Blocks(theme=theme, css=css) as interface:\n","\n","    # ==================== СОСТОЯНИЯ ===============================\n","\n","    # загруженные фрагменты текста (список объектов langchain Document)\n","    documents = gr.State([])\n","    # БД\n","    db = gr.State(None)\n","    # обогащенный контекстом промт пользователя\n","    user_message_with_context = gr.State('')\n","    # флаг поддержки системного промта моделью\n","    support_system_role = gr.State(start_support_system_role)\n","\n","    model = gr.State({'model': start_model})\n","    embed_model = gr.State({'embed_model': start_embed_model})\n","\n","    # ==================== СТРАНИЦА БОТА =================================\n","\n","    # функция получения окна чат бота с названием текущего режима - RAG или Chatbot\n","    # а так же параметров RAG -\n","    def get_rag_settings(rag_mode: bool):\n","        # кол-во релевантных фрагментов текста для поиска в БД\n","        k = gr.Radio(\n","            choices=[1, 2, 3, 'max'],\n","            value=2,\n","            label='Количество релевантных документов для поиска',\n","            visible=rag_mode,\n","            render=False,\n","            )\n","\n","        # порог для поиска релевантных фрагментов текста от 0 до 1 (чем ниже тем больше документов будет найдено)\n","        score_threshold = gr.Slider(\n","            label='searh_score_threshold',\n","            value=0.5,\n","            minimum=0,\n","            maximum=1,\n","            step=0.1,\n","            visible=rag_mode,\n","            render=False,\n","            )\n","        return k, score_threshold\n","\n","\n","    with gr.Tab(label='Generate'):\n","        with gr.Row():\n","            with gr.Column(scale=3):\n","                # окошко чат бота\n","                chatbot = gr.Chatbot(\n","                    show_copy_button=True,\n","                    bubble_full_width=False,\n","                    height=480,\n","                )\n","                # сообщение пользователя\n","                user_message = gr.Textbox(label='User')\n","\n","                with gr.Row():\n","                    # кнопки отправить сообщение, стоп генерации и удалить историю чата\n","                    user_message_btn = gr.Button('Отправить')\n","                    stop_btn = gr.Button('Стоп')\n","                    clear_btn = gr.Button('Очистить чат')\n","\n","\n","            # ------------------ ПАРАМЕТРЫ ГЕНЕРАЦИИ -------------------------\n","\n","            with gr.Column(scale=1, min_width=80):\n","                with gr.Group():\n","                    # длина истории которую будет учитывать модель\n","                    gr.Markdown('Размер истории')\n","                    history_len = gr.Slider(\n","                        minimum=0,\n","                        maximum=5,\n","                        value=HISTORY_LEN,\n","                        step=1,\n","                        info='Кол-во предыдущих сообщенией, учитываемых в истории',\n","                        label='history len',\n","                        show_label=False,\n","                        )\n","\n","                    with gr.Group():\n","                        gr.Markdown('Параметры генерации')\n","                        # переключатель активации случайного семплирования при генерации текста моделью\n","                        do_sample = gr.Checkbox(\n","                            value=False,\n","                            label='do_sample',\n","                            info='Активация случайного семплирования',\n","                            )\n","                        # настройки семплирования\n","                        generate_args = get_generate_args(do_sample.value)\n","                        do_sample.change(\n","                            fn=get_generate_args,\n","                            inputs=do_sample,\n","                            outputs=generate_args,\n","                            show_progress=False,\n","                            )\n","\n","        # переключатель включить или выключить режим RAG (даже если БД готова к работе RAG можно отключить)\n","        rag_mode = gr.Checkbox(value=False, label='Режим RAG', scale=1, visible=False)\n","        rag_mode.change(\n","            fn=get_rag_settings,\n","            inputs=[rag_mode],\n","            outputs=[k, score_threshold],\n","            )\n","\n","        # отобразить в этом месте экрана параметры k и score_threshold\n","        # число релевантных фргаментов текста для контекста и параметр score_threshold\n","        k, score_threshold = get_rag_settings(rag_mode)\n","        with gr.Row()\n","            k.render()\n","            score_threshold.render()\n","\n","        # -------------------- СИСТЕМНЫЙ ПРОМТ И ИТОГОВЫЙ ПРОМТ ---------------------------\n","\n","        with gr.Accordion('Промт', open=True):\n","            # окошко для системного промта\n","            system_prompt = get_system_prompt_component(interactive=support_system_role.value)\n","            # итоговыый промт который подается в модель\n","            user_message_with_context = get_user_message_with_context(gr.Chatbot().value, rag_mode.value)\n","\n","        # ------------------ КНОПКИ ОТПРАВИТЬ ОЧИСТИТЬ И СТОП ------------\n","\n","        # нажатие Enter и кнопка отправить\n","        generate_event = gr.on(\n","            triggers=[user_message.submit, user_message_btn.click],\n","            fn=get_promt_with_context,\n","            inputs=[user_message, chatbot, history_len, rag_mode, db, system_prompt, k, score_threshold],\n","            outputs=[user_message, chatbot, full_promt],\n","            queue=True,\n","        ).then(\n","            fn=lambda promt: promt,\n","            inputs=full_promt,\n","            outputs=full_promt,\n","            queue=False,\n","        ).then(\n","            fn=generate_text,\n","            inputs=[chatbot, full_promt, do_sample, *generate_args],\n","            outputs=[chatbot],\n","            queue=True,\n","            )\n","\n","        # кнопка Стоп\n","        stop_btn.click(\n","            fn=None,\n","            inputs=None,\n","            outputs=None,\n","            cancels=generate_event,\n","            queue=False,\n","        )\n","\n","        # кнопка Очистить чат\n","        clear_btn.click(\n","            fn=lambda: (None, ''),\n","            inputs=None,\n","            outputs=[chatbot, full_promt],\n","            queue=False,\n","            )\n","\n","\n","    # ===================== СТРАНИЦА ЗАГРУЗКИ ФАЙЛОВ =========================\n","\n","    with gr.Tab(label='Load documents'):\n","        with gr.Row(variant='compact'):\n","            # загрузка файлов и ссылок\n","            upload_files = gr.File(file_count='multiple', label='Загрузка текстовых файлов')\n","            web_links = gr.Textbox(lines=6, label='Ссылки на Web сайты или Ютуб')\n","\n","        with gr.Row(variant='compact'):\n","            # параметры нарезки текста на фрагменты\n","            chunk_size = gr.Slider(50, 2000, value=500, step=50, label='Длина фрагментов')\n","            chunk_overlap = gr.Slider(0, 200, value=20, step=10, label='Длина пересечения фрагментов')\n","\n","            # язык субтитров\n","            subtitles_lang = gr.Radio(\n","                SUBTITLES_LANGUAGES,\n","                value=SUBTITLES_LANGUAGES[0],\n","                label='Язык субтитров YouTube',\n","                )\n","\n","        # кнопка загрузки документов и инициализации БД\n","        load_documents_btn = gr.Button(value='Загрузить документы и инициализировать БД')\n","        # статус прогресса загрузки файлов и инициализации БД\n","        load_docs_log = gr.Textbox(label='Прогресс загрузки и разделения документов', interactive=False)\n","\n","        # главный цикл загрузки доков и инициализации ретривера\n","        load_event = load_documents_btn.click(\n","            fn=load_documents_and_create_db,\n","            inputs=[upload_files, web_links, subtitles_lang, chunk_size, chunk_overlap],\n","            outputs=[documents, db, load_docs_log],\n","            )\n","\n","        # сменить название бота на RAG или Chatbot в зависимости от готовности ретривера\n","        def documents_load_success(chatbot_history, db):\n","            rag_mode = db is not None\n","            chatbot, k, score_threshold = get_chatbot_and_rag_settings(chatbot_history, rag_mode)\n","            rag_mode_checkbox = gr.Checkbox(value=rag_mode, label='Режим RAG', scale=1, visible=rag_mode)\n","            return rag_mode_checkbox\n","\n","        load_event.success(\n","            fn=documents_load_success,\n","            inputs=[chatbot, db],\n","            outputs=[rag_mode],\n","            )\n","\n","\n","    # ================= СТРАНИЦА ПРОСМОТРА ВСЕХ ДОКУМЕНТОВ =================\n","\n","    with gr.Tab(label='View documents'):\n","        # кнопка и текстовое поле для отображения загруженных фрагментов текста\n","        view_documents_btn = gr.Button(value='Отобразить загруженные фрагменты')\n","        view_documents_textbox = gr.Textbox(\n","            lines=1,\n","            placeholder='Для просмотра фрагментов загрузите документы на вкладке Load documents',\n","            label='Загруженные фрагменты',\n","            )\n","        sep = '=' * 20\n","        # отображение загруженных фрагментов текста если они готовы\n","        view_documents_btn.click(\n","            lambda documents: f'\\n{sep}\\n\\n'.join([doc.page_content for doc in documents]),\n","            inputs=documents,\n","            outputs=view_documents_textbox,\n","        )\n","\n","    # ============== СТРАНИЦА ЗАГРУЗКИ GGUF МОДЕЛЕЙ =====================\n","\n","    with gr.Tab('Load model'):\n","        new_model_id = gr.Textbox(\n","            value='',\n","            label='Добавить репозиторий',\n","            placeholder='Ссылка на репозиторий HF моделей в формате GGUF',\n","            )\n","        new_model_btn = gr.Button('Добавить репозиторий')\n","\n","        model_id = gr.Dropdown(\n","            choices=MODELS_IDS,\n","            value=None,\n","            label='Репозиторий модели HF',\n","            )\n","\n","        model_path = gr.Dropdown(\n","            choices=[],\n","            value=None,\n","            label='Файл модели GGUF',\n","            )\n","\n","        n_ctx = gr.Slider(500, 500 * 8, step=500, label='n_ctx')\n","        load_model_btn = gr.Button('Загрука и инициализация модели')\n","\n","        load_model_log = gr.Textbox(\n","            value=f'Модель {MODELS_IDS[0]} загружена по умолчанию',\n","            label='Статус загрузки модели',\n","            )\n","\n","        with gr.Group():\n","            gr.Markdown('Освободить место на диске путем удаления всех моделей кроме текущей')\n","            remove_models_btn = gr.Button('Очистить папку')\n","\n","        new_model_btn.click(\n","            fn=add_new_model_id,\n","            inputs=[new_model_id, model_ids_state],\n","            outputs=[model_id, load_model_log],\n","        ).success(\n","            fn=lambda: '',\n","            inputs=None,\n","            outputs=new_model_id,\n","        )\n","\n","        model_id.change(\n","            fn=get_gguf_model_names,\n","            inputs=[model_id],\n","            outputs=[model_path],\n","        )\n","\n","        load_model_btn.click(\n","            fn=load_model,\n","            inputs=[model_id, model_path, n_ctx],\n","            outputs=[model, load_model_log],\n","            queue=True,\n","        ).success(\n","            fn=lambda log: log + get_memory_usage(),\n","            inputs=load_model_log,\n","            outputs=load_model_log,\n","        )\n","\n","        remove_models_btn.click(\n","            fn=clear_folder,\n","            inputs=[model_path],\n","            outputs=None,\n","        ).success(\n","            fn=lambda model: f'Модели кроме {model} удалены',\n","            inputs=model_path,\n","            outputs=None,\n","        )\n","\n","    # ============== СТРАНИЦА ЗАГРУЗКИ ЭМБЕДИНГ МОДЕЛЕЙ =================\n","    with gr.Tab('Load embed model'):\n","        new_embed_id = gr.Textbox(\n","            value='',\n","            label='Добавить репозиторий',\n","            placeholder='Ссылка на репозиторий модели HF',\n","            )\n","        new_embed_btn = gr.Button('Добавить репозиторий')\n","\n","        embed_id = gr.Dropdown(\n","            choices=EMBEDERS_IDS,\n","            value=None,\n","            label='Репозиторий модели HF',\n","            )\n","\n","        load_embed_btn = gr.Button('Загрука и инициализация модели')\n","        load_embed_log = gr.Textbox(\n","            value=f'Модель {EMBEDERS_IDS[0]} загружена по умолчанию',\n","            label='Статус загрузки модели',\n","            )\n","        with gr.Group():\n","            gr.Markdown('Освободить место на диске путем удаления всех моделей кроме текущей')\n","            remove_embed_models_btn = gr.Button('Очистить папку')\n","\n","        new_embed_btn.click(\n","            fn=add_new_model_id,\n","            inputs=[new_embed_id, embed_ids_state],\n","            outputs=[embed_id, load_embed_log],\n","        ).success(\n","            fn=lambda: '',\n","            inputs=None,\n","            outputs=new_embed_id,\n","        )\n","\n","        load_embed_btn.click(\n","            fn=load_embed_model,\n","            inputs=[embed_id],\n","            outputs=[embed_model, load_embed_log],\n","        ).success(\n","            fn=lambda log: log + get_memory_usage(),\n","            inputs=load_embed_log,\n","            outputs=load_embed_log,\n","        )\n","\n","        remove_embed_models_btn.click(\n","            fn=clear_folder,\n","            inputs=[embed_id],\n","            outputs=None,\n","        ).success(\n","            fn=lambda model: f'Модели кроме {model} удалены',\n","            inputs=embed_id,\n","            outputs=None,\n","        )\n","\n","\n","demo.launch(debug=True)  # server_name='0.0.0.0'"],"metadata":{"id":"HcJJK_3K8A1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Структура проекта и итоговый код по модулям"],"metadata":{"id":"A7w7hpUsKpM1"}},{"cell_type":"markdown","source":["Код итогового приложения (не для для запуска в Colab)"],"metadata":{"id":"WY4-lIAnkaPP"}},{"cell_type":"markdown","source":["**Структура проекта:**\n","\n"," - 📁 `embed_model`\n"," - 📁 `model`\n"," - 📁 `tokenizer`\n"," - `requirements-base.txt`\n"," - `requirements-cpu.txt`\n"," - `requirements-cuda.txt`\n"," - `app.py`\n"," - `models.py`\n"," - `utils.py`\n","\n"],"metadata":{"id":"IASD77EW1_G3"}},{"cell_type":"markdown","source":["**Установка и запуск:**\n","\n","1. Создание и активация виртуального окружения (опционально)\n","\n"," Создание окружения:\n"," ```\n","python3 -m venv env\n"," ```\n"," Активация окружения:\n"," - Linux\n","```\n","source env/bin/activate\n","```\n","\n"," - Windows\n","```\n","env\\Scripts\\activate.bat\n","```\n","\n","2. Установка библиотек\n"," - с поддержкой CPU\n"," ```\n"," pip install -r requirements-cpu.txt\n"," ```\n","\n"," - с поддержкой CUDA\n","\n","     - Linux\n","     ```\n","     CMAKE_ARGS='-DLLAMA_CUBLAS=on' FORCE_CMAKE=1 pip install -r requirements-cuda.txt\n","     ```\n","     - Windows - потребуются дополнительно инструменты для сборки, [инструкции](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#installation)\n","     ```\n","     set CMAKE_ARGS=-DLLAMA_CUBLAS=ON && set CMAKE_ARGS=-DLLAMA_CUBLAS=ON\n","     pip install llama-cpp-python\n","     ```\n","\n","3. Запуск приложения\n","```\n","python3 app.py\n","```\n"," Перейти в браузере http://localhost:7860/ после того как появится надпись `Running on local URL:  http://127.0.0.1:7860`"],"metadata":{"id":"JnLtJ518xLrE"}},{"cell_type":"markdown","source":["Чтобы заменить модели и токенайзер на другие, изменить переменные в модуле `models.py`:\n","\n"," - `tokenizer_name` - название репозитория на HF\n"," - `embed_model_name` - название репозитория на HF\n"," - `llm_model_url` - прямая ссылка на модель в формате GGUF"],"metadata":{"id":"0y8kSwWJN5Pb"}},{"cell_type":"markdown","source":["**Содержимое файлов `requirements:`**\n","\n","`requirements-base.txt`\n","```\n","accelerate==0.31.0\n","gradio==4.36.1\n","langchain==0.2.3\n","langchain-community==0.2.4\n","langchain-core==0.2.5\n","langchain-huggingface==0.0.3\n","langchain-text-splitters==0.2.1\n","llama_cpp_python==0.2.77\n","pdfminer.six==20231228\n","sentence-transformers==3.0.1\n","transformers==4.41.2\n","youtube-transcript-api==0.6.2\n","```\n","\n","`requirements-cpu.txt`\n","```\n","--extra-index-url https://download.pytorch.org/whl/cpu\n","torch==2.3.0\n","faiss-cpu==1.8.0\n","-r ./requirements-base.txt\n","```\n","\n","`requirements-cuda.txt`\n","```\n","--extra-index-url https://download.pytorch.org/whl/cu121\n","torch==2.3.0\n","faiss-cpu==1.8.0\n","-r ./requirements-base.txt\n","```\n","\n","Если в системе установлена CUDA не 12 а 11 версии - заменить в `requirements-cuda.txt`  \n","`https://download.pytorch.org/whl/cu121`  \n","на  \n","`https://download.pytorch.org/whl/cu118`  \n","для корректной устаноки Pytorch\n","\n","Версия `Python:` `3.8+` (проверено на `3.10`)\n","\n","Папка `env` с зависимостями для CPU занимает 1.5Gb, для CUDA 6Gb"],"metadata":{"id":"ix3buTO-yK_M"}},{"cell_type":"markdown","source":["### Модуль `models.py`"],"metadata":{"id":"iVg5n13XXpR5"}},{"cell_type":"markdown","source":["Модуль `models.py` для загрузки и инициализации моделей и токенайзера"],"metadata":{"id":"1UDM-0jsXvxK"}},{"cell_type":"code","source":["from pathlib import Path\n","from typing import List, Tuple, Dict, Union, Iterable, Optional, Any\n","from tqdm import tqdm\n","\n","import requests\n","import torch\n","\n","from transformers import AutoTokenizer, GenerationConfig\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from llama_cpp import Llama\n","\n","\n","def download_file(file_url: str, file_path: Union[str, Path]) -> None:\n","    response = requests.get(file_url, stream=True)\n","    total_size = int(response.headers.get('content-length', 0))\n","    chunk_size = 4096  # 4 Kb\n","    progress_bar = tqdm(desc='Загрузка файла GGUF', total=total_size, unit='iB', unit_scale=True)\n","\n","    with open(file_path, 'wb') as file:\n","        for data in response.iter_content(chunk_size):\n","            file.write(data)\n","            progress_bar.update(len(data))\n","\n","\n","print('Инициализация токенайзера ...')\n","\n","# не забыть установить токенайзер, соотвествующий модели GGUF\n","# tokenizer_name = 'openchat/openchat-3.6-8b-20240522'\n","tokenizer_name = 'unsloth/gemma-2-2b-it'  # (оригинальный репозиторий google/gemma-2-2b требует HF токен)\n","\n","tokenizer_path = Path('tokenizer')\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=tokenizer_path)\n","\n","\n","print('Инициализация модели эмбедингов ...')\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# embed_model_name = 'sentence-transformers/all-mpnet-base-v2'\n","embed_model_name = 'cointegrated/rubert-tiny2'\n","\n","embed_model_path = Path('embed_model')\n","embed_model = HuggingFaceEmbeddings(\n","    model_name=embed_model_name,\n","    model_kwargs={'device': device},  # cpu cuda\n","    cache_folder=str(embed_model_path),\n","    )\n","\n","# llm_model_url = 'https://huggingface.co/bartowski/openchat-3.6-8b-20240522-GGUF/resolve/main/openchat-3.6-8b-20240522-IQ4_XS.gguf'\n","llm_model_url = 'https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf'\n","\n","llm_model_path = Path('model') / llm_model_url.rsplit('/')[-1]\n","llm_model_path.parent.mkdir(exist_ok=True)\n","\n","# если модель отсутствует в папке models то нужно ее загрузить - например через библиотеку wget\n","if not llm_model_path.is_file():\n","    if not str(llm_model_url).endswith('.gguf'):\n","        raise Exception('Ссылка на модель LLM должна быть прямой ссылкой на файл GGUF')\n","    else:\n","        print('Загрузка файла LLM модели ...')\n","        download_file(llm_model_url, llm_model_path)\n","\n","print('Инициализация LLM модели ...')\n","llm_model = Llama(model_path=str(llm_model_path), n_gpu_layers=-1, n_ctx=4096)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BfEG2Av0YnPt","outputId":"c331204b-5c0d-41b3-aa37-f835870d4caf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Инициализация токенайзера ...\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["Инициализация модели эмбедингов ...\n","Загрузка файла LLM модели ...\n"]},{"output_type":"stream","name":"stderr","text":["Загрузка файла GGUF: 100%|██████████| 3.08G/3.08G [00:51<00:00, 59.4MiB/s]\n","llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from model/openchat-3.5-0106.Q2_K.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = openchat_openchat-3.5-0106\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 10\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n","llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q2_K:   65 tensors\n","llama_model_loader: - type q3_K:  160 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens cache size = 261\n","llm_load_vocab: token to piece cache size = 0.1637 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32002\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n"]},{"output_type":"stream","name":"stdout","text":["Инициализация LLM модели ...\n"]},{"output_type":"stream","name":"stderr","text":["llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q2_K - Medium\n","llm_load_print_meta: model params     = 7.24 B\n","llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n","llm_load_print_meta: general.name     = openchat_openchat-3.5-0106\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 32000 '<|end_of_turn|>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.15 MiB\n","llm_load_tensors:        CPU buffer size =  2939.58 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n","llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n","llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '8192', 'general.name': 'openchat_openchat-3.5-0106', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\n","Using chat eos_token: <|end_of_turn|>\n","Using chat bos_token: <s>\n"]}]},{"cell_type":"markdown","source":["Другие варианты загрузки репозиториев с HF"],"metadata":{"id":"S0UlDutbQAIA"}},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download, snapshot_download\n","\n","snapshot_download(\n","    repo_id=model_id,\n","    local_dir=folder_path,\n","    ignore_patterns='*.h5',\n",")\n","\n","hf_hub_download(\n","    repo_id=model_id,\n","    filename=model_file,\n","    local_dir=MODELS_PATH,\n","    )"],"metadata":{"id":"r5sYo8xPybe8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Модуль `utils.py`"],"metadata":{"id":"Ul7fSIJY1uPK"}},{"cell_type":"markdown","source":["Модуль с функциями `utils.py`"],"metadata":{"id":"WGijM8xMwXIB"}},{"cell_type":"code","source":["import csv\n","from pathlib import Path\n","from typing import List, Tuple, Dict, Union, Optional, Any\n","from tqdm import tqdm\n","\n","import requests\n","from requests.exceptions import MissingSchema\n","\n","import gradio as gr\n","from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled\n","\n","from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","\n","from langchain_community.document_loaders import (\n","    CSVLoader,\n","    PDFMinerLoader,\n","    PyPDFLoader,\n","    TextLoader,\n","    UnstructuredHTMLLoader,\n","    UnstructuredMarkdownLoader,\n","    UnstructuredPowerPointLoader,\n","    UnstructuredWordDocumentLoader,\n","    WebBaseLoader,\n","    YoutubeLoader,\n","    DirectoryLoader,\n",")\n","\n","# annotations\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n","from langchain_core.retrievers import BaseRetriever\n","from langchain.docstore.document import Document\n","from langchain_core.vectorstores import VectorStore\n","from langchain_core.embeddings import Embeddings\n","\n","from models import tokenizer, embed_model, llm_model\n","\n","\n","# ========================= ПАРАМЕТРЫ ===============================\n","\n","LOADER_CLASSES = {\n","    '.csv': CSVLoader,\n","    '.doc': UnstructuredWordDocumentLoader,\n","    '.docx': UnstructuredWordDocumentLoader,\n","    '.html': UnstructuredHTMLLoader,\n","    '.md': UnstructuredMarkdownLoader,\n","    '.pdf': PDFMinerLoader,\n","    '.ppt': UnstructuredPowerPointLoader,\n","    '.pptx': UnstructuredPowerPointLoader,\n","    '.txt': TextLoader,\n","    'web': WebBaseLoader,\n","    'directory': DirectoryLoader,\n","    'youtube': YoutubeLoader,\n","}\n","\n","GENERATE_KWARGS = dict(\n","    temp=0.5,\n","    top_k=40,\n","    top_p=1,\n","    repeat_penalty=1,\n",")\n","CONTEXT_TEMPLATE = '''Ответь на вопрос при условии контекста\n","\n","Контекст:\n","{context}\n","\n","Дан вопрос:\n","{user_message}\n","\n","Ответ:'''\n","\n","\n","# ============================= ФУНКЦИИ =============================\n","\n","\n","def clear_text(text: str) -> str:\n","    lines = text.split('\\n')\n","    lines = [line for line in lines if len(line.strip()) > 2]\n","    text = '\\n'.join(lines).strip()\n","    return text\n","\n","\n","def clear_documents(documents: List[Document]) -> List[Document]:\n","    output_documents = []\n","    for document in documents:\n","        text = clear_text(document.page_content)\n","        if len(text) > 10:\n","            document.page_content = text\n","            output_documents.append(document)\n","    return output_documents\n","\n","\n","def get_csv_delimiter(file_path: str) -> str:\n","    n_bytes = 4096\n","    with open(file_path) as csvfile:\n","        delimiter = csv.Sniffer().sniff(csvfile.read(n_bytes)).delimiter\n","    return delimiter\n","\n","\n","def check_subtitles_available(yt_video_link: str, target_lang: str) -> Tuple[bool, str]:\n","    video_id = yt_video_link.split('watch?v=')[-1].split('&')[0]\n","    load_log = ''\n","    available = True\n","    try:\n","        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n","        try:\n","            transcript = transcript_list.find_transcript([target_lang])\n","            if transcript.is_generated:\n","                load_log += f'Будут загружены автоматические субтитры, ручные недоступны для видео {yt_video_link}\\n'\n","            else:\n","                load_log += f'Будут загружены ручные субтитры для видео {yt_video_link}\\n'\n","        except NoTranscriptFound:\n","            load_log += f'Язык субтитров {target_lang} недоступен для видео {yt_video_link}\\n'\n","            available = False\n","    except TranscriptsDisabled:\n","        load_log += f'Нет субтитров для видео {yt_video_link}\\n'\n","        available = False\n","    return available, load_log\n","\n","\n","def load_documents_from_files(upload_files: List[str]) -> Tuple[List[Document], str]:\n","    load_log = ''\n","    documents = []\n","    for upload_file in upload_files:\n","        file_extension = f\".{upload_file.split('.')[-1]}\"\n","        if file_extension in LOADER_CLASSES:\n","            loader_class = LOADER_CLASSES[file_extension]\n","            loader_kwargs = {}\n","            if file_extension == '.csv':\n","                delimiter = get_csv_delimiter(upload_file)\n","                loader_kwargs = {'csv_args': {'delimiter': delimiter}}\n","            try:\n","                load_documents = loader_class(upload_file, **loader_kwargs).load()\n","                documents.extend(load_documents)\n","            except Exception as ex:\n","                load_log += f'Ошибка загрузки файла {upload_file}\\n'\n","                load_log += f'Код ошибки: {ex}\\n'\n","                continue\n","        else:\n","            load_log += f'Неподдерживаемый формат файла {upload_file}\\n'\n","            continue\n","    return documents, load_log\n","\n","\n","def load_documents_from_links(web_links: str, subtitles_lang: str) -> Tuple[List[Document], str]:\n","    load_log = ''\n","    documents = []\n","    loader_class_kwargs = {}\n","    web_links = [web_link.strip() for web_link in web_links.split('\\n') if web_link.strip()]\n","\n","    for web_link in web_links:\n","        if 'youtube.com' in web_link:\n","            available, log = check_subtitles_available(web_link, subtitles_lang)\n","            load_log += log\n","            if not available:\n","                continue\n","            loader_class = LOADER_CLASSES['youtube'].from_youtube_url\n","            loader_class_kwargs = {'language': subtitles_lang}\n","\n","        else:\n","            loader_class = LOADER_CLASSES['web']\n","        try:\n","            if requests.get(web_link).status_code != 200:\n","                load_log += f'Ссылка недоступна для Python requests: {web_link}\\n'\n","                continue\n","            load_documents = loader_class(web_link, **loader_class_kwargs).load()\n","            if len(load_documents) == 0:\n","                load_log += f'Фрагменты текста не были найдены по ссылке: {web_link}\\n'\n","                continue\n","            documents.extend(load_documents)\n","        except MissingSchema:\n","            load_log += f'Неверная ссылка: {web_link}\\n'\n","            continue\n","        except Exception as ex:\n","            load_log += f'Ошибка загрузки web лоадером данных по ссылке: {web_link}\\n'\n","            load_log += f'Код ошибки: {ex}\\n'\n","            continue\n","    return documents, load_log\n","\n","\n","def load_documents_and_create_db(\n","        upload_files: Optional[List[str]],\n","        web_links: str,\n","        subtitles_lang: str,\n","        chunk_size: int,\n","        chunk_overlap: int,\n","        ) -> Tuple[List[Document], Optional[BaseRetriever], str]:\n","\n","    load_log = ''\n","    all_documents = []\n","    db = None\n","    progress = gr.Progress()\n","\n","    if upload_files is None and not web_links:\n","        load_log = 'Не выбраны файлы или ссылки'\n","        return all_documents, db, load_log\n","\n","    if upload_files is not None:\n","        progress(0.3, desc='Шаг 1/2: Загрузка документов из файлов')\n","        docs, log = load_documents_from_files(upload_files)\n","        all_documents.extend(docs)\n","        load_log += log\n","\n","    if web_links:\n","        progress(0.3 if upload_files is None else 0.5, desc='Шаг 1/2: Загрузка документов по ссылкам')\n","        docs, log = load_documents_from_links(web_links, subtitles_lang)\n","        all_documents.extend(docs)\n","        load_log += log\n","\n","    if len(all_documents) == 0:\n","        load_log += 'Загрузка прервана так как не было извлечено ни одного документа\\n'\n","        load_log += 'Режим RAG не может быть активирован'\n","        return all_documents, db, load_log\n","\n","    load_log += f'Загружено документов: {len(all_documents)}\\n'\n","\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size,\n","        chunk_overlap=chunk_overlap,\n","    )\n","    documents = text_splitter.split_documents(all_documents)\n","    documents = clear_documents(documents)\n","    load_log += f'Документы разделены, кол-во фрагментов текста: {len(documents)}\\n'\n","\n","    progress(0.7, desc='Шаг 2/2: Инициализация БД')\n","    db = FAISS.from_documents(documents=documents, embedding=embed_model)\n","    load_log += 'Режим RAG активирован и может быть дективирован на вкладке Generate'\n","    return documents, db, load_log\n","\n","\n","def get_promt_with_context(\n","        user_message: str,\n","        chatbot: List[List[Optional[str]]],\n","        history_len: int,\n","        rag_mode: bool,\n","        db: VectorStore,\n","        system_prompt: str,\n","        k: Union[int, str],\n","        score_threshold: float,\n","        ) -> Tuple[str, List[List[Optional[str]]], str]:\n","\n","    chatbot.append([user_message, None])\n","    if not user_message.strip():\n","        return '', chatbot, full_promt\n","\n","    messages = []\n","    if history_len == 0 and system_prompt:\n","        messages.append({'role': 'system', 'content': system_prompt})\n","\n","    if history_len != 0:\n","        for user_msg, bot_msg in chatbot[:-1][-history_len:]:\n","            messages.append({'role': 'user', 'content': user_msg})\n","            messages.append({'role': 'assistant', 'content': bot_msg})\n","\n","    if db is not None and rag_mode:\n","        if k == 'max':\n","            k = len(documents)\n","\n","        docs_and_distances = db.similarity_search_with_relevance_scores(\n","            user_message,\n","            k=k,\n","            score_threshold=score_threshold,\n","            )\n","\n","        if len(docs_and_distances) == 0:\n","            gr.Info((\n","                f'Релевантные запросу документы не найдены, '\n","                f'используется промт без контекста (попробуйте уменьшить searh_score_threshold)'\n","                ))\n","        else:\n","            retriever_context = '\\n\\n'.join([doc[0].page_content for doc in docs_and_distances])\n","            user_message = CONTEXT_TEMPLATE.format(\n","                user_message=user_message,\n","                context=retriever_context,\n","                )\n","\n","    messages.append({'role': 'user', 'content': user_message})\n","    full_promt = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True,\n","        )\n","    return '', chatbot, full_promt\n","\n","\n","def generate_text(\n","        chatbot: List[List[Optional[str]]],\n","        full_promt: str,\n","        do_sample: bool,\n","        *generate_args: List[Union[int, float]],\n","        ) -> List[List[Optional[str]]]:\n","\n","    if chatbot[-1][0].strip() == '':\n","        yield chatbot[:-1]\n","\n","    else:\n","        tokens_indxs = llm_model.tokenize(full_promt.encode('utf-8'), special=True, add_bos=False)\n","        gen_kwargs = dict(zip(GENERATE_KWARGS.keys(), generate_args))\n","        if not do_sample:\n","            gen_kwargs['top_k'] = 1\n","            gen_kwargs['repeat_penalty'] = 1\n","\n","        chatbot[-1][1] = ''\n","        generator = llm_model.generate(tokens_indxs, **gen_kwargs)\n","        for token_indx in generator:\n","            if token_indx == llm_model.token_eos():\n","                break\n","            token = llm_model.detokenize([token_indx]).decode('utf-8', errors='ignore')\n","            chatbot[-1][1] += token\n","            yield chatbot"],"metadata":{"id":"3K8u3DwMKv6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Файл приложения `app.py`"],"metadata":{"id":"5TNIDzJX13lQ"}},{"cell_type":"markdown","source":["Главный файл приложения `app.py`"],"metadata":{"id":"B8xZfSRbyD6-"}},{"cell_type":"code","source":["from pathlib import Path\n","from typing import List, Tuple, Dict, Union, Iterable, Optional, Any\n","\n","import requests\n","import gradio as gr\n","import torch\n","\n","from utils import (\n","    load_documents_and_create_db,\n","    get_promt_with_context,\n","    generate_text,\n",")\n","\n","\n","# ========================= ПАРАМЕТРЫ ===============================\n","\n","SUBTITLES_LANGUAGES = ['ru', 'en']\n","HISTORY_LEN = 0\n","GENERATE_KWARGS = dict(\n","    temp=0.5,\n","    top_k=40,\n","    top_p=1,\n","    repeat_penalty=1,\n",")\n","\n","\n","# ============================ ПРИЛОЖЕНИЕ ==============================\n","\n","CSS = '''\n",".gradio-container {width: 70% !important}\n","'''\n","\n","with gr.Blocks(theme=gr.themes.Monochrome(), css=CSS) as demo:\n","\n","    # ==================== СОСТОЯНИЯ ===============================\n","\n","    documents = gr.State([])\n","    db = gr.State(None)\n","    full_promt = gr.State('')\n","\n","\n","    # ==================== СТРАНИЦА БОТА =================================\n","\n","    def get_chatbot_and_rag_settings(chatbot_history: list = [], rag_mode: bool = False):\n","        label = 'RAG' if rag_mode else 'Chatbot'\n","        chatbot = gr.Chatbot(\n","                value=chatbot_history,\n","                show_copy_button=True,\n","                bubble_full_width=False,\n","                label=label,\n","                # height=300,\n","                )\n","        k = gr.Radio(\n","            choices=[1, 2, 3, 'max'],\n","            value=2,\n","            label='Количество релевантных документов для поиска',\n","            visible=rag_mode,\n","            render=False,\n","            )\n","        score_threshold = gr.Slider(\n","            label='searh_score_threshold',\n","            value=0.5,\n","            minimum=0,\n","            maximum=1,\n","            step=0.1,\n","            visible=rag_mode,\n","            render=False,\n","            )\n","        return chatbot, k, score_threshold\n","\n","\n","    with gr.Tab(label='Generate'):\n","        with gr.Row():\n","            with gr.Column(scale=3):\n","                chatbot, k, score_threshold = get_chatbot_and_rag_settings()\n","                user_message = gr.Textbox(label='User')\n","                with gr.Row():\n","                    user_message_btn = gr.Button('Отправить')\n","                    stop_btn = gr.Button('Стоп')\n","                    clear_btn = gr.Button('Очистить чат')\n","\n","            # ------------------ ПАРАМЕТРЫ ГЕНЕРАЦИИ -------------------------\n","            def get_generate_args(do_sample: bool):\n","                visible = do_sample\n","                generate_args = [\n","                    gr.Slider(0.1, 4, GENERATE_KWARGS['temp'], step=0.1, label='temperature', visible=visible),\n","                    gr.Slider(5, 50, GENERATE_KWARGS['top_k'], step=5, label='top_k', visible=visible),\n","                    gr.Slider(0.1, 1, GENERATE_KWARGS['top_p'], step=0.1, label='top_p', visible=visible),\n","                    gr.Slider(1, 3, GENERATE_KWARGS['repeat_penalty'], step=0.1, label='repeat penalty', visible=visible),\n","                ]\n","                return generate_args\n","\n","            with gr.Column(scale=1, min_width=80):\n","                with gr.Group():\n","                    gr.Markdown('Размер истории')\n","                    history_len = gr.Slider(\n","                        minimum=0,\n","                        maximum=5,\n","                        value=HISTORY_LEN,\n","                        step=1,\n","                        info='Кол-во предыдущих сообщенией, учитываемых в истории',\n","                        label='history len',\n","                        show_label=False,\n","                        )\n","\n","                    with gr.Group():\n","                        gr.Markdown('Параметры генерации')\n","                        do_sample = gr.Checkbox(\n","                            value=False,\n","                            label='do_sample',\n","                            info='Активация случайного семплирования',\n","                            )\n","                        generate_args = get_generate_args(do_sample.value)\n","                        do_sample.change(\n","                            fn=get_generate_args,\n","                            inputs=do_sample,\n","                            outputs=generate_args,\n","                            show_progress=False,\n","                            )\n","\n","        rag_mode = gr.Checkbox(value=False, label='Режим RAG', scale=1, visible=False)\n","        rag_mode.change(\n","            fn=get_chatbot_and_rag_settings,\n","            inputs=[chatbot, rag_mode],\n","            outputs=[chatbot, k, score_threshold],\n","            )\n","\n","        k.render()\n","        score_threshold.render()\n","\n","        # -------------------- СИСТЕМНЫЙ ПРОМТ И ИТОГОВЫЙ ПРОМТ ---------------------------\n","\n","        with gr.Accordion('Промт', open=True):\n","            system_prompt = gr.Textbox('', label='Задать системный промт')\n","            full_promt = gr.Textbox(label='Полный текст текущего промта с контекстом', interactive=False)\n","\n","        # ------------------ КНОПКИ ОТПРАВИТЬ ОЧИСТИТЬ И СТОП ------------\n","\n","        generate_event = gr.on(\n","            triggers=[user_message.submit, user_message_btn.click],\n","            fn=get_promt_with_context,\n","            inputs=[user_message, chatbot, history_len, rag_mode, db, system_prompt, k, score_threshold],\n","            outputs=[user_message, chatbot, full_promt],\n","            queue=True,\n","        ).then(\n","            fn=lambda promt: promt,\n","            inputs=full_promt,\n","            outputs=full_promt,\n","            queue=False,\n","        ).then(\n","            fn=generate_text,\n","            inputs=[chatbot, full_promt, do_sample, *generate_args],\n","            outputs=chatbot,\n","            queue=True,\n","            )\n","\n","        stop_btn.click(\n","            fn=None,\n","            inputs=None,\n","            outputs=None,\n","            cancels=generate_event,\n","            queue=False,\n","        )\n","\n","        clear_btn.click(\n","            fn=lambda: (None, ''),\n","            inputs=None,\n","            outputs=[chatbot, full_promt],\n","            queue=False,\n","            )\n","\n","\n","    # ===================== СТРАНИЦА ЗАГРУЗКИ ФАЙЛОВ =========================\n","\n","    with gr.Tab(label='Load documents'):\n","        with gr.Row(variant='compact'):\n","            upload_files = gr.File(file_count='multiple', label='Загрузка текстовых файлов')\n","            web_links = gr.Textbox(lines=6, label='Ссылки на Web сайты или Ютуб')\n","\n","        with gr.Row(variant='compact'):\n","            chunk_size = gr.Slider(50, 2000, value=500, step=50, label='Длина фрагментов')\n","            chunk_overlap = gr.Slider(0, 200, value=20, step=10, label='Длина пересечения фрагментов')\n","\n","            subtitles_lang = gr.Radio(\n","                SUBTITLES_LANGUAGES,\n","                value=SUBTITLES_LANGUAGES[0],\n","                label='Язык субтитров YouTube',\n","                )\n","\n","        load_documents_btn = gr.Button(value='Загрузить документы и инициализировать БД')\n","        load_docs_log = gr.Textbox(label='Прогресс загрузки и разделения документов', interactive=False)\n","\n","        load_event = load_documents_btn.click(\n","            fn=load_documents_and_create_db,\n","            inputs=[upload_files, web_links, subtitles_lang, chunk_size, chunk_overlap],\n","            outputs=[documents, db, load_docs_log],\n","            )\n","\n","        def documents_load_success(chatbot_history, db):\n","            rag_mode = db is not None\n","            chatbot, k, score_threshold = get_chatbot_and_rag_settings(chatbot_history, rag_mode)\n","            rag_mode_checkbox = gr.Checkbox(value=rag_mode, label='Режим RAG', scale=1, visible=rag_mode)\n","            return rag_mode_checkbox\n","\n","        load_event.success(\n","            fn=documents_load_success,\n","            inputs=[chatbot, db],\n","            outputs=[rag_mode],\n","            )\n","\n","\n","    # ================= СТРАНИЦА ПРОСМОТРА ВСЕХ ДОКУМЕНТОВ =================\n","\n","    with gr.Tab(label='View documents'):\n","        view_documents_btn = gr.Button(value='Отобразить загруженные фрагменты')\n","        view_documents_textbox = gr.Textbox(\n","            lines=1,\n","            placeholder='Для просмотра фрагментов загрузите документы на вкладке Load documents',\n","            label='Загруженные фрагменты',\n","            )\n","        sep = '=' * 20\n","        view_documents_btn.click(\n","            lambda documents: f'\\n{sep}\\n\\n'.join([doc.page_content for doc in documents]),\n","            inputs=documents,\n","            outputs=view_documents_textbox,\n","        )\n","\n","# для докера устанвоить server_name='0.0.0.0' либо в Dockerfile установить ENV GRADIO_SERVER_NAME='0.0.0.0'\n","demo.launch()"],"metadata":{"id":"mYnRwI7hyDbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Docker"],"metadata":{"id":"hK89gjq0Ksmb"}},{"cell_type":"markdown","source":["Содержимое всех файлов кроме `*docker*` такое же как в разделе `Итоговое приложение` выше"],"metadata":{"id":"miTbW2Whzomq"}},{"cell_type":"markdown","source":["Инструкции по образам Docker для `llama-cpp-python`  \n","https://github.com/abetlen/llama-cpp-python/tree/main/docker"],"metadata":{"id":"TvDqqUiVuWMw"}},{"cell_type":"markdown","source":["**Структура проекта:**\n","\n"," - 📁 `embed_model`\n"," - 📁 `model`\n"," - 📁 `tokenizer`\n"," - `.dockerignore`\n"," - `Dockerfile-cpu`\n"," - `Dockerfile-cuda`\n"," - `requirements-base.txt`\n"," - `requirements-cpu.txt`\n"," - `requirements-cuda.txt`\n"," - `app.py`\n"," - `models.py`\n"," - `utils.py`\n","\n"],"metadata":{"id":"mzcjNqzXxqlv"}},{"cell_type":"markdown","source":["**Сборка образа и запуск контейнера:**\n","\n","1. Сборка образа  \n","\n"," - с поддержкой CPU\n","```\n","docker build -t chatbot-rag-cpu -f Dockerfile-cpu .\n","```\n"," - с поддержкой CUDA\n","```\n","docker build -t chatbot-rag-cuda -f Dockerfile-cuda .\n","```\n","\n","2. Запуск контейнера на 7860 порту с пробросом папок через `docker volumes`\n"," - с поддержкой CPU\n","```\n","docker run -it -p 7860:7860 \\\n","\t-v $(pwd)/tokenizer:/app/tokenizer \\\n","\t-v $(pwd)/embed_model:/app/embed_model \\\n","\t-v $(pwd)/model:/app/model \\\n","\tchatbot-rag-cpu\n","```\n","\n"," - с поддержкой CUDA\n","```\n","docker run -it --gpus all --ipc=host -p 7860:7860 \\\n","\t-v $(pwd)/tokenizer:/app/tokenizer \\\n","\t-v $(pwd)/embed_model:/app/embed_model \\\n","\t-v $(pwd)/model:/app/model \\\n","\tchatbot-rag-cuda\n","```\n","\n","Перейти в браузере http://localhost:7860/ после того как появится надпись `Running on local URL:  http://0.0.0.0:7860`"],"metadata":{"id":"p-7rkxCW5X_J"}},{"cell_type":"markdown","source":["Образ CPU занимает 2.65GB  \n","Образ CUDA занимает 13.9GB"],"metadata":{"id":"Zmw1popXeDLU"}},{"cell_type":"markdown","source":["**Содержимое `.dockerignore`:**  \n","```\n","__pycache__\n","model/*\n","embed_model/*\n","tokenizer/*\n","env*\n","*.ipynb\n","```"],"metadata":{"id":"m5w1cgqpBP2v"}},{"cell_type":"markdown","source":["**Содержимое `Dockerfile-cpu`**\n","```\n","FROM python:3.10\n","WORKDIR /app\n","COPY . .\n","RUN pip install --no-cache-dir -r requirements-cpu.txt\n","EXPOSE 7860\n","ENV GRADIO_SERVER_NAME='0.0.0.0'\n","CMD ['python', 'app.py']\n","```"],"metadata":{"id":"VfzlfUyi1BmY"}},{"cell_type":"markdown","source":["**Содержимое `Dockerfile-cuda`**\n","```\n","FROM nvidia/cuda:12.1.1-devel-ubuntu22.04\n","WORKDIR /app\n","COPY . .\n","RUN apt-get update && apt-get upgrade -y \\\n","    && apt-get install -y git build-essential \\\n","    python3 python3-pip gcc wget \\\n","    ocl-icd-opencl-dev opencl-headers clinfo \\\n","    libclblast-dev libopenblas-dev \\\n","    && mkdir -p /etc/OpenCL/vendors && echo 'libnvidia-opencl.so.1' > /etc/OpenCL/vendors/nvidia.icd\n","\n","ENV CUDA_DOCKER_ARCH=all\n","ENV LLAMA_CUDA=1\n","ENV CMAKE_ARGS='-DLLAMA_CUBLAS=ON'\n","\n","RUN pip install --no-cache-dir -r requirements-cuda.txt\n","EXPOSE 7860\n","ENV GRADIO_SERVER_NAME='0.0.0.0'\n","CMD ['python3', 'app.py']\n","```"],"metadata":{"id":"A_q0Q48R3oBu"}},{"cell_type":"markdown","source":["Для других версий CUDA заменить в `Dockerfile-cuda` соответствующий базовый образ вместо  \n","```\n","FROM nvidia/cuda:12.1.1-devel-ubuntu22.04\n","```"],"metadata":{"id":"9TkzXoLDEwB7"}}],"metadata":{"colab":{"toc_visible":true,"provenance":[{"file_id":"1h7AYwLTb-WCYXItz-jB49gJZ_BOJe0WD","timestamp":1727126774827}],"collapsed_sections":["A7w7hpUsKpM1","hK89gjq0Ksmb"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}